var documenterSearchIndex = {"docs":
[{"location":"api.html#API","page":"API","title":"API","text":"","category":"section"},{"location":"api.html","page":"API","title":"API","text":"Pages   = [\"api.md\"]\nModules = [CTBenchmarks]\nOrder = [:module, :constant, :type, :function, :macro]","category":"page"},{"location":"api.html#CTBenchmarks.benchmark-Tuple{}","page":"API","title":"CTBenchmarks.benchmark","text":"benchmark(;\n    outpath,\n    problems,\n    solver_models,\n    grid_sizes,\n    disc_methods,\n    tol,\n    ipopt_mu_strategy,\n    print_trace,\n    max_iter,\n    max_wall_time,\n    grid_size_max_cpu\n) -> String\n\nRun benchmarks on optimal control problems and save results to a JSON file.\n\nThis function performs the following steps:\n\nDetects CUDA availability and filters out :exa_gpu if CUDA is not functional\nRuns benchmarks using benchmark_data() to generate a DataFrame of results\nCollects environment metadata (Julia version, OS, machine, timestamp)\nBuilds a JSON-friendly payload combining results and metadata\nSaves the payload to outpath as pretty-printed JSON\n\nThe JSON file can be easily loaded and converted back to a DataFrame using:\n\nusing JSON, DataFrames\ndata = JSON.parsefile(\"path/to/data.json\")\ndf = DataFrame(data[\"results\"])\n\nArguments\n\noutpath: Path to save the JSON file\nproblems: Vector of problem names (Symbols)\nsolver_models: Vector of Pairs mapping solver => models (e.g., [:ipopt => [:JuMP, :adnlp], :madnlp => [:exa, :exa_gpu]])\ngrid_sizes: Vector of grid sizes (Int)\ndisc_methods: Vector of discretization methods (Symbols)\ntol: Solver tolerance (Float64)\nipopt_mu_strategy: Mu strategy for Ipopt (String)\nprint_trace: Boolean - whether to print solver output (for debugging)\nmax_iter: Maximum number of iterations (Int)\nmax_wall_time: Maximum wall time in seconds (Float64)\ngrid_size_max_cpu: Maximum grid size for CPU models (Int)\n\nReturns\n\nThe outpath of the saved JSON file.\n\n\n\n\n\n","category":"method"},{"location":"api.html#CTBenchmarks.benchmark_data-Tuple{}","page":"API","title":"CTBenchmarks.benchmark_data","text":"benchmark_data(;\n    problems,\n    solver_models,\n    grid_sizes,\n    disc_methods,\n    tol,\n    ipopt_mu_strategy,\n    print_trace\n    max_iter,\n    max_wall_time,\n    grid_size_max_cpu\n) -> DataFrame\n\nRun benchmarks on optimal control problems and return results as a DataFrame.\n\nFor each combination of problem, solver, model, and grid size, this function:\n\nSets up and solves the optimization problem\nCaptures timing and memory statistics using @btimed or CUDA.@timed\nExtracts solver statistics (objective value, iterations)\nStores all data in a DataFrame row\n\nArguments\n\nproblems: Vector of problem names (Symbols)\nsolver_models: Vector of Pairs mapping solver => models (e.g., [:ipopt => [:JuMP, :adnlp], :madnlp => [:exa, :exa_gpu]])\ngrid_sizes: Vector of grid sizes (Int)\ndisc_methods: Vector of discretization methods (Symbols)\ntol: Solver tolerance (Float64)\nipopt_mu_strategy: Mu strategy for Ipopt (String)\nprint_trace: Boolean - whether to print solver output (for debugging)\nmax_iter: Maximum number of iterations (Int)\nmax_wall_time: Maximum wall time in seconds (Float64)\n\nReturns\n\nA DataFrame with columns:\n\nproblem: Symbol - problem name\nsolver: Symbol - solver used (:ipopt or :madnlp)\nmodel: Symbol - model type (:JuMP, :adnlp, :exa, or :exa_gpu)\ndisc_method: Symbol - discretization method\ngrid_size: Int - number of grid points\ntol: Float64 - solver tolerance\nmu_strategy: Union{String, Missing} - mu strategy for Ipopt (missing for MadNLP)\nprint_level: Any - print level for solver (Int for Ipopt, MadNLP.LogLevels for MadNLP)\nmax_iter: Int - maximum number of iterations\nmax_wall_time: Float64 - maximum wall time in seconds\nbenchmark: NamedTuple - full benchmark object from @btimed or CUDA.@timed\nobjective: Union{Float64, Missing} - objective function value (missing if failed)\niterations: Union{Int, Missing} - number of solver iterations (missing if failed)\nstatus: Any - termination status (type depends on solver/model)\nsuccess: Bool - whether the solve succeeded\n\n\n\n\n\n","category":"method"},{"location":"api.html#CTBenchmarks.build_payload-Tuple{DataFrames.DataFrame, Dict}","page":"API","title":"CTBenchmarks.build_payload","text":"build_payload(results::DataFrame, meta::Dict) -> Dict\n\nCombine benchmark results DataFrame and metadata into a JSON-friendly dictionary. The DataFrame is converted to a vector of dictionaries (one per row) for easy JSON serialization and reconstruction.\n\n\n\n\n\n","category":"method"},{"location":"api.html#CTBenchmarks.filter_models_for_backend-Tuple{Vector{Symbol}, Symbol}","page":"API","title":"CTBenchmarks.filter_models_for_backend","text":"filter_models_for_backend(models::Vector{Symbol}, disc_method::Symbol) -> Vector{Symbol}\n\nFilter solver models depending on backend availability and discretization support.\n\nGPU models (ending with _gpu) are kept only if CUDA is available.\nJuMP models are kept only when disc_method == :trapeze.\n\n\n\n\n\n","category":"method"},{"location":"api.html#CTBenchmarks.generate_metadata-Tuple{}","page":"API","title":"CTBenchmarks.generate_metadata","text":"generate_metadata() -> Dict{String, String}\n\nReturn metadata about the current environment:\n\ntimestamp (UTC, ISO8601)\njulia_version\nos\nmachine hostname\npkg_status - output of Pkg.status() with ANSI colors\nversioninfo - output of versioninfo() with ANSI colors\npkg_manifest - output of Pkg.status(mode=PKGMODE_MANIFEST) with ANSI colors\n\n\n\n\n\n","category":"method"},{"location":"api.html#CTBenchmarks.is_cuda_on-Tuple{}","page":"API","title":"CTBenchmarks.is_cuda_on","text":"is_cuda_on() -> Bool\n\nReturn true if CUDA is functional on this machine.\n\n\n\n\n\n","category":"method"},{"location":"api.html#CTBenchmarks.prettymemory-Tuple{Any}","page":"API","title":"CTBenchmarks.prettymemory","text":"prettymemory(bytes::Integer) -> String\n\nFormat a memory footprint bytes into a human-readable string using binary prefixes (bytes, KiB, MiB, GiB) with two decimal places.\n\n\n\n\n\n","category":"method"},{"location":"api.html#CTBenchmarks.prettytime-Tuple{Any}","page":"API","title":"CTBenchmarks.prettytime","text":"prettytime(t::Real) -> String\n\nFormat a duration t expressed in seconds into a human-readable string with three decimal places and adaptive units (ns, Î¼s, ms, s).\n\n\n\n\n\n","category":"method"},{"location":"api.html#CTBenchmarks.print_benchmark_line-Tuple{Symbol, NamedTuple}","page":"API","title":"CTBenchmarks.print_benchmark_line","text":"print_benchmark_line(model::Symbol, stats::NamedTuple)\n\nPrint a formatted line summarizing benchmark statistics for model with colors. Handles both CPU benchmarks (from @btimed) and GPU benchmarks (from CUDA.@timed).\n\nDisplays: time, allocations/memory, objective, iterations, and success status\n\n\n\n\n\n","category":"method"},{"location":"api.html#CTBenchmarks.run","page":"API","title":"CTBenchmarks.run","text":"Run the benchmarks for a specific version.\n\nArguments\n\nversion::Symbol: version to run (:complete or :minimal)\noutpath::Union{AbstractString, Nothing}: directory path to save results (nothing for no saving)\nprint_trace::Bool: whether to print the trace of the solver\n\nReturns\n\nnothing\n\n\n\n\n\n","category":"function"},{"location":"api.html#CTBenchmarks.sanitize_for_json-Tuple{Any}","page":"API","title":"CTBenchmarks.sanitize_for_json","text":"sanitize_for_json(obj)\n\nRecursively replace NaN and Inf values with null for JSON compatibility.\n\n\n\n\n\n","category":"method"},{"location":"api.html#CTBenchmarks.save_json-Tuple{Dict, AbstractString}","page":"API","title":"CTBenchmarks.save_json","text":"save_json(payload::Dict, outpath::AbstractString)\n\nSave a JSON payload to a file. Creates the parent directory if needed. Uses pretty printing for readability. Sanitizes NaN and Inf values to null for JSON compatibility.\n\n\n\n\n\n","category":"method"},{"location":"api.html#CTBenchmarks.set_print_level-Tuple{Symbol, Bool}","page":"API","title":"CTBenchmarks.set_print_level","text":"set_print_level(solver::Symbol, print_trace::Bool) -> Int\n\nSet print level based on solver and print_trace flag.\n\n\n\n\n\n","category":"method"},{"location":"api.html#CTBenchmarks.solve_and_extract_data-Tuple{Symbol, Symbol, Symbol, Int64, Symbol, Float64, Union{Missing, String}, Bool, Int64, Float64}","page":"API","title":"CTBenchmarks.solve_and_extract_data","text":"solve_and_extract_data(problem, solver, model, grid_size, disc_method, \n                      tol, mu_strategy, print_level, max_iter, max_wall_time) -> NamedTuple\n\nSolve an optimal control problem and extract performance and solver statistics.\n\nThis internal helper function handles the solve process and data extraction for different model types (JuMP, adnlp, exa, exa_gpu).\n\nArguments\n\nproblem::Symbol: problem name (e.g., :beam, :chain)\nsolver::Symbol: solver to use (:ipopt or :madnlp)\nmodel::Symbol: model type (:JuMP, :adnlp, :exa, or :exa_gpu)\ngrid_size::Int: number of grid points\ndisc_method::Symbol: discretization method\ntol::Float64: solver tolerance\nmu_strategy::Union{String, Missing}: mu strategy for Ipopt (missing for MadNLP)\nprint_level::Union{Int, MadNLP.LogLevels, Missing}: print level for solver (Int for Ipopt, MadNLP.LogLevels for MadNLP)\nmax_iter::Int: maximum number of iterations\nmax_wall_time::Float64: maximum wall time in seconds\n\nReturns\n\nA NamedTuple with fields:\n\nbenchmark: full benchmark object from @btimed (CPU) or CUDA.@timed (GPU)\nobjective::Union{Float64, Missing}: objective function value (missing if failed)\niterations::Union{Int, Missing}: number of solver iterations (missing if failed)\nstatus::Any: termination status (type depends on solver/model)\nsuccess::Bool: whether the solve succeeded\n\n\n\n\n\n","category":"method"},{"location":"api.html#CTBenchmarks.strip_benchmark_value-Tuple{Any}","page":"API","title":"CTBenchmarks.strip_benchmark_value","text":"strip_benchmark_value(bench)\n\nRemove the value field from benchmark outputs (NamedTuple or Dict) to ensure JSON-serializable data while preserving all other statistics.\n\n\n\n\n\n","category":"method"},{"location":"dev.html#Development-Guidelines","page":"Development Guidelines","title":"Development Guidelines","text":"","category":"section"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"This guide explains how to add a new benchmark to the CTBenchmarks.jl pipeline.","category":"page"},{"location":"dev.html#Overview","page":"Development Guidelines","title":"Overview","text":"","category":"section"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Adding a new benchmark involves creating several components:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"JSON configuration entry â­ Simple - Add benchmark config to JSON file (1 entry to add)\nBenchmark script â­ Simple - Julia script that runs the benchmark\nGitHub label â­ Simple - Label to trigger the benchmark on pull requests (manual step on GitHub)\nIndividual workflow â­ Optional - Workflow for manual testing (reads from JSON)\nDocumentation page â­ Optional - Display benchmark results in the documentation","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"tip: Estimated Time\nStep 1 (JSON): ~2 minutes\nStep 2 (Script): ~5-10 minutes\nStep 3 (Label): ~1 minute\nStep 4 (Optional workflow): ~5 minutes\nStep 5 (Optional docs): ~10 minutes","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"success: Key Improvement\nThe orchestrator now uses a JSON configuration file and matrix strategy. Adding a benchmark requires modifying only one JSON entry instead of multiple workflow files!","category":"page"},{"location":"dev.html#Step-by-Step-Guide","page":"Development Guidelines","title":"Step-by-Step Guide","text":"","category":"section"},{"location":"dev.html#1.-Add-Configuration-to-JSON","page":"Development Guidelines","title":"1. Add Configuration to JSON","text":"","category":"section"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Edit .github/benchmarks-config.json and add your benchmark configuration:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"{\n  \"benchmarks\": [\n    {\n      \"id\": \"your-benchmark-id\",\n      \"julia_version\": \"1.11\",\n      \"julia_arch\": \"x64\",\n      \"runs_on\": \"ubuntu-latest\",\n      \"runner\": \"github\"\n    }\n  ]\n}","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Configuration fields:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"id (required): Unique identifier for the benchmark (kebab-case)\nConvention: {family}-{runner} (e.g., core-ubuntu-latest, core-moonshot)\nUsed as script filename: benchmarks/{id}.jl\nUsed in label: run bench {id}\njulia_version (required): Julia version to use (e.g., \"1.11\")\njulia_arch (required): Architecture (typically \"x64\")\nruns_on (required): GitHub runner specification\nFor standard runners: \"ubuntu-latest\"\nFor self-hosted: \"[\\\"self-hosted\\\", \\\"Linux\\\", \\\"gpu\\\", \\\"cuda\\\", \\\"cuda12\\\"]\"\nrunner (required): Runner type for caching strategy\n\"github\" for standard GitHub runners (uses julia-actions/cache)\n\"self-hosted\" for self-hosted runners (uses actions/cache for artifacts only)","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Examples:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"// Standard GitHub runner\n{\n  \"id\": \"core-ubuntu-latest\",\n  \"julia_version\": \"1.11\",\n  \"julia_arch\": \"x64\",\n  \"runs_on\": \"ubuntu-latest\",\n  \"runner\": \"github\"\n}\n\n// Self-hosted GPU runner\n{\n  \"id\": \"core-moonshot\",\n  \"julia_version\": \"1.11\",\n  \"julia_arch\": \"x64\",\n  \"runs_on\": \"[\\\"self-hosted\\\", \\\"Linux\\\", \\\"gpu\\\", \\\"cuda\\\", \\\"cuda12\\\"]\",\n  \"runner\": \"self-hosted\"\n}","category":"page"},{"location":"dev.html#2.-Create-the-Benchmark-Script","page":"Development Guidelines","title":"2. Create the Benchmark Script","text":"","category":"section"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Create a new Julia script in the benchmarks/ directory with the filename {id}.jl:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Important: The script filename must exactly match the id in the JSON configuration.","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Example: For \"id\": \"core-ubuntu-latest\", create benchmarks/core-ubuntu-latest.jl","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"using Pkg\nconst project_dir = normpath(@__DIR__, \"..\")\nENV[\"PROJECT\"] = project_dir\n\nPkg.activate(project_dir)\nPkg.instantiate()\n\nusing CTBenchmarks\n\nfunction main()\n    outpath = joinpath(project_dir, \"docs\", \"src\", \"assets\", \"benchmarks\", \"<id>\")\n    CTBenchmarks.benchmark(;\n        outpath = outpath,\n        problems = [:problem1, :problem2, ...],\n        solver_models = [:solver => [:model1, :model2]],\n        grid_sizes = [100, 500, 1000],\n        disc_methods = [:trapeze],\n        tol = 1e-6,\n        ipopt_mu_strategy = \"adaptive\",\n        print_trace = false,\n        max_iter = 1000,\n        max_wall_time = 500.0\n    )\n    return outpath\nend\n\nmain()","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Key points:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"All parameters are required - the benchmark function has no optional arguments\nThe main() function is crucial - it must:\nTake no arguments\nReturn the output path where files are saved\nThe benchmark function generates JSON and TOML files in the specified outpath\nPrint statements (like println(\"ð¦ Activating...\")) are optional but helpful for debugging\nThe output directory follows the pattern docs/src/assets/benchmarks/{id}\nAvailable problems: The list of problems you can choose is available in the OptimalControlProblems.jl documentation","category":"page"},{"location":"dev.html#2.-Automatic-Workflow-Execution","page":"Development Guidelines","title":"2. Automatic Workflow Execution","text":"","category":"section"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Good news! You don't need to create a workflow file manually. The orchestrator automatically runs your benchmark based on the JSON configuration using a matrix strategy.","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"When you add a label to a PR (e.g., run bench your-benchmark-id), the orchestrator:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Reads .github/benchmarks-config.json\nFinds your benchmark configuration\nCalls the reusable workflow with the correct parameters\nConstructs the script path as benchmarks/{id}.jl","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Everything is automatic! â¨","category":"page"},{"location":"dev.html#3.-Create-the-GitHub-Label","page":"Development Guidelines","title":"3. Create the GitHub Label","text":"","category":"section"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"On GitHub, create a new label for your benchmark:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Go to your repository â Issues â Labels\nClick New label\nName: run bench {id} where {id} matches your JSON configuration\nExample: run bench core-ubuntu-latest\nExample: run bench core-moonshot\nImportant: Use the exact benchmark ID from JSON\nChoose a color and description\nClick Create label","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Label types:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Individual labels - Trigger a specific benchmark:\nFormat: run bench {id}\nExample: run bench core-moonshot\nExample: run bench minimal-ubuntu-latest\nGroup labels - Trigger all benchmarks with a common prefix:\nFormat: run bench {prefix}-all\nExample: run bench core-all â runs all core-* benchmarks\nExample: run bench minimal-all â runs all minimal-* benchmarks\nExample: run bench gpu-all â runs all gpu-* benchmarks","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Naming convention for benchmark families:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"To use group labels effectively, follow this naming convention:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"{family}-{runner} format (e.g., core-ubuntu-latest, core-moonshot)\nAll benchmarks in the same family share the same prefix\nGroup label run bench {family}-all will run all benchmarks in that family","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Examples:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"core-ubuntu-latest, core-moonshot, core-mothra â run bench core-all\nminimal-ubuntu-latest, minimal-macos â run bench minimal-all\ngpu-cuda12, gpu-cuda13 â run bench gpu-all","category":"page"},{"location":"dev.html#4.-(Optional)-Create-Individual-Workflow","page":"Development Guidelines","title":"4. (Optional) Create Individual Workflow","text":"","category":"section"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"info: Optional Step\nIndividual workflows are optional. The orchestrator will automatically run your benchmark based on the JSON configuration. Individual workflows are useful for:Manual testing via workflow_dispatch\nRunning a specific benchmark without the orchestrator\nDebugging","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Create .github/workflows/benchmark-{id}.yml:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"name: Benchmark {Name}\n\non:\n  workflow_call:\n  workflow_dispatch:\n\npermissions:\n  contents: write\n  pull-requests: write\n\njobs:\n  load-config:\n    runs-on: ubuntu-latest\n    outputs:\n      config: ${{ steps.get-config.outputs.config }}\n    steps:\n      - uses: actions/checkout@v5\n      - name: Get benchmark config\n        id: get-config\n        run: |\n          CONFIG=$(jq -c '.benchmarks[] | select(.id == \"{id}\")' .github/benchmarks-config.json)\n          echo \"config=$CONFIG\" >> $GITHUB_OUTPUT\n  \n  bench:\n    needs: load-config\n    uses: ./.github/workflows/benchmark-reusable.yml\n    with:\n      script_path: benchmarks/${{ fromJSON(needs.load-config.outputs.config).id }}.jl\n      julia_version: ${{ fromJSON(needs.load-config.outputs.config).julia_version }}\n      julia_arch: ${{ fromJSON(needs.load-config.outputs.config).julia_arch }}\n      runs_on: ${{ fromJSON(needs.load-config.outputs.config).runs_on }}\n      runner: ${{ fromJSON(needs.load-config.outputs.config).runner }}","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Key features:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Reads configuration from JSON - Single source of truth\nUses ID to construct script path - benchmarks/${{ fromJSON(...).id }}.jl ensures consistency\nCan be triggered manually via workflow_dispatch for testing\nCan be called by orchestrator via workflow_call\nNo hardcoded values - Everything comes from JSON configuration","category":"page"},{"location":"dev.html#5.-Create-Documentation-Page-(Optional)","page":"Development Guidelines","title":"5. Create Documentation Page (Optional)","text":"","category":"section"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"If you want to display results in the documentation, create docs/src/benchmark-<name>.md.template:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"# <Name> Benchmark\n\n```@setup BENCH_<NAME>\ninclude(joinpath(@__DIR__, \"assets\", \"utils.jl\"))\n\nconst BENCH_DIR = \"benchmark-<name>\"\nconst BENCH_DATA = _read_benchmark_json(joinpath(@__DIR__, \"assets\", BENCH_DIR, \"data.json\"))\n```\n\n## Description\n\nBrief description of your benchmark configuration.\n\n**Benchmark Configuration:**\n\n- **Solvers:** List of solvers\n- **Models:** List of models\n- **Grid sizes:** Discretisation points\n- **Tolerance:** 1e-6\n- **Limits:** Max iterations and wall time\n\n### ð¥ï¸ Environment\n\n<!-- INCLUDE_ENVIRONMENT:\nBENCH_DATA = BENCH_DATA\nBENCH_DIR = BENCH_DIR\nENV_NAME = BENCH_<NAME>\n-->\n\n### ð Results\n\n```@example BENCH_<NAME>\n_print_results(BENCH_DATA) # hide\nnothing # hide\n```","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Then add it to docs/make.jl:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"pages = [\n    \"Introduction\" => \"index.md\",\n    \"Core benchmark\" => \"benchmark-core.md\",\n    \"<Name> Benchmark\" => \"benchmark-<name>.md\",\n    \"API\" => \"api.md\",\n    \"Development Guidelines\" => \"dev.md\",\n]","category":"page"},{"location":"dev.html#Testing-Your-Benchmark","page":"Development Guidelines","title":"Testing Your Benchmark","text":"","category":"section"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Local testing: Run your script locally to verify it works\nPush changes: Commit and push all files\nCreate PR: Open a pull request\nAdd label: Add the run bench <name> label to trigger the workflow\nMonitor: Check the Actions tab to monitor execution","category":"page"},{"location":"dev.html#Troubleshooting","page":"Development Guidelines","title":"Troubleshooting","text":"","category":"section"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Cache issues on self-hosted runners:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Ensure runner: 'self-hosted' is set in your workflow\nThe reusable workflow uses actions/cache for artifacts only on self-hosted runners\nIf you see slow cache operations on self-hosted runners, verify the runner parameter is set correctly\nStandard runners should NOT have the runner parameter (let it default to use julia-actions/cache)","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Workflow not triggering:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Verify the label name matches exactly in the orchestrator\nCheck that the orchestrator's guard job includes your benchmark in outputs","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Benchmark script fails:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Check Julia version compatibility\nVerify all dependencies are available on the target runner\nReview the benchmark function parameters","category":"page"},{"location":"dev.html#Examples","page":"Development Guidelines","title":"Examples","text":"","category":"section"},{"location":"dev.html#Example-1:-Core-Moonshot-Benchmark-(CUDA-12)","page":"Development Guidelines","title":"Example 1: Core Moonshot Benchmark (CUDA 12)","text":"","category":"section"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"A complete GPU benchmark using CUDA 12:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Script: benchmarks/core-moonshot.jl\nWorkflow: .github/workflows/benchmark-core-moonshot.yml\nLabel: run bench core moonshot\nRunner: [\"self-hosted\", \"Linux\", \"gpu\", \"cuda\", \"cuda12\"]\nDocumentation: docs/src/benchmark-core.md.template","category":"page"},{"location":"dev.html#Example-2:-Core-Mothra-Benchmark-(CUDA-13)","page":"Development Guidelines","title":"Example 2: Core Mothra Benchmark (CUDA 13)","text":"","category":"section"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"A GPU benchmark identical to Moonshot but using CUDA 13 to compare performance:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"JSON entry: Added to .github/benchmarks-config.json\njson   {   \"id\": \"core-mothra\",   \"julia_version\": \"1.11\",   \"julia_arch\": \"x64\",   \"runs_on\": \"[\\\"self-hosted\\\", \\\"Linux\\\", \\\"gpu\\\", \\\"cuda\\\", \\\"cuda13\\\"]\",   \"runner\": \"self-hosted\"   }\nScript: benchmarks/core-mothra.jl\nOnly difference: outpath points to docs/src/assets/benchmarks/core-mothra\nLabel: run bench core-mothra\nWorkflow (optional): .github/workflows/benchmark-core-mothra.yml reads from JSON","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"This example demonstrates how to create a variant of an existing benchmark to test different hardware configurations.","category":"page"},{"location":"dev.html#How-the-Orchestrator-Works","page":"Development Guidelines","title":"How the Orchestrator Works","text":"","category":"section"},{"location":"dev.html#Matrix-Strategy","page":"Development Guidelines","title":"Matrix Strategy","text":"","category":"section"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"The orchestrator uses a matrix strategy to dynamically call benchmarks:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Guard job reads .github/benchmarks-config.json\nBased on PR labels, it builds a JSON array of selected benchmarks\nBenchmark job uses matrix to iterate over selected benchmarks\nEach matrix iteration calls benchmark-reusable.yml with the appropriate parameters","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Benefits:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"No need to declare individual jobs for each benchmark\nAdding a benchmark requires only JSON modification\nAll benchmarks run in parallel (matrix strategy)\nConsistent behavior across all benchmarks","category":"page"},{"location":"dev.html#Label-System","page":"Development Guidelines","title":"Label System","text":"","category":"section"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"The orchestrator supports two types of labels with automatic prefix detection:","category":"page"},{"location":"dev.html#Individual-Labels","page":"Development Guidelines","title":"Individual Labels","text":"","category":"section"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Format: run bench {id}\nBehavior: Runs the specific benchmark with that exact ID\nExamples:\nrun bench core-ubuntu-latest â runs only core-ubuntu-latest\nrun bench minimal-macos â runs only minimal-macos","category":"page"},{"location":"dev.html#Group-Labels-(Generic)","page":"Development Guidelines","title":"Group Labels (Generic)","text":"","category":"section"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Format: run bench {prefix}-all\nBehavior: Automatically runs all benchmarks whose ID starts with {prefix}-\nHow it works:\nThe orchestrator extracts the prefix from the label (e.g., core from run bench core-all)\nIt scans all benchmark IDs in the JSON\nIt selects all benchmarks matching the pattern {prefix}-*\nExamples:\nrun bench core-all â runs core-ubuntu-latest, core-moonshot, core-mothra\nrun bench minimal-all â runs minimal-ubuntu-latest, minimal-macos\nrun bench gpu-all â runs gpu-cuda12, gpu-cuda13","category":"page"},{"location":"dev.html#Multiple-Labels","page":"Development Guidelines","title":"Multiple Labels","text":"","category":"section"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"You can combine multiple labels on a PR:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"run bench core-all + run bench minimal-ubuntu-latest â runs all core-* benchmarks + minimal-ubuntu-latest\nrun bench core-moonshot + run bench gpu-all â runs core-moonshot + all gpu-* benchmarks","category":"page"},{"location":"dev.html#Automatic-Discovery","page":"Development Guidelines","title":"Automatic Discovery","text":"","category":"section"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"The system is completely generic - no hardcoded family names:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Add benchmarks with any prefix (e.g., perf-*, stress-*, validation-*)\nCreate corresponding group labels (e.g., run bench perf-all)\nThe orchestrator automatically detects and processes them","category":"page"},{"location":"dev.html#Configuration-File","page":"Development Guidelines","title":"Configuration File","text":"","category":"section"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"The .github/benchmarks-config.json file is the single source of truth:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Orchestrator reads it to discover available benchmarks\nIndividual workflows read it to get their configuration\nEasy to maintain and validate\nCan be extended with additional metadata","category":"page"},{"location":"benchmark-core.html#Core-benchmark","page":"Core benchmark","title":"Core benchmark","text":"","category":"section"},{"location":"benchmark-core.html#Ubuntu-Latest","page":"Core benchmark","title":"Ubuntu Latest","text":"","category":"section"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"This benchmark suite evaluates 14 optimal control problems on a standard CPU platform using GitHub Actions runners.","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"Benchmark Configuration:","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"Solvers: Ipopt and MadNLP\nModels: JuMP, ADNLPModels, ExaModels (CPU)\nGrid sizes: 200, 500, 1000 discretisation points\nDiscretisation: Trapeze method\nTolerance: 1e-6\nIpopt strategy: Adaptive barrier parameter\nLimits: 1000 iterations max, 500s wall time","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"This configuration focuses on CPU-based solvers and provides a comprehensive comparison across different modelling frameworks.","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"note: Note\nThe linear solver is MUMPS for all experiments.","category":"page"},{"location":"benchmark-core.html#Environment","page":"Core benchmark","title":"ð¥ï¸ Environment","text":"","category":"section"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"_basic_metadata(BENCH_CORE_UBUNTU) # hide","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"_downloads_toml(BENCH_CORE_UBUNTU) # hide","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"<details style=\"margin-bottom: 0.5em; margin-top: 0.5em;\"><summary>â¹ï¸ Version info</summary>","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"_bench_data(BENCH_CORE_UBUNTU) # hide","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"</details>","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"<details style=\"margin-bottom: 0.5em;\"><summary>ð¦ Package status</summary>","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"_package_status(BENCH_CORE_UBUNTU) # hide","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"</details>","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"<details style=\"margin-bottom: 0.5em;\"><summary>ð Complete manifest</summary>","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"_complete_manifest(BENCH_CORE_UBUNTU) # hide","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"</details>","category":"page"},{"location":"benchmark-core.html#Results","page":"Core benchmark","title":"ð Results","text":"","category":"section"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"_print_results(BENCH_CORE_UBUNTU) # hide\nnothing # hide","category":"page"},{"location":"benchmark-core.html#Moonshot","page":"Core benchmark","title":"Moonshot","text":"","category":"section"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"This benchmark suite evaluates 14 optimal control problems on GPU-accelerated hardware, focusing on large-scale problems.","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"Benchmark Configuration:","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"Solver: MadNLP (GPU-optimised)\nModels: ExaModels (CPU), ExaModels (GPU)\nGrid sizes: 1000, 5000, 10000 discretisation points\nDiscretisation: Trapeze method\nTolerance: 1e-6\nLimits: 1000 iterations max, 1000s wall time","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"This configuration demonstrates GPU acceleration capabilities with ExaModels on large-scale problems, comparing CPU vs GPU performance for the same modelling framework.","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"note: Note\nThe linear solver is MUMPS for all experiments.","category":"page"},{"location":"benchmark-core.html#Environment-2","page":"Core benchmark","title":"ð Environment","text":"","category":"section"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"_basic_metadata(BENCH_CORE_MOONSHOT) # hide","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"_downloads_toml(BENCH_CORE_MOONSHOT) # hide","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"<details style=\"margin-bottom: 0.5em; margin-top: 0.5em;\"><summary>â¹ï¸ Version info</summary>","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"_bench_data(BENCH_CORE_MOONSHOT) # hide","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"</details>","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"<details style=\"margin-bottom: 0.5em;\"><summary>ð¦ Package status</summary>","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"_package_status(BENCH_CORE_MOONSHOT) # hide","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"</details>","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"<details style=\"margin-bottom: 0.5em;\"><summary>ð Complete manifest</summary>","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"_complete_manifest(BENCH_CORE_MOONSHOT) # hide","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"</details>","category":"page"},{"location":"benchmark-core.html#Results-2","page":"Core benchmark","title":"â¡ Results","text":"","category":"section"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"_print_results(BENCH_CORE_MOONSHOT) # hide\nnothing # hide","category":"page"},{"location":"index.html#CTBenchmarks","page":"Introduction","title":"CTBenchmarks","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"CTBenchmarks.jl is a comprehensive benchmarking suite for optimal control problems, designed to evaluate and compare the performance of different solvers and modelling approaches within the control-toolbox ecosystem.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"This package provides:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"ð Pre-configured benchmark suites for quick performance evaluation\nð Automated result collection and analysis\nð§ Flexible API for creating custom benchmarks\nð Detailed performance metrics including timing, memory usage, and solver statistics","category":"page"},{"location":"index.html#Installation","page":"Introduction","title":"Installation","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"warning: Development Package\nCTBenchmarks.jl is not yet registered in the Julia General Registry. You must clone the repository to use it.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"To install CTBenchmarks.jl, clone the repository and activate the project:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"using Pkg\n\n# Clone the repository\nPkg.develop(url=\"https://github.com/control-toolbox/CTBenchmarks.jl\")\n\n# Or clone manually and activate\n# git clone https://github.com/control-toolbox/CTBenchmarks.jl.git\n# cd CTBenchmarks.jl\n# julia --project=.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Once installed, load the package:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"using CTBenchmarks","category":"page"},{"location":"index.html#Quick-Start","page":"Introduction","title":"Quick Start","text":"","category":"section"},{"location":"index.html#Running-Pre-configured-Benchmarks","page":"Introduction","title":"Running Pre-configured Benchmarks","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"CTBenchmarks provides two pre-configured benchmark suites:","category":"page"},{"location":"index.html#Minimal-Benchmark-(Fast)","page":"Introduction","title":"Minimal Benchmark (Fast)","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Run a quick benchmark on a single problem to test your setup:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"CTBenchmarks.run(:minimal)","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"This runs the :beam problem with:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Grid size: 100\nDiscretisation: trapeze\nSolvers: Ipopt and MadNLP\nModels: JuMP, adnlp, exa, exa_gpu","category":"page"},{"location":"index.html#Complete-Benchmark-(Comprehensive)","page":"Introduction","title":"Complete Benchmark (Comprehensive)","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Run the full benchmark suite across all problems:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"CTBenchmarks.run(:complete)","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"This runs 14 optimal control problems with:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Grid sizes: 100, 200, 500\nDiscretisations: trapeze, midpoint\nSolvers: Ipopt and MadNLP\nModels: JuMP, adnlp, exa, exa_gpu","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"tip: Solver Output\nBy default, solver output is suppressed. To see detailed solver traces, use:CTBenchmarks.run(:minimal; print_trace=true)","category":"page"},{"location":"index.html#Saving-Results","page":"Introduction","title":"Saving Results","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"To save benchmark results to a directory:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"CTBenchmarks.run(:minimal; outpath=\"my_results\")","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"This creates a directory containing:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"data.json - Benchmark results in JSON format\nProject.toml - Package dependencies\nManifest.toml - Complete dependency tree","category":"page"},{"location":"index.html#Creating-Custom-Benchmarks","page":"Introduction","title":"Creating Custom Benchmarks","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"For more control over your benchmarks, use the CTBenchmarks.benchmark function directly:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"CTBenchmarks.benchmark(;\n    outpath = \"custom_benchmark\",\n    problems = [:beam, :chain, :robot],\n    solver_models = [\n        :ipopt => [:JuMP, :adnlp, :exa],\n        :madnlp => [:exa, :exa_gpu]\n    ],\n    grid_sizes = [200, 500, 1000],\n    disc_methods = [:trapeze],\n    tol = 1e-6,\n    ipopt_mu_strategy = \"adaptive\",\n    print_trace = false,\n    max_iter = 1000,\n    max_wall_time = 500.0\n)","category":"page"},{"location":"index.html#Available-Problems","page":"Introduction","title":"Available Problems","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"CTBenchmarks includes 14 optimal control problems from OptimalControlProblems.jl:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":":beam - Beam control problem\n:chain - Chain of masses\n:double_oscillator - Double oscillator\n:ducted_fan - Ducted fan control\n:electric_vehicle - Electric vehicle optimisation\n:glider - Glider trajectory\n:insurance - Insurance problem\n:jackson - Jackson problem\n:robbins - Robbins problem\n:robot - Robot arm control\n:rocket - Rocket trajectory\n:space_shuttle - Space shuttle re-entry\n:steering - Steering control\n:vanderpol - Van der Pol oscillator","category":"page"},{"location":"index.html#Solver-and-Model-Combinations","page":"Introduction","title":"Solver and Model Combinations","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Supported Solvers:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":":ipopt - Interior Point Optimizer\n:madnlp - Matrix-free Augmented Lagrangian NLP solver","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Supported Models:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":":JuMP - JuMP modelling framework\n:adnlp - Automatic differentiation NLP models\n:exa - ExaModels (CPU)\n:exa_gpu - ExaModels (GPU acceleration)","category":"page"},{"location":"index.html#Benchmark-Parameters","page":"Introduction","title":"Benchmark Parameters","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"grid_sizes: Number of discretisation points (e.g., [100, 200, 500])\ndisc_methods: Discretisation schemes (:trapeze, :midpoint)\ntol: Solver tolerance (default: 1e-6)\nmax_iter: Maximum solver iterations (default: 1000)\nmax_wall_time: Maximum wall time in seconds (default: 500.0)","category":"page"},{"location":"index.html#Benchmark-Results-in-This-Documentation","page":"Introduction","title":"Benchmark Results in This Documentation","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"This documentation includes pre-computed benchmark results from continuous integration runs on different platforms:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Ubuntu Latest - Standard CPU benchmarks on GitHub Actions runners\nMoonshot - GPU-accelerated benchmarks on dedicated hardware","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"These results provide reference performance data and demonstrate the capabilities of different solver and model combinations. You can explore them in the Core Benchmark section.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Each benchmark result page includes:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"ð Performance metrics (time, memory, iterations)\nð¥ï¸ Environment information (Julia version, OS, hardware)\nð Reproducible benchmark scripts\nð¦ Complete dependency information","category":"page"},{"location":"index.html#Understanding-Benchmark-Output","page":"Introduction","title":"Understanding Benchmark Output","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"When you run a benchmark, you'll see output similar to:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Benchmarks results:\n\nââ problem: beam\nâ\nââââ¬ solver: ipopt, disc_method: trapeze\nâ  â\nâ  â  N : 100\nâ  â  â | JuMP    | time:    1.234 s | iters: 42    | obj: 1.234567e+00 | CPU:    2.5 MiB\nâ  â  â | adnlp   | time:    0.987 s | iters: 42    | obj: 1.234567e+00 | CPU:    2.1 MiB\nâ  â  â | exa     | time:    0.765 s | iters: 42    | obj: 1.234567e+00 | CPU:    1.8 MiB\nâ  ââ\nââ","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Legend:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"â / â - Success or failure indicator\nModel - Modelling framework (JuMP, ADNLPModels, ExaModels)\ntime - Total solve time\niters - Number of solver iterations\nobj - Objective function value\nMemory - CPU memory usage (GPU memory shown separately for GPU models)","category":"page"},{"location":"index.html#Documentation-build-environment","page":"Introduction","title":"Documentation build environment","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"_downloads_toml(\".\") # hide","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"<details style=\"margin-bottom: 0.5em; margin-top: 1em;\"><summary>â¹ï¸ Version info</summary>","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"versioninfo() # hide","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"</details>","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"<details style=\"margin-bottom: 0.5em;\"><summary>ð¦ Package status</summary>","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Pkg.status() # hide","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"</details>","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"<details style=\"margin-bottom: 0.5em;\"><summary>ð Complete manifest</summary>","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Pkg.status(; mode = PKGMODE_MANIFEST) # hide","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"</details>","category":"page"}]
}
