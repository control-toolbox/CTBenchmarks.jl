name: Benchmark Reusable

on:
  workflow_call:
    inputs:
      script_path:
        description: "Julia script to run benchmark"
        required: true
        type: string
      julia_version:
        description: "Julia version to install"
        required: true
        type: string
      julia_arch:
        description: "Julia architecture (e.g., x64, aarch64)"
        required: true
        type: string
      runs_on:
        description: "GitHub runner labels (e.g., ubuntu-latest or JSON array for self-hosted)"
        required: true
        type: string
      runner:
        description: "Runner type: 'github' for standard runners or 'self-hosted' for self-hosted runners"
        required: true
        type: string

permissions:
  contents: write
  pull-requests: write

jobs:
  run:
    runs-on: ${{ fromJSON(inputs.runs_on) }}
    timeout-minutes: 600 # max is 360 on GitHub runners
    env:
      SCRIPT_PATH: ${{ inputs.script_path }}

    steps:
      # ---------------------------
      # Checkout Repository
      # ---------------------------
      - name: Checkout repository
        uses: actions/checkout@v5
        with:
          fetch-depth: 0

      - name: ğŸ“‹ Repository checkout status
        run: |
          echo "âœ… Repository checked out successfully"
          echo "ğŸ“ Current directory: $(pwd)"
          echo "ğŸ“Š Total files: $(find . -type f | wc -l)"

      - name: ğŸ“„ Prepare benchmark output filename
        run: |
          OUTPUT_FILE="benchmark_output_path_$(date +%s)_$RANDOM.txt"
          echo "BENCHMARK_OUTPUT_FILE=$OUTPUT_FILE" >> $GITHUB_ENV
          echo "ğŸ“„ Benchmark output file will be stored in: $OUTPUT_FILE"

      # ---------------------------
      # Setup Julia Environment
      # ---------------------------
      - uses: julia-actions/setup-julia@v2
        with:
          version: ${{ inputs.julia_version }}
          arch: ${{ inputs.julia_arch }}

      - name: ğŸ”‹ Julia setup status
        run: |
          echo "âœ… Julia $(julia --version | cut -d' ' -f3) installed successfully"
          echo "ğŸ“ Julia location: $(which julia)"

      # ---------------------------
      # Julia caching
      # ---------------------------
      - name: Cache Julia packages (standard runners)
        if: inputs.runner != 'self-hosted'
        uses: julia-actions/cache@v2

      - name: Cache Julia artifacts (self-hosted runners)
        if: inputs.runner == 'self-hosted'
        uses: actions/cache@v4
        env:
          cache-name: cache-artifacts
        with:
          path: ~/.julia/artifacts
          key: ${{ runner.os }}-benchmark-${{ env.cache-name }}-${{ hashFiles('**/Project.toml') }}
          restore-keys: |
            ${{ runner.os }}-benchmark-${{ env.cache-name }}-
            ${{ runner.os }}-benchmark-
            ${{ runner.os }}-

      - name: ğŸ’¾ Julia cache status
        run: |
          if [ "${{ inputs.runner }}" = "self-hosted" ]; then
            echo "âœ… Julia artifacts cache configured (self-hosted strategy)"
          else
            echo "âœ… Julia package cache configured (standard strategy)"
          fi

      - uses: julia-actions/julia-buildpkg@v1
        with:
          ignore-no-cache: true

      - name: ğŸ”¨ Package build status
        run: |
          echo "âœ… Julia package built successfully"
          echo "ğŸ“¦ CTBenchmarks package ready for use"

      # ---------------------------
      # Run Benchmark Script
      # ---------------------------
      - name: Run benchmark script
        id: benchmark
        env:
          SCRIPT_PATH: ${{ env.SCRIPT_PATH }}
        run: |
          echo "ğŸš€ Starting benchmark execution..."
          
          BENCH_FILENAME=$(basename "$SCRIPT_PATH")
          BENCH_ID="${BENCH_FILENAME%.jl}"
          OUTPUT_DIR="docs/src/assets/benchmarks/$BENCH_ID"
          mkdir -p "$OUTPUT_DIR"
          export BENCHMARK_OUTPUT_DIR="$OUTPUT_DIR"
          export BENCHMARK_ID="$BENCH_ID"
          echo "BENCHMARK_OUTPUT_DIR=$OUTPUT_DIR" >> $GITHUB_ENV
          echo "BENCHMARK_ID=$BENCH_ID" >> $GITHUB_ENV

          echo "ğŸ·ï¸  Benchmark ID: $BENCH_ID"
          echo "ğŸ“ Output directory (pre-created): $OUTPUT_DIR"

          julia --color=yes -e '
            using Pkg
            
            println("ğŸ“¦ Activating project environment...")
            Pkg.activate(".")
            
            println("ğŸ“¥ Installing dependencies...")
            Pkg.instantiate()
            
            println("ğŸ”„ Updating dependencies...")
            Pkg.update()
            
            println("ğŸ“¦ Ensuring JSON package is available...")
            Pkg.add("JSON")
            using JSON
            
            println("ğŸ”„ Loading CTBenchmarks package...")
            using CTBenchmarks

            include(ENV["SCRIPT_PATH"])
            if !isdefined(Main, :run)
              error("Benchmark script must define a run() function that returns benchmark results")
            end

            println("â–¶ï¸  Running benchmark script...")
            results = run()

            bench_id = ENV["BENCHMARK_ID"]
            outpath = ENV["BENCHMARK_OUTPUT_DIR"]
            figures_dir = joinpath(outpath, "figures")

            println("ğŸ“Š Generating solution plots...")
            CTBenchmarks.plot_solutions(results, figures_dir)
            
            json_path = joinpath(outpath, string(bench_id, ".json"))
            println("ğŸ’¾ Saving results to ", json_path)
            CTBenchmarks.save_json(results, json_path)
            println("ğŸ“„ JSON file created: ", json_path)
            open(ENV["GITHUB_ENV"], "a") do io
              println(io, "BENCHMARK_JSON=$json_path")
            end
          '
          
          echo "âœ… Benchmark execution completed"
          echo "$OUTPUT_DIR" > "$BENCHMARK_OUTPUT_FILE"
          echo "ğŸ’¾ Output path saved to $BENCHMARK_OUTPUT_FILE"
          echo "benchmark_success=true" >> $GITHUB_OUTPUT

      # ---------------------------
      # Copy benchmark scripts and TOML files
      # ---------------------------
      - name: ğŸ“‹ Copy benchmark script to output directory
        if: steps.benchmark.outputs.benchmark_success == 'true'
        run: |
          echo "ğŸ“‹ Copying benchmark script to output directory..."
          
          if [ -f "$BENCHMARK_OUTPUT_FILE" ]; then
            OUTPUT_DIR=$(cat "$BENCHMARK_OUTPUT_FILE")
            echo "ğŸ“ Output directory: $OUTPUT_DIR"
            
            BENCH_ID=$(basename "$OUTPUT_DIR")
            SCRIPT_DEST="$OUTPUT_DIR/$BENCH_ID.jl"
            
            echo "ğŸ“„ Source script: $SCRIPT_PATH"
            echo "ğŸ“„ Destination: $SCRIPT_DEST"
            
            cp "$SCRIPT_PATH" "$SCRIPT_DEST"
            echo "âœ… Benchmark script copied successfully to $SCRIPT_DEST"
          else
            echo "âŒ ERROR: $BENCHMARK_OUTPUT_FILE not found"
            exit 1
          fi

      - name: ğŸ“‹ Copy TOML files to output directory
        if: steps.benchmark.outputs.benchmark_success == 'true'
        run: |
          echo "ğŸ“‹ Copying TOML files to output directory..."
          
          if [ -f "$BENCHMARK_OUTPUT_FILE" ]; then
            OUTPUT_DIR=$(cat "$BENCHMARK_OUTPUT_FILE")
            echo "ğŸ“ Output directory: $OUTPUT_DIR"
            
            cp Project.toml "$OUTPUT_DIR/Project.toml"
            cp Manifest.toml "$OUTPUT_DIR/Manifest.toml"
            
            echo "âœ… TOML files copied successfully to $OUTPUT_DIR"
          else
            echo "âŒ ERROR: $BENCHMARK_OUTPUT_FILE not found"
            exit 1
          fi

      # ---------------------------
      # Benchmark results validation
      # ---------------------------
      - name: ğŸ“Š Benchmark results validation
        if: steps.benchmark.outputs.benchmark_success == 'true'
        run: |
          echo "ğŸ” Validating benchmark results..."
          
          if [ -f "$BENCHMARK_OUTPUT_FILE" ]; then
            OUTPUT_DIR=$(cat "$BENCHMARK_OUTPUT_FILE")
            echo "âœ… Benchmark output directory found: $OUTPUT_DIR"
            
            BENCH_ID="${BENCHMARK_ID:-$(basename "$OUTPUT_DIR") }"
            DATA_JSON_PATH="$OUTPUT_DIR/$BENCH_ID.json"
            echo "ğŸ“„ Resolved JSON path: $DATA_JSON_PATH"
            
            if [ -f "$DATA_JSON_PATH" ]; then
              FILE_SIZE=$(stat -f%z "$DATA_JSON_PATH" 2>/dev/null || stat -c%s "$DATA_JSON_PATH" 2>/dev/null || echo "unknown")
              LINE_COUNT=$(wc -l < "$DATA_JSON_PATH")
              echo "âœ… Benchmark JSON file created successfully"
              echo "ğŸ“ File size: $FILE_SIZE bytes"
              echo "ğŸ“ File lines: $LINE_COUNT"
              echo "ğŸ“ File location: $DATA_JSON_PATH"

              for filename in Project.toml Manifest.toml; do
                FILE_PATH="$OUTPUT_DIR/$filename"
                if [ -f "$FILE_PATH" ]; then
                  echo "âœ… Found $filename alongside benchmark output"
                else
                  echo "âŒ ERROR: $filename not found in $OUTPUT_DIR"
                  exit 1
                fi
              done
              
              DIR_NAME=$(basename "$OUTPUT_DIR")
              SCRIPT_FILE="$OUTPUT_DIR/$DIR_NAME.jl"
              if [ -f "$SCRIPT_FILE" ]; then
                echo "âœ… Found benchmark script: $DIR_NAME.jl"
              else
                echo "âŒ ERROR: Benchmark script not found at $SCRIPT_FILE"
                exit 1
              fi
            else
              echo "âŒ ERROR: Benchmark JSON file not found at $DATA_JSON_PATH"
              exit 1
            fi
          else
            echo "âŒ ERROR: $BENCHMARK_OUTPUT_FILE not found"
            exit 1
          fi

      # ---------------------------
      # Commit benchmark results (silent version)
      # ---------------------------
      - name: Commit benchmark results to current branch
        if: steps.benchmark.outputs.benchmark_success == 'true'
        run: |
          echo "ğŸ”§ Configuring git..."
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"

          # Identify current branch (works for PRs and pushes)
          if [ -n "${{ github.head_ref }}" ]; then
            BRANCH_NAME="${{ github.head_ref }}"
          else
            BRANCH_NAME="${{ github.ref_name }}"
          fi
          echo "ğŸŒ³ Working branch: $BRANCH_NAME"

          # Fetch latest remote state quietly
          if git ls-remote --exit-code origin "$BRANCH_NAME" >/dev/null 2>&1; then
            git fetch origin "$BRANCH_NAME" --quiet
            git reset --mixed origin/"$BRANCH_NAME" --quiet
          fi

          # Determine output directory
          if [ -f "$BENCHMARK_OUTPUT_FILE" ]; then
            OUTPUT_DIR=$(cat "$BENCHMARK_OUTPUT_FILE")
          else
            echo "âŒ ERROR: $BENCHMARK_OUTPUT_FILE not found"
            exit 1
          fi

          echo "ğŸ“ Benchmark output directory: $OUTPUT_DIR"
          DIR_NAME=$(basename "$OUTPUT_DIR")

          ARTIFACTS=(
            "$OUTPUT_DIR/$DIR_NAME.json"
            "$OUTPUT_DIR/Project.toml"
            "$OUTPUT_DIR/Manifest.toml"
            "$OUTPUT_DIR/$DIR_NAME.jl"
            "$OUTPUT_DIR/figures"
          )

          # Add only benchmark artifacts
          git add "${ARTIFACTS[@]}" >/dev/null 2>&1

          # Quick summary of staged files
          STAGED=$(git diff --cached --name-only | wc -l)
          if [ "$STAGED" -eq 0 ]; then
            echo "â„¹ï¸  No changes detected in benchmark results â€” skipping commit"
            exit 0
          fi

          echo "ğŸ“ Committing $STAGED file(s) to branch $BRANCH_NAME..."
          git commit -m "ğŸ“Š Add benchmark results (${DIR_NAME})" \
                      -m "Results saved to ${OUTPUT_DIR}/${DIR_NAME}.json" \
                      -m "Includes environment TOMLs and benchmark script" \
                      >/dev/null 2>&1 || {
            echo "âŒ Commit failed"
            exit 1
          }

          # Try to rebase quietly before pushing (handle concurrent runs)
          if git ls-remote --exit-code origin "$BRANCH_NAME" >/dev/null 2>&1; then
            git pull --rebase --autostash origin "$BRANCH_NAME" --quiet || {
              echo "âš ï¸  Rebase conflict â€” retrying with merge"
              git rebase --abort || true
              git pull --strategy=ours origin "$BRANCH_NAME" --quiet
            }
          fi

          # Push only updated results
          echo "ğŸš€ Pushing results to $BRANCH_NAME..."
          git push origin HEAD:"$BRANCH_NAME" --quiet || {
            echo "âŒ Push failed (possible concurrency issue)"
            exit 1
          }

          echo "âœ… Benchmark results pushed successfully for $OUTPUT_DIR"

      # ---------------------------
      # Workflow summary
      # ---------------------------
      - name: ğŸ“ƒ Benchmark workflow summary
        if: steps.benchmark.outputs.benchmark_success == 'true'
        run: |
          if [ -f "$BENCHMARK_OUTPUT_FILE" ]; then
            OUTPUT_DIR=$(cat "$BENCHMARK_OUTPUT_FILE")
            echo "ğŸ“ Using benchmark output directory: $OUTPUT_DIR"
          else
            echo "âŒ ERROR: $BENCHMARK_OUTPUT_FILE not found"
            exit 1
          fi

          DIR_NAME=$(basename "$OUTPUT_DIR")
          echo "ğŸ“Š Benchmark workflow summary:"
          echo "âœ… Benchmark execution: SUCCESS"
          echo "ğŸ“ Results saved to: $OUTPUT_DIR/$DIR_NAME.json"
          echo "ğŸŒ³ Results committed to: ${{ github.head_ref || github.ref_name }} branch"
          echo "ğŸ“˜ Ready for documentation generation"
          echo "ğŸ‰ Reusable benchmark workflow completed successfully!"
