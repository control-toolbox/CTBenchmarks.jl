name: Benchmark Reusable

on:
  workflow_call:
    inputs:
      script_path:
        description: "Julia script to run benchmark"
        required: true
        type: string
      julia_version:
        description: "Julia version to install"
        required: true
        type: string
      julia_arch:
        description: "Julia architecture (e.g., x64, aarch64)"
        required: true
        type: string
      runs_on:
        description: "GitHub runner labels (e.g., ubuntu-latest or JSON array for self-hosted)"
        required: true
        type: string
      runner:
        description: "Runner identifier forwarded to benchmark script"
        required: false
        default: 'standard'
        type: string

permissions:
  contents: write
  pull-requests: write

jobs:
  run:
    runs-on: ${{ fromJSON(inputs.runs_on) }}
    timeout-minutes: 300
    env:
      SCRIPT_PATH: ${{ inputs.script_path }}

    steps:
      # ---------------------------
      # Checkout Repository
      # ---------------------------
      - name: Checkout repository
        uses: actions/checkout@v5
        with:
          fetch-depth: 0

      - name: ðŸ“‹ Repository checkout status
        run: |
          echo "âœ… Repository checked out successfully"
          echo "ðŸ“ Current directory: $(pwd)"
          echo "ðŸ“Š Total files: $(find . -type f | wc -l)"

      - name: ðŸ“„ Prepare benchmark output filename
        run: |
          OUTPUT_FILE="benchmark_output_path_$(date +%s)_$RANDOM.txt"
          echo "BENCHMARK_OUTPUT_FILE=$OUTPUT_FILE" >> $GITHUB_ENV
          echo "ðŸ“„ Benchmark output file will be stored in: $OUTPUT_FILE"

      # ---------------------------
      # Setup Julia Environment
      # ---------------------------
      - uses: julia-actions/setup-julia@v2
        with:
          version: ${{ inputs.julia_version }}
          arch: ${{ inputs.julia_arch }}

      - name: ðŸ”‹ Julia setup status
        run: |
          echo "âœ… Julia $(julia --version | cut -d' ' -f3) installed successfully"
          echo "ðŸ“ Julia location: $(which julia)"

      # Cache strategy: julia-actions/cache for standard runners, actions/cache for self-hosted
      - name: Cache Julia packages (standard runners)
        if: inputs.runner != 'self-hosted'
        uses: julia-actions/cache@v2

      - name: Cache Julia artifacts (self-hosted runners)
        if: inputs.runner == 'self-hosted'
        uses: actions/cache@v4
        env:
          cache-name: cache-artifacts
        with:
          path: ~/.julia/artifacts
          key: ${{ runner.os }}-benchmark-${{ env.cache-name }}-${{ hashFiles('**/Project.toml') }}
          restore-keys: |
            ${{ runner.os }}-benchmark-${{ env.cache-name }}-
            ${{ runner.os }}-benchmark-
            ${{ runner.os }}-

      - name: ðŸ’¾ Julia cache status
        run: |
          if [ "${{ inputs.runner }}" = "self-hosted" ]; then
            echo "âœ… Julia artifacts cache configured (self-hosted strategy)"
          else
            echo "âœ… Julia package cache configured (standard strategy)"
          fi

      - uses: julia-actions/julia-buildpkg@v1
        with:
          ignore-no-cache: true

      - name: ðŸ”¨ Package build status
        run: |
          echo "âœ… Julia package built successfully"
          echo "ðŸ“¦ CTBenchmarks package ready for use"

      # ---------------------------
      # Run Benchmark Script
      # ---------------------------
      - name: Run benchmark script
        id: benchmark
        env:
          SCRIPT_PATH: ${{ env.SCRIPT_PATH }}
        run: |
          echo "ðŸš€ Starting benchmark execution..."
          
          julia --color=yes -e '
            using Pkg
            Pkg.add("JSON")
            using JSON
            include(ENV["SCRIPT_PATH"])
            out = main()
            println("ðŸ“„ Output file: ", out)
            open(ENV["BENCHMARK_OUTPUT_FILE"], "w") do f
              write(f, string(out))
            end
            println("ðŸ’¾ Output path saved to ", ENV["BENCHMARK_OUTPUT_FILE"])
          '
          
          echo "âœ… Benchmark execution completed"
          echo "benchmark_success=true" >> $GITHUB_OUTPUT

      - name: ðŸ“‹ Copy benchmark script to output directory
        if: steps.benchmark.outputs.benchmark_success == 'true'
        run: |
          echo "ðŸ“‹ Copying benchmark script to output directory..."
          
          if [ -f "$BENCHMARK_OUTPUT_FILE" ]; then
            OUTPUT_DIR=$(cat "$BENCHMARK_OUTPUT_FILE")
            echo "ðŸ“ Output directory: $OUTPUT_DIR"
            
            # Extract the directory name (e.g., benchmark-core-moonshot)
            DIR_NAME=$(basename "$OUTPUT_DIR")
            SCRIPT_DEST="$OUTPUT_DIR/$DIR_NAME.jl"
            
            echo "ðŸ“„ Source script: $SCRIPT_PATH"
            echo "ðŸ“„ Destination: $SCRIPT_DEST"
            
            cp "$SCRIPT_PATH" "$SCRIPT_DEST"
            echo "âœ… Benchmark script copied successfully to $SCRIPT_DEST"
          else
            echo "âŒ ERROR: $BENCHMARK_OUTPUT_FILE not found"
            exit 1
          fi

      - name: ðŸ“Š Benchmark results validation
        if: steps.benchmark.outputs.benchmark_success == 'true'
        run: |
          echo "ðŸ” Validating benchmark results..."
          
          if [ -f "$BENCHMARK_OUTPUT_FILE" ]; then
            OUTPUT_DIR=$(cat "$BENCHMARK_OUTPUT_FILE")
            echo "âœ… Benchmark output directory found: $OUTPUT_DIR"
            
            DATA_JSON_PATH="$OUTPUT_DIR/data.json"
            echo "ðŸ“„ Resolved JSON path: $DATA_JSON_PATH"
            
            if [ -f "$DATA_JSON_PATH" ]; then
              FILE_SIZE=$(stat -f%z "$DATA_JSON_PATH" 2>/dev/null || stat -c%s "$DATA_JSON_PATH" 2>/dev/null || echo "unknown")
              LINE_COUNT=$(wc -l < "$DATA_JSON_PATH")
              echo "âœ… Benchmark JSON file created successfully"
              echo "ðŸ“ File size: $FILE_SIZE bytes"
              echo "ðŸ“ File lines: $LINE_COUNT"
              echo "ðŸ“ File location: $DATA_JSON_PATH"

              for filename in Project.toml Manifest.toml; do
                FILE_PATH="$OUTPUT_DIR/$filename"
                if [ -f "$FILE_PATH" ]; then
                  echo "âœ… Found $filename alongside benchmark output"
                else
                  echo "âŒ ERROR: $filename not found in $OUTPUT_DIR"
                  exit 1
                fi
              done
              
              # Validate benchmark script
              DIR_NAME=$(basename "$OUTPUT_DIR")
              SCRIPT_FILE="$OUTPUT_DIR/$DIR_NAME.jl"
              if [ -f "$SCRIPT_FILE" ]; then
                echo "âœ… Found benchmark script: $DIR_NAME.jl"
              else
                echo "âŒ ERROR: Benchmark script not found at $SCRIPT_FILE"
                exit 1
              fi
            else
              echo "âŒ ERROR: Benchmark JSON file not found at $DATA_JSON_PATH"
              exit 1
            fi
          else
            echo "âŒ ERROR: $BENCHMARK_OUTPUT_FILE not found"
            exit 1
          fi

      - name: Commit benchmark results to current branch
        if: steps.benchmark.outputs.benchmark_success == 'true'
        run: |
          echo "ðŸ”§ Configuring git user..."
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          echo "âœ… Git user configured"

          echo "ðŸŒ³ Checking current git state..."
          echo "Current HEAD: $(git rev-parse --short HEAD)"
          echo "Current branch: $(git branch --show-current || echo 'DETACHED HEAD')"
          
          if [ -n "${{ github.head_ref }}" ]; then
            BRANCH_NAME="${{ github.head_ref }}"
            echo "ðŸ”„ This is a PR, switching to branch: $BRANCH_NAME"
            git checkout -B "$BRANCH_NAME"
            echo "âœ… Now on branch: $(git branch --show-current)"
          else
            BRANCH_NAME="${{ github.ref_name }}"
            echo "ðŸ”„ This is a push, switching to branch: $BRANCH_NAME"
            git checkout -B "$BRANCH_NAME"
            echo "âœ… Now on branch: $(git branch --show-current)"
          fi

          echo "ðŸ“Š Adding benchmark results to current branch..."
          
          if [ -f "$BENCHMARK_OUTPUT_FILE" ]; then
            OUTPUT_DIR=$(cat "$BENCHMARK_OUTPUT_FILE")
            echo "ðŸ“ Using benchmark output directory: $OUTPUT_DIR"
          else
            echo "âŒ ERROR: $BENCHMARK_OUTPUT_FILE not found"
            exit 1
          fi

          # Get directory name for script filename
          DIR_NAME=$(basename "$OUTPUT_DIR")
          
          ARTIFACTS=(
            "$OUTPUT_DIR/data.json"
            "$OUTPUT_DIR/Project.toml"
            "$OUTPUT_DIR/Manifest.toml"
            "$OUTPUT_DIR/$DIR_NAME.jl"
          )

          git add "${ARTIFACTS[@]}"

          STAGED_ALL=true
          for artifact in "${ARTIFACTS[@]}"; do
            if git diff --cached --name-only | grep -q "$artifact"; then
              echo "âœ… $artifact staged for commit"
            else
              echo "âš ï¸  $artifact not staged (possibly no changes)"
              STAGED_ALL=false
            fi
          done

          if [ "$STAGED_ALL" = true ]; then
            echo "ðŸ“‹ Files to be committed:"
            git diff --cached --name-status
          fi

          if ! git diff --cached --quiet; then
            echo "ðŸ“ Committing benchmark results to current branch..."
            git commit -m "ðŸ“Š Add benchmark results" -m "Generated by reusable benchmark workflow" -m "Results saved to ${OUTPUT_DIR}/data.json" -m "Includes environment TOMLs and benchmark script"
            echo "âœ… Benchmark results committed successfully"
            
            echo "ðŸ”„ Fetching remote branch state before overwrite..."
            git fetch origin "$BRANCH_NAME" || echo "âš ï¸  Remote branch does not exist yet, will create it"

            echo "ðŸš€ Force-pushing benchmark results to branch: $BRANCH_NAME"
            git push --force-with-lease origin "$BRANCH_NAME"
            echo "âœ… Benchmark results force-pushed to $BRANCH_NAME"
          else
            echo "â„¹ï¸  No changes detected in benchmark results"
            echo "ðŸ“Š Current results are identical to previous run"
          fi

      - name: ðŸ“ƒ Benchmark workflow summary
        if: steps.benchmark.outputs.benchmark_success == 'true'
        run: |
          if [ -f "$BENCHMARK_OUTPUT_FILE" ]; then
            OUTPUT_DIR=$(cat "$BENCHMARK_OUTPUT_FILE")
            echo "ðŸ“ Using benchmark output directory: $OUTPUT_DIR"
          else
            echo "âŒ ERROR: $BENCHMARK_OUTPUT_FILE not found"
            exit 1
          fi

          echo "ðŸ“Š Benchmark workflow summary:"
          echo "âœ… Benchmark execution: SUCCESS"
          echo "ðŸ“ Results saved to: $OUTPUT_DIR/data.json"
          echo "ðŸŒ³ Results committed to: ${{ github.head_ref || github.ref_name }} branch"
          echo "ðŸ“˜ Ready for documentation generation"
          echo "ðŸŽ‰ Reusable benchmark workflow completed successfully!"
