<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Add a New Benchmark · CTBenchmarks</title><meta name="title" content="Add a New Benchmark · CTBenchmarks"/><meta property="og:title" content="Add a New Benchmark · CTBenchmarks"/><meta property="twitter:title" content="Add a New Benchmark · CTBenchmarks"/><meta name="description" content="Documentation for CTBenchmarks."/><meta property="og:description" content="Documentation for CTBenchmarks."/><meta property="twitter:description" content="Documentation for CTBenchmarks."/><script data-outdated-warner src="assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="search_index.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script><link href="https://control-toolbox.org/assets/css/documentation.css" rel="stylesheet" type="text/css"/><script src="https://control-toolbox.org/assets/js/documentation.js"></script><script src="assets/js/ctbenchmarks-details.js"></script><link href="assets/css/ctbenchmarks-details.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="index.html">CTBenchmarks</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="index.html">Introduction</a></li><li><a class="tocitem" href="performance_profile.html">Performance Profile</a></li><li><span class="tocitem">Core benchmarks</span><ul><li><a class="tocitem" href="core/cpu.html">CPU</a></li><li><a class="tocitem" href="core/gpu.html">GPU</a></li><li><input class="collapse-toggle" id="menuitem-3-3" type="checkbox"/><label class="tocitem" for="menuitem-3-3"><span class="docs-label">Problems</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="core/problems/beam.html">beam</a></li><li><a class="tocitem" href="core/problems/chain.html">chain</a></li><li><a class="tocitem" href="core/problems/double_oscillator.html">double_oscillator</a></li><li><a class="tocitem" href="core/problems/electric_vehicle.html">electric_vehicle</a></li><li><a class="tocitem" href="core/problems/glider.html">glider</a></li><li><a class="tocitem" href="core/problems/insurance.html">insurance</a></li><li><a class="tocitem" href="core/problems/jackson.html">jackson</a></li><li><a class="tocitem" href="core/problems/robbins.html">robbins</a></li><li><a class="tocitem" href="core/problems/robot.html">robot</a></li><li><a class="tocitem" href="core/problems/rocket.html">rocket</a></li><li><a class="tocitem" href="core/problems/space_shuttle.html">space_shuttle</a></li><li><a class="tocitem" href="core/problems/steering.html">steering</a></li><li><a class="tocitem" href="core/problems/vanderpol.html">vanderpol</a></li></ul></li></ul></li><li><span class="tocitem">API Reference</span><ul><li><a class="tocitem" href="api/public.html">Public</a></li><li><a class="tocitem" href="api/private.html">Private</a></li></ul></li><li><span class="tocitem">Developers Guidelines</span><ul><li class="is-active"><a class="tocitem" href="add_benchmark.html">Add a New Benchmark</a><ul class="internal"><li><a class="tocitem" href="#Overview"><span>Overview</span></a></li><li><a class="tocitem" href="#Step-by-Step-Guide"><span>Step-by-Step Guide</span></a></li><li><a class="tocitem" href="#Testing-Your-Benchmark"><span>Testing Your Benchmark</span></a></li><li><a class="tocitem" href="#Troubleshooting"><span>Troubleshooting</span></a></li><li><a class="tocitem" href="#Examples"><span>Examples</span></a></li><li><a class="tocitem" href="#How-the-Orchestrator-Works"><span>How the Orchestrator Works</span></a></li></ul></li><li><a class="tocitem" href="documentation_process.html">Documentation Process</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Developers Guidelines</a></li><li class="is-active"><a href="add_benchmark.html">Add a New Benchmark</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="add_benchmark.html">Add a New Benchmark</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/control-toolbox/CTBenchmarks.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Development-Guidelines"><a class="docs-heading-anchor" href="#Development-Guidelines">Development Guidelines</a><a id="Development-Guidelines-1"></a><a class="docs-heading-anchor-permalink" href="#Development-Guidelines" title="Permalink"></a></h1><p>This guide explains how to add a new benchmark to the CTBenchmarks.jl pipeline.</p><div class="admonition is-info" id="Note-12fd86634dcb3436"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-12fd86634dcb3436" title="Permalink"></a></header><div class="admonition-body"><p>This page focuses on the CI and configuration aspects of benchmarks. For a detailed explanation of how documentation pages are generated from templates (including <code>INCLUDE_ENVIRONMENT</code>, <code>INCLUDE_FIGURE</code>, <code>INCLUDE_TEXT</code>, and <code>@setup BENCH</code> blocks), see the <a href="documentation_process.html#documentation-process">Documentation Generation Process</a>.</p></div></div><h2 id="Overview"><a class="docs-heading-anchor" href="#Overview">Overview</a><a id="Overview-1"></a><a class="docs-heading-anchor-permalink" href="#Overview" title="Permalink"></a></h2><p>Adding a new benchmark involves creating several components:</p><table><tr><th style="text-align: right">Step</th><th style="text-align: right">Description</th><th style="text-align: right">Status</th></tr><tr><td style="text-align: right"><a href="add_benchmark.html#benchmark-script"><strong>Benchmark script</strong></a></td><td style="text-align: right">Julia script that runs the benchmark</td><td style="text-align: right">Required</td></tr><tr><td style="text-align: right"><a href="add_benchmark.html#json-config"><strong>JSON configuration</strong></a></td><td style="text-align: right">Add benchmark config to JSON file</td><td style="text-align: right">Required</td></tr><tr><td style="text-align: right"><a href="add_benchmark.html#github-label"><strong>GitHub label</strong></a></td><td style="text-align: right">Label to trigger the benchmark on pull requests</td><td style="text-align: right">Required</td></tr><tr><td style="text-align: right"><a href="add_benchmark.html#individual-workflow"><strong>Individual workflow</strong></a></td><td style="text-align: right">Workflow for manual testing (reads from JSON)</td><td style="text-align: right">Optional</td></tr><tr><td style="text-align: right"><a href="add_benchmark.html#documentation-page"><strong>Documentation page</strong></a></td><td style="text-align: right">Display benchmark results in the documentation</td><td style="text-align: right">Optional</td></tr></table><h2 id="Step-by-Step-Guide"><a class="docs-heading-anchor" href="#Step-by-Step-Guide">Step-by-Step Guide</a><a id="Step-by-Step-Guide-1"></a><a class="docs-heading-anchor-permalink" href="#Step-by-Step-Guide" title="Permalink"></a></h2><h3 id="benchmark-script"><a class="docs-heading-anchor" href="#benchmark-script">1. Create the Benchmark Script</a><a id="benchmark-script-1"></a><a class="docs-heading-anchor-permalink" href="#benchmark-script" title="Permalink"></a></h3><p>Create a new Julia script in the <code>benchmarks/</code> directory. Choose a descriptive filename that will serve as your benchmark identifier.</p><p><strong>Naming convention</strong>: Use kebab-case (e.g., <code>core-ubuntu-latest.jl</code>, <code>core-moonshot-gpu.jl</code>)</p><p><strong>Example</strong>: <code>benchmarks/core-ubuntu-latest.jl</code></p><pre><code class="language-julia hljs"># Benchmark script for &lt;id&gt;
# Setup (Pkg.activate, instantiate, update, using CTBenchmarks) is handled by the workflow

function run()
    results = CTBenchmarks.benchmark(;
        problems = [:problem1, :problem2, ...],
        solver_models = [:solver =&gt; [:model1, :model2]],
        grid_sizes = [100, 500, 1000],
        disc_methods = [:trapeze],
        tol = 1e-6,
        ipopt_mu_strategy = &quot;adaptive&quot;,
        print_trace = false,
        max_iter = 1000,
        max_wall_time = 500.0
    )
    println(&quot;✅ Benchmark completed successfully!&quot;)
    return results
end</code></pre><p><strong>Key points:</strong></p><ul><li><strong>Setup code is handled by the workflow</strong> - No need to include <code>using Pkg</code>, <code>Pkg.activate()</code>, <code>Pkg.instantiate()</code>, <code>Pkg.update()</code>, or <code>using CTBenchmarks</code> in your script. The GitHub Actions workflow handles all environment setup automatically.</li><li><strong>All parameters are required</strong> - the <code>benchmark</code> function has no optional arguments</li><li><strong>Define a <code>run()</code> function</strong> - it must take no arguments, return the <code>Dict</code> payload from <code>CTBenchmarks.benchmark</code>, and should not perform any file I/O</li><li>The workflow calls <code>run()</code>, saves the returned payload as <code>{id}.json</code>, and stores it under <code>docs/src/assets/benchmarks/{id}/</code></li><li><strong>TOML files are copied by the workflow</strong> - <code>Project.toml</code> and <code>Manifest.toml</code> are automatically copied to the output directory by the GitHub Actions workflow to ensure reproducibility</li><li><strong>Available problems:</strong> The list of problems you can choose is available in the <a href="https://control-toolbox.org/OptimalControlProblems.jl/stable/problems_browser.html">OptimalControlProblems.jl documentation</a></li><li><strong>For local testing:</strong> See <code>benchmarks/local.jl</code> for an example that includes the setup code needed to run benchmarks locally</li></ul><h3 id="json-config"><a class="docs-heading-anchor" href="#json-config">2. Add Configuration to JSON</a><a id="json-config-1"></a><a class="docs-heading-anchor-permalink" href="#json-config" title="Permalink"></a></h3><p>Edit <code>benchmarks/benchmarks-config.json</code> and add your benchmark configuration:</p><pre><code class="language-json hljs">{
  &quot;benchmarks&quot;: [
    {
      &quot;id&quot;: &quot;your-benchmark-id&quot;,
      &quot;julia_version&quot;: &quot;1.11&quot;,
      &quot;julia_arch&quot;: &quot;x64&quot;,
      &quot;runs_on&quot;: &quot;ubuntu-latest&quot;,
      &quot;runner&quot;: &quot;github&quot;
    }
  ]
}</code></pre><p><strong>Configuration fields:</strong></p><ul><li><p><strong><code>id</code></strong> (required): Unique identifier for the benchmark (kebab-case)</p><ul><li><strong>Must exactly match your script filename</strong> (without the <code>.jl</code> extension)</li><li>Convention: <code>{family}-{runner}</code> (e.g., <code>core-ubuntu-latest</code>, <code>core-moonshot</code>)</li><li>Example: if your script is <code>benchmarks/core-ubuntu-latest.jl</code>, use <code>&quot;id&quot;: &quot;core-ubuntu-latest&quot;</code></li><li>Used in label: <code>run bench {id}</code></li></ul></li><li><p><strong><code>julia_version</code></strong> (required): Julia version to use (e.g., <code>&quot;1.11&quot;</code>)</p></li><li><p><strong><code>julia_arch</code></strong> (required): Architecture (typically <code>&quot;x64&quot;</code>)</p></li><li><p><strong><code>runs_on</code></strong> (required): GitHub runner specification</p><ul><li>For standard runners: <code>&quot;ubuntu-latest&quot;</code></li><li>For self-hosted runners with custom labels: <code>&quot;[\&quot;moonshot\&quot;]&quot;</code> or <code>&quot;[\&quot;mothra\&quot;]&quot;</code> (use the runner label configured in your self-hosted runner)`</li></ul></li><li><p><strong><code>runner</code></strong> (required): Runner type for caching strategy</p><ul><li><code>&quot;github&quot;</code> for standard GitHub runners (uses <code>julia-actions/cache</code>)</li><li><code>&quot;self-hosted&quot;</code> for self-hosted runners (uses <code>actions/cache</code> for artifacts only)</li></ul></li></ul><p><strong>Examples:</strong></p><pre><code class="language-json hljs">// Standard GitHub runner
{
  &quot;id&quot;: &quot;core-ubuntu-latest&quot;,
  &quot;julia_version&quot;: &quot;1.11&quot;,
  &quot;julia_arch&quot;: &quot;x64&quot;,
  &quot;runs_on&quot;: &quot;ubuntu-latest&quot;,
  &quot;runner&quot;: &quot;github&quot;
}

// Self-hosted runner with custom label
{
  &quot;id&quot;: &quot;core-moonshot&quot;,
  &quot;julia_version&quot;: &quot;1.11&quot;,
  &quot;julia_arch&quot;: &quot;x64&quot;,
  &quot;runs_on&quot;: &quot;[\&quot;moonshot\&quot;]&quot;,
  &quot;runner&quot;: &quot;self-hosted&quot;
}</code></pre><p>Conceptually, each JSON entry is mapped directly to the inputs of the reusable workflow:</p><pre><code class="language-text hljs">benchmarks-config.json
   └─ for each {id, julia_version, julia_arch, runs_on, runner}
        └─ orchestrator matrix entry
             └─ benchmark-reusable.yml inputs:
                  script_path  = benchmarks/{id}.jl
                  julia_version
                  julia_arch
                  runs_on
                  runner</code></pre><p><strong>Good news!</strong> You don&#39;t need to create a workflow file manually. The orchestrator automatically runs your benchmark based on the JSON configuration using a matrix strategy.</p><p>When you add a label to a PR (e.g., <code>run bench your-benchmark-id</code>), the orchestrator:</p><ol><li>Reads <code>benchmarks/benchmarks-config.json</code></li><li>Finds your benchmark configuration by matching the label with the <code>id</code> field</li><li>Calls the reusable workflow with the parameters from the JSON (Julia version, architecture, runner, etc.)</li><li>The reusable workflow loads and executes your script at <code>benchmarks/{id}.jl</code></li><li>Results are saved to <code>docs/src/assets/benchmarks/{id}/{id}.json</code></li></ol><p><strong>Everything is automatic!</strong> ✨</p><p>The full CI/data flow is:</p><pre><code class="language-text hljs">GitHub label on PR: &quot;run bench {id}&quot; or &quot;run bench {prefix}-all&quot;
   └─ Orchestrator workflow (benchmarks-orchestrator.yml)
        ├─ Guard job:
        │    ├─ read benchmarks/benchmarks-config.json
        │    └─ build JSON matrix of selected benchmarks
        ├─ Benchmark job (matrix over selected benchmarks)
        │    └─ calls benchmark-reusable.yml with
        │         script_path = benchmarks/{id}.jl
        │         julia_version, julia_arch, runs_on, runner
        │         └─ run Julia script → run() → results Dict
        │              └─ save {id}.json + TOML + script under docs/src/assets/benchmarks/{id}/
        └─ Docs job
             └─ include(&quot;docs/make.jl&quot;)
                  └─ build &amp; deploy docs using latest JSON results</code></pre><h3 id="github-label"><a class="docs-heading-anchor" href="#github-label">3. Create the GitHub Label</a><a id="github-label-1"></a><a class="docs-heading-anchor-permalink" href="#github-label" title="Permalink"></a></h3><p>On GitHub, create a new label for your benchmark:</p><ol><li>Go to your repository → <strong>Issues</strong> → <strong>Labels</strong></li><li>Click <strong>New label</strong></li><li>Name: <code>run bench {id}</code> where <code>{id}</code> matches your JSON configuration<ul><li>Example: <code>run bench core-ubuntu-latest</code></li><li>Example: <code>run bench core-moonshot-gpu</code></li><li><strong>Important</strong>: Use the exact benchmark ID from JSON</li></ul></li><li>Choose a color and description</li><li>Click <strong>Create label</strong></li></ol><p><strong>Label types:</strong></p><ol><li><p><strong>Individual labels</strong> - Trigger a specific benchmark:</p><ul><li>Format: <code>run bench {id}</code></li><li>Example: <code>run bench core-moonshot-gpu</code></li><li>Example: <code>run bench minimal-ubuntu-latest</code></li></ul></li><li><p><strong>Group labels</strong> - Trigger all benchmarks with a common prefix:</p><ul><li>Format: <code>run bench {prefix}-all</code></li><li>Example: <code>run bench core-all</code> → runs all <code>core-*</code> benchmarks</li><li>Example: <code>run bench minimal-all</code> → runs all <code>minimal-*</code> benchmarks</li><li>Example: <code>run bench gpu-all</code> → runs all <code>gpu-*</code> benchmarks</li></ul></li></ol><p><strong>Naming convention for benchmark families:</strong></p><p>To use group labels effectively, follow this naming convention:</p><ul><li><code>{family}-{runner}</code> format (e.g., <code>core-ubuntu-latest</code>, <code>core-moonshot</code>)</li><li>All benchmarks in the same family share the same prefix</li><li>Group label <code>run bench {family}-all</code> will run all benchmarks in that family</li></ul><p><strong>Examples:</strong></p><ul><li><code>core-ubuntu-latest</code>, <code>core-moonshot-gpu</code>, <code>core-mothra-gpu</code> → <code>run bench core-all</code></li><li><code>minimal-ubuntu-latest</code>, <code>minimal-moonshot-gpu</code>, <code>minimal-mothra-gpu</code> → <code>run bench minimal-all</code></li><li><code>gpu-cuda12</code>, <code>gpu-cuda13</code> → <code>run bench gpu-all</code></li></ul><h3 id="individual-workflow"><a class="docs-heading-anchor" href="#individual-workflow">4. (Optional) Create Individual Workflow</a><a id="individual-workflow-1"></a><a class="docs-heading-anchor-permalink" href="#individual-workflow" title="Permalink"></a></h3><div class="admonition is-info" id="Optional-Step-69388d9d2a473b68"><header class="admonition-header">Optional Step<a class="admonition-anchor" href="#Optional-Step-69388d9d2a473b68" title="Permalink"></a></header><div class="admonition-body"><p>Individual workflows are <strong>optional</strong>. The orchestrator will automatically run your benchmark based on the JSON configuration. Individual workflows are useful for:</p><ul><li>Manual testing via <code>workflow_dispatch</code></li><li>Running a specific benchmark without the orchestrator</li><li>Debugging</li></ul></div></div><p>Create <code>.github/workflows/benchmark-{id}.yml</code>:</p><pre><code class="language-yaml hljs">name: Benchmark {Name}

on:
  workflow_call:
  workflow_dispatch:

permissions:
  contents: write
  pull-requests: write

jobs:
  load-config:
    runs-on: ubuntu-latest
    outputs:
      config: ${{ steps.get-config.outputs.config }}
    steps:
      - uses: actions/checkout@v5
      - name: Get benchmark config
        id: get-config
        run: |
          CONFIG=$(jq -c &#39;.benchmarks[] | select(.id == &quot;{id}&quot;)&#39; benchmarks/benchmarks-config.json)
          echo &quot;config=$CONFIG&quot; &gt;&gt; $GITHUB_OUTPUT
  
  bench:
    needs: load-config
    uses: ./.github/workflows/benchmark-reusable.yml
    with:
      script_path: benchmarks/${{ fromJSON(needs.load-config.outputs.config).id }}.jl
      julia_version: ${{ fromJSON(needs.load-config.outputs.config).julia_version }}
      julia_arch: ${{ fromJSON(needs.load-config.outputs.config).julia_arch }}
      runs_on: ${{ fromJSON(needs.load-config.outputs.config).runs_on }}
      runner: ${{ fromJSON(needs.load-config.outputs.config).runner }}</code></pre><p><strong>Key features:</strong></p><ul><li><strong>Reads configuration from JSON</strong> - Single source of truth</li><li><strong>Uses ID to construct script path</strong> - <code>benchmarks/${{ fromJSON(...).id }}.jl</code> ensures consistency</li><li><strong>Can be triggered manually</strong> via <code>workflow_dispatch</code> for testing</li><li><strong>Can be called by orchestrator</strong> via <code>workflow_call</code></li><li><strong>No hardcoded values</strong> - Everything comes from JSON configuration</li></ul><h3 id="documentation-page"><a class="docs-heading-anchor" href="#documentation-page">5. (Optional) Create Documentation Page</a><a id="documentation-page-1"></a><a class="docs-heading-anchor-permalink" href="#documentation-page" title="Permalink"></a></h3><p>If you want to display results in the documentation, you can create a template file (for example <code>docs/src/core/cpu.md.template</code> for a family of benchmarks, or <code>docs/src/benchmark-&lt;name&gt;.md.template</code> for a single benchmark) and let the documentation pipeline generate the final <code>.md</code> page.</p><p>At a high level, a benchmark documentation page:</p><ul><li>Defines a single <code>@setup BENCH</code> block that includes <code>utils.jl</code>.</li><li>Uses <code>INCLUDE_ENVIRONMENT</code> blocks to display environment and configuration information based on the benchmark ID.</li><li>Uses <code>INCLUDE_FIGURE</code> blocks to generate clickable figures (SVG + PDF).</li><li>Uses <code>INCLUDE_TEXT</code> blocks to insert performance-profile summaries or tables when needed.</li><li>Uses <code>@example BENCH</code> blocks with <code>_print_benchmark_log(&quot;&lt;id&gt;&quot;)</code> to show detailed results.</li></ul><p>For a concrete template example and a full description of how these blocks are processed, see the <a href="documentation_process.html#documentation-process">Documentation Generation Process</a>.</p><h2 id="Testing-Your-Benchmark"><a class="docs-heading-anchor" href="#Testing-Your-Benchmark">Testing Your Benchmark</a><a id="Testing-Your-Benchmark-1"></a><a class="docs-heading-anchor-permalink" href="#Testing-Your-Benchmark" title="Permalink"></a></h2><ol><li><strong>Local testing:</strong> Run your script locally to verify it works</li><li><strong>Push changes:</strong> Commit and push all files</li><li><strong>Create PR:</strong> Open a pull request</li><li><strong>Add label:</strong> Add the <code>run bench &lt;name&gt;</code> label to trigger the workflow</li><li><strong>Monitor:</strong> Check the Actions tab to monitor execution</li></ol><h2 id="Troubleshooting"><a class="docs-heading-anchor" href="#Troubleshooting">Troubleshooting</a><a id="Troubleshooting-1"></a><a class="docs-heading-anchor-permalink" href="#Troubleshooting" title="Permalink"></a></h2><p><strong>Cache issues on self-hosted runners:</strong></p><ul><li>Ensure <code>&quot;runner&quot;: &quot;self-hosted&quot;</code> is set in your JSON configuration</li><li>The reusable workflow uses <code>actions/cache</code> for artifacts only on self-hosted runners</li><li>Standard GitHub runners should use <code>&quot;runner&quot;: &quot;github&quot;</code> to enable full package caching</li></ul><p><strong>Workflow not triggering:</strong></p><ul><li>Verify the label name matches exactly: <code>run bench {id}</code> where <code>{id}</code> is from your JSON</li><li>Check that your benchmark ID exists in <code>benchmarks/benchmarks-config.json</code></li><li>Ensure the benchmark script file exists at <code>benchmarks/{id}.jl</code></li></ul><p><strong>Benchmark script fails:</strong></p><ul><li>Check Julia version compatibility</li><li>Verify all dependencies are available on the target runner</li><li>Review the benchmark function parameters</li></ul><h2 id="Examples"><a class="docs-heading-anchor" href="#Examples">Examples</a><a id="Examples-1"></a><a class="docs-heading-anchor-permalink" href="#Examples" title="Permalink"></a></h2><h3 id="Example-1:-Standard-GitHub-Runner"><a class="docs-heading-anchor" href="#Example-1:-Standard-GitHub-Runner">Example 1: Standard GitHub Runner</a><a id="Example-1:-Standard-GitHub-Runner-1"></a><a class="docs-heading-anchor-permalink" href="#Example-1:-Standard-GitHub-Runner" title="Permalink"></a></h3><p>A CPU benchmark running on GitHub Actions:</p><p><strong>JSON configuration:</strong></p><pre><code class="language-json hljs">{
  &quot;id&quot;: &quot;core-ubuntu-latest&quot;,
  &quot;julia_version&quot;: &quot;1.11&quot;,
  &quot;julia_arch&quot;: &quot;x64&quot;,
  &quot;runs_on&quot;: &quot;\&quot;ubuntu-latest\&quot;&quot;,
  &quot;runner&quot;: &quot;github&quot;
}</code></pre><p><strong>Files:</strong></p><ul><li><strong>Script</strong>: <code>benchmarks/core-ubuntu-latest.jl</code></li><li><strong>Label</strong>: <code>run bench core-ubuntu-latest</code></li><li><strong>Documentation</strong>: <code>docs/src/benchmark-core.md.template</code></li></ul><h3 id="Example-2:-Self-Hosted-Runner-(Moonshot)"><a class="docs-heading-anchor" href="#Example-2:-Self-Hosted-Runner-(Moonshot)">Example 2: Self-Hosted Runner (Moonshot)</a><a id="Example-2:-Self-Hosted-Runner-(Moonshot)-1"></a><a class="docs-heading-anchor-permalink" href="#Example-2:-Self-Hosted-Runner-(Moonshot)" title="Permalink"></a></h3><p>A GPU benchmark on a self-hosted runner with custom label:</p><p><strong>JSON configuration:</strong></p><pre><code class="language-json hljs">{
  &quot;id&quot;: &quot;core-moonshot-gpu&quot;,
  &quot;julia_version&quot;: &quot;1.11&quot;,
  &quot;julia_arch&quot;: &quot;x64&quot;,
  &quot;runs_on&quot;: &quot;[\&quot;moonshot\&quot;]&quot;,
  &quot;runner&quot;: &quot;self-hosted&quot;
}</code></pre><p><strong>Files:</strong></p><ul><li><strong>Script</strong>: <code>benchmarks/core-moonshot-gpu.jl</code></li><li><strong>Label</strong>: <code>run bench core-moonshot-gpu</code></li></ul><p><strong>Key points:</strong></p><ul><li>Uses simplified runner label <code>[&quot;moonshot&quot;]</code> instead of full system labels</li><li>The <code>runner: &quot;self-hosted&quot;</code> field tells the workflow to use artifact-only caching</li></ul><h3 id="Example-3:-Multiple-Runners,-Same-Hardware"><a class="docs-heading-anchor" href="#Example-3:-Multiple-Runners,-Same-Hardware">Example 3: Multiple Runners, Same Hardware</a><a id="Example-3:-Multiple-Runners,-Same-Hardware-1"></a><a class="docs-heading-anchor-permalink" href="#Example-3:-Multiple-Runners,-Same-Hardware" title="Permalink"></a></h3><p>You can create CPU and GPU variants for the same hardware:</p><p><strong>CPU variant:</strong></p><pre><code class="language-json hljs">{
  &quot;id&quot;: &quot;core-moonshot-cpu&quot;,
  &quot;julia_version&quot;: &quot;1.11&quot;,
  &quot;julia_arch&quot;: &quot;x64&quot;,
  &quot;runs_on&quot;: &quot;[\&quot;moonshot\&quot;]&quot;,
  &quot;runner&quot;: &quot;self-hosted&quot;
}</code></pre><p><strong>GPU variant:</strong></p><pre><code class="language-json hljs">{
  &quot;id&quot;: &quot;core-moonshot-gpu&quot;,
  &quot;julia_version&quot;: &quot;1.11&quot;,
  &quot;julia_arch&quot;: &quot;x64&quot;,
  &quot;runs_on&quot;: &quot;[\&quot;moonshot\&quot;]&quot;,
  &quot;runner&quot;: &quot;self-hosted&quot;
}</code></pre><p>Both use the same runner label but different benchmark scripts with different solver configurations.</p><h2 id="How-the-Orchestrator-Works"><a class="docs-heading-anchor" href="#How-the-Orchestrator-Works">How the Orchestrator Works</a><a id="How-the-Orchestrator-Works-1"></a><a class="docs-heading-anchor-permalink" href="#How-the-Orchestrator-Works" title="Permalink"></a></h2><h3 id="Matrix-Strategy"><a class="docs-heading-anchor" href="#Matrix-Strategy">Matrix Strategy</a><a id="Matrix-Strategy-1"></a><a class="docs-heading-anchor-permalink" href="#Matrix-Strategy" title="Permalink"></a></h3><p>The orchestrator uses a <strong>matrix strategy</strong> to dynamically call benchmarks:</p><ol><li><strong>Guard job</strong> reads <code>benchmarks/benchmarks-config.json</code></li><li>Based on PR labels, it builds a JSON array of selected benchmarks</li><li><strong>Benchmark job</strong> uses matrix to iterate over selected benchmarks</li><li>Each matrix iteration calls <code>benchmark-reusable.yml</code> with the appropriate parameters</li></ol><p><strong>Benefits:</strong></p><ul><li>No need to declare individual jobs for each benchmark</li><li>Adding a benchmark requires only JSON modification</li><li>All benchmarks run in parallel (matrix strategy)</li><li>Consistent behavior across all benchmarks</li></ul><h3 id="Label-System"><a class="docs-heading-anchor" href="#Label-System">Label System</a><a id="Label-System-1"></a><a class="docs-heading-anchor-permalink" href="#Label-System" title="Permalink"></a></h3><p>The orchestrator supports two types of labels with <strong>automatic prefix detection</strong>:</p><h4 id="Individual-Labels"><a class="docs-heading-anchor" href="#Individual-Labels">Individual Labels</a><a id="Individual-Labels-1"></a><a class="docs-heading-anchor-permalink" href="#Individual-Labels" title="Permalink"></a></h4><ul><li><strong>Format</strong>: <code>run bench {id}</code></li><li><strong>Behavior</strong>: Runs the specific benchmark with that exact ID</li><li><strong>Examples</strong>:<ul><li><code>run bench core-ubuntu-latest</code> → runs only <code>core-ubuntu-latest</code></li><li><code>run bench minimal-macos</code> → runs only <code>minimal-macos</code></li></ul></li></ul><h4 id="Group-Labels-(Generic)"><a class="docs-heading-anchor" href="#Group-Labels-(Generic)">Group Labels (Generic)</a><a id="Group-Labels-(Generic)-1"></a><a class="docs-heading-anchor-permalink" href="#Group-Labels-(Generic)" title="Permalink"></a></h4><ul><li><strong>Format</strong>: <code>run bench {prefix}-all</code></li><li><strong>Behavior</strong>: Automatically runs <strong>all</strong> benchmarks whose ID starts with <code>{prefix}-</code></li><li><strong>How it works</strong>:<ol><li>The orchestrator extracts the prefix from the label (e.g., <code>core</code> from <code>run bench core-all</code>)</li><li>It scans all benchmark IDs in the JSON</li><li>It selects all benchmarks matching the pattern <code>{prefix}-*</code></li></ol></li><li><strong>Examples</strong>:<ul><li><code>run bench core-all</code> → runs <code>core-ubuntu-latest</code>, <code>core-moonshot-cpu</code>, <code>core-moonshot-gpu</code>, <code>core-mothra-gpu</code></li><li><code>run bench minimal-all</code> → runs all benchmarks starting with <code>minimal-</code></li></ul></li></ul><h4 id="Multiple-Labels"><a class="docs-heading-anchor" href="#Multiple-Labels">Multiple Labels</a><a id="Multiple-Labels-1"></a><a class="docs-heading-anchor-permalink" href="#Multiple-Labels" title="Permalink"></a></h4><p>You can combine multiple labels on a PR:</p><ul><li><code>run bench core-all</code> + <code>run bench minimal-ubuntu-latest</code> → runs all <code>core-*</code> benchmarks + <code>minimal-ubuntu-latest</code></li><li><code>run bench core-moonshot</code> + <code>run bench gpu-all</code> → runs <code>core-moonshot</code> + all <code>gpu-*</code> benchmarks</li></ul><h4 id="Automatic-Discovery"><a class="docs-heading-anchor" href="#Automatic-Discovery">Automatic Discovery</a><a id="Automatic-Discovery-1"></a><a class="docs-heading-anchor-permalink" href="#Automatic-Discovery" title="Permalink"></a></h4><p>The system is <strong>completely generic</strong> - no hardcoded family names:</p><ul><li>Add benchmarks with any prefix (e.g., <code>perf-*</code>, <code>stress-*</code>, <code>validation-*</code>)</li><li>Create corresponding group labels (e.g., <code>run bench perf-all</code>)</li><li>The orchestrator automatically detects and processes them</li></ul><h3 id="Configuration-File"><a class="docs-heading-anchor" href="#Configuration-File">Configuration File</a><a id="Configuration-File-1"></a><a class="docs-heading-anchor-permalink" href="#Configuration-File" title="Permalink"></a></h3><p>The <code>benchmarks/benchmarks-config.json</code> file is the <strong>single source of truth</strong>:</p><ul><li>Orchestrator reads it to discover available benchmarks</li><li>Individual workflows read it to get their configuration</li><li>Easy to maintain and validate</li><li>Can be extended with additional metadata</li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="api/private.html">« Private</a><a class="docs-footer-nextpage" href="documentation_process.html">Documentation Process »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Thursday 8 January 2026 16:05">Thursday 8 January 2026</span>. Using Julia version 1.12.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
