<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Development Guidelines ¬∑ CTBenchmarks</title><meta name="title" content="Development Guidelines ¬∑ CTBenchmarks"/><meta property="og:title" content="Development Guidelines ¬∑ CTBenchmarks"/><meta property="twitter:title" content="Development Guidelines ¬∑ CTBenchmarks"/><meta name="description" content="Documentation for CTBenchmarks."/><meta property="og:description" content="Documentation for CTBenchmarks."/><meta property="twitter:description" content="Documentation for CTBenchmarks."/><script data-outdated-warner src="assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="search_index.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script><link href="https://control-toolbox.org/assets/css/documentation.css" rel="stylesheet" type="text/css"/><script src="https://control-toolbox.org/assets/js/documentation.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="index.html">CTBenchmarks</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="index.html">Introduction</a></li><li><a class="tocitem" href="benchmark-core.html">Core benchmark</a></li><li><a class="tocitem" href="api.html">API</a></li><li class="is-active"><a class="tocitem" href="dev.html">Development Guidelines</a><ul class="internal"><li><a class="tocitem" href="#Overview"><span>Overview</span></a></li><li><a class="tocitem" href="#Step-by-Step-Guide"><span>Step-by-Step Guide</span></a></li><li><a class="tocitem" href="#Testing-Your-Benchmark"><span>Testing Your Benchmark</span></a></li><li><a class="tocitem" href="#Troubleshooting"><span>Troubleshooting</span></a></li><li><a class="tocitem" href="#Examples"><span>Examples</span></a></li><li><a class="tocitem" href="#How-the-Orchestrator-Works"><span>How the Orchestrator Works</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href="dev.html">Development Guidelines</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="dev.html">Development Guidelines</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/control-toolbox/CTBenchmarks.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands">ÔÇõ</span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Development-Guidelines"><a class="docs-heading-anchor" href="#Development-Guidelines">Development Guidelines</a><a id="Development-Guidelines-1"></a><a class="docs-heading-anchor-permalink" href="#Development-Guidelines" title="Permalink"></a></h1><p>This guide explains how to add a new benchmark to the CTBenchmarks.jl pipeline.</p><h2 id="Overview"><a class="docs-heading-anchor" href="#Overview">Overview</a><a id="Overview-1"></a><a class="docs-heading-anchor-permalink" href="#Overview" title="Permalink"></a></h2><p>Adding a new benchmark involves creating several components:</p><ol><li><strong>JSON configuration entry</strong> ‚≠ê <em>Simple</em> - Add benchmark config to JSON file (<strong>1 entry to add</strong>)</li><li><strong>Benchmark script</strong> ‚≠ê <em>Simple</em> - Julia script that runs the benchmark</li><li><strong>GitHub label</strong> ‚≠ê <em>Simple</em> - Label to trigger the benchmark on pull requests (manual step on GitHub)</li><li><strong>Individual workflow</strong> ‚≠ê <em>Optional</em> - Workflow for manual testing (reads from JSON)</li><li><strong>Documentation page</strong> ‚≠ê <em>Optional</em> - Display benchmark results in the documentation</li></ol><div class="admonition is-success" id="Estimated-Time-93f652ba398fcdaa"><header class="admonition-header">Estimated Time<a class="admonition-anchor" href="#Estimated-Time-93f652ba398fcdaa" title="Permalink"></a></header><div class="admonition-body"><ul><li>Step 1 (JSON): ~2 minutes</li><li>Step 2 (Script): ~5-10 minutes</li><li>Step 3 (Label): ~1 minute</li><li>Step 4 (Optional workflow): ~5 minutes</li><li>Step 5 (Optional docs): ~10 minutes</li></ul></div></div><div class="admonition is-category-success" id="Key-Improvement-e6c667e04c49a1ad"><header class="admonition-header">Key Improvement<a class="admonition-anchor" href="#Key-Improvement-e6c667e04c49a1ad" title="Permalink"></a></header><div class="admonition-body"><p>The orchestrator now uses a <strong>JSON configuration file</strong> and <strong>matrix strategy</strong>. Adding a benchmark requires modifying only <strong>one JSON entry</strong> instead of multiple workflow files!</p></div></div><h2 id="Step-by-Step-Guide"><a class="docs-heading-anchor" href="#Step-by-Step-Guide">Step-by-Step Guide</a><a id="Step-by-Step-Guide-1"></a><a class="docs-heading-anchor-permalink" href="#Step-by-Step-Guide" title="Permalink"></a></h2><h3 id="1.-Add-Configuration-to-JSON"><a class="docs-heading-anchor" href="#1.-Add-Configuration-to-JSON">1. Add Configuration to JSON</a><a id="1.-Add-Configuration-to-JSON-1"></a><a class="docs-heading-anchor-permalink" href="#1.-Add-Configuration-to-JSON" title="Permalink"></a></h3><p>Edit <code>benchmarks/benchmarks-config.json</code> and add your benchmark configuration:</p><pre><code class="language-json hljs">{
  &quot;benchmarks&quot;: [
    {
      &quot;id&quot;: &quot;your-benchmark-id&quot;,
      &quot;julia_version&quot;: &quot;1.11&quot;,
      &quot;julia_arch&quot;: &quot;x64&quot;,
      &quot;runs_on&quot;: &quot;ubuntu-latest&quot;,
      &quot;runner&quot;: &quot;github&quot;
    }
  ]
}</code></pre><p><strong>Configuration fields:</strong></p><ul><li><p><strong><code>id</code></strong> (required): Unique identifier for the benchmark (kebab-case)</p><ul><li>Convention: <code>{family}-{runner}</code> (e.g., <code>core-ubuntu-latest</code>, <code>core-moonshot</code>)</li><li>Used as script filename: <code>benchmarks/{id}.jl</code></li><li>Used in label: <code>run bench {id}</code></li></ul></li><li><p><strong><code>julia_version</code></strong> (required): Julia version to use (e.g., <code>&quot;1.11&quot;</code>)</p></li><li><p><strong><code>julia_arch</code></strong> (required): Architecture (typically <code>&quot;x64&quot;</code>)</p></li><li><p><strong><code>runs_on</code></strong> (required): GitHub runner specification</p><ul><li>For standard runners: <code>&quot;ubuntu-latest&quot;</code></li><li>For self-hosted: <code>&quot;[\&quot;self-hosted\&quot;, \&quot;Linux\&quot;, \&quot;gpu\&quot;, \&quot;cuda\&quot;, \&quot;cuda12\&quot;]&quot;</code></li></ul></li><li><p><strong><code>runner</code></strong> (required): Runner type for caching strategy</p><ul><li><code>&quot;github&quot;</code> for standard GitHub runners (uses <code>julia-actions/cache</code>)</li><li><code>&quot;self-hosted&quot;</code> for self-hosted runners (uses <code>actions/cache</code> for artifacts only)</li></ul></li></ul><p><strong>Examples:</strong></p><pre><code class="language-json hljs">// Standard GitHub runner
{
  &quot;id&quot;: &quot;core-ubuntu-latest&quot;,
  &quot;julia_version&quot;: &quot;1.11&quot;,
  &quot;julia_arch&quot;: &quot;x64&quot;,
  &quot;runs_on&quot;: &quot;ubuntu-latest&quot;,
  &quot;runner&quot;: &quot;github&quot;
}

// Self-hosted GPU runner
{
  &quot;id&quot;: &quot;core-moonshot&quot;,
  &quot;julia_version&quot;: &quot;1.11&quot;,
  &quot;julia_arch&quot;: &quot;x64&quot;,
  &quot;runs_on&quot;: &quot;[\&quot;self-hosted\&quot;, \&quot;Linux\&quot;, \&quot;gpu\&quot;, \&quot;cuda\&quot;, \&quot;cuda12\&quot;]&quot;,
  &quot;runner&quot;: &quot;self-hosted&quot;
}</code></pre><h3 id="2.-Create-the-Benchmark-Script"><a class="docs-heading-anchor" href="#2.-Create-the-Benchmark-Script">2. Create the Benchmark Script</a><a id="2.-Create-the-Benchmark-Script-1"></a><a class="docs-heading-anchor-permalink" href="#2.-Create-the-Benchmark-Script" title="Permalink"></a></h3><p>Create a new Julia script in the <code>benchmarks/</code> directory with the filename <code>{id}.jl</code>:</p><p><strong>Important</strong>: The script filename must <strong>exactly match</strong> the <code>id</code> in the JSON configuration.</p><p><strong>Example</strong>: For <code>&quot;id&quot;: &quot;core-ubuntu-latest&quot;</code>, create <code>benchmarks/core-ubuntu-latest.jl</code></p><pre><code class="language-julia hljs"># Benchmark script for &lt;id&gt;
# Setup (Pkg.activate, instantiate, update, using CTBenchmarks) is handled by the workflow

function main()
    project_dir = normpath(@__DIR__, &quot;..&quot;)
    outpath = joinpath(project_dir, &quot;docs&quot;, &quot;src&quot;, &quot;assets&quot;, &quot;benchmarks&quot;, &quot;&lt;id&gt;&quot;)
    CTBenchmarks.benchmark(;
        outpath = outpath,
        problems = [:problem1, :problem2, ...],
        solver_models = [:solver =&gt; [:model1, :model2]],
        grid_sizes = [100, 500, 1000],
        disc_methods = [:trapeze],
        tol = 1e-6,
        ipopt_mu_strategy = &quot;adaptive&quot;,
        print_trace = false,
        max_iter = 1000,
        max_wall_time = 500.0
    )
    return outpath
end

main()</code></pre><p><strong>Key points:</strong></p><ul><li><strong>Setup code is handled by the workflow</strong> - No need to include <code>using Pkg</code>, <code>Pkg.activate()</code>, <code>Pkg.instantiate()</code>, <code>Pkg.update()</code>, or <code>using CTBenchmarks</code> in your script. The GitHub Actions workflow handles all environment setup automatically.</li><li><strong>All parameters are required</strong> - the <code>benchmark</code> function has no optional arguments</li><li><strong>The <code>main()</code> function is crucial</strong> - it must:<ul><li>Take no arguments</li><li>Return the output path where files are saved</li></ul></li><li>The <code>benchmark</code> function generates a JSON file (<code>data.json</code>) in the specified <code>outpath</code></li><li><strong>TOML files are copied by the workflow</strong> - <code>Project.toml</code> and <code>Manifest.toml</code> are automatically copied to the output directory by the GitHub Actions workflow to ensure reproducibility</li><li>The output directory follows the pattern <code>docs/src/assets/benchmarks/{id}</code></li><li><strong>Available problems:</strong> The list of problems you can choose is available in the <a href="https://control-toolbox.org/OptimalControlProblems.jl/stable/problems_browser.html">OptimalControlProblems.jl documentation</a></li><li><strong>For local testing:</strong> See <code>benchmarks/local.jl</code> for an example that includes the setup code needed to run benchmarks locally</li></ul><h3 id="2.-Automatic-Workflow-Execution"><a class="docs-heading-anchor" href="#2.-Automatic-Workflow-Execution">2. Automatic Workflow Execution</a><a id="2.-Automatic-Workflow-Execution-1"></a><a class="docs-heading-anchor-permalink" href="#2.-Automatic-Workflow-Execution" title="Permalink"></a></h3><p><strong>Good news!</strong> You don&#39;t need to create a workflow file manually. The orchestrator automatically runs your benchmark based on the JSON configuration using a matrix strategy.</p><p>When you add a label to a PR (e.g., <code>run bench your-benchmark-id</code>), the orchestrator:</p><ol><li>Reads <code>benchmarks/benchmarks-config.json</code></li><li>Finds your benchmark configuration</li><li>Calls the reusable workflow with the correct parameters</li><li>Constructs the script path as <code>benchmarks/{id}.jl</code></li></ol><p><strong>Everything is automatic!</strong> ‚ú®</p><h3 id="3.-Create-the-GitHub-Label"><a class="docs-heading-anchor" href="#3.-Create-the-GitHub-Label">3. Create the GitHub Label</a><a id="3.-Create-the-GitHub-Label-1"></a><a class="docs-heading-anchor-permalink" href="#3.-Create-the-GitHub-Label" title="Permalink"></a></h3><p>On GitHub, create a new label for your benchmark:</p><ol><li>Go to your repository ‚Üí <strong>Issues</strong> ‚Üí <strong>Labels</strong></li><li>Click <strong>New label</strong></li><li>Name: <code>run bench {id}</code> where <code>{id}</code> matches your JSON configuration<ul><li>Example: <code>run bench core-ubuntu-latest</code></li><li>Example: <code>run bench core-moonshot</code></li><li><strong>Important</strong>: Use the exact benchmark ID from JSON</li></ul></li><li>Choose a color and description</li><li>Click <strong>Create label</strong></li></ol><p><strong>Label types:</strong></p><ol><li><p><strong>Individual labels</strong> - Trigger a specific benchmark:</p><ul><li>Format: <code>run bench {id}</code></li><li>Example: <code>run bench core-moonshot</code></li><li>Example: <code>run bench minimal-ubuntu-latest</code></li></ul></li><li><p><strong>Group labels</strong> - Trigger all benchmarks with a common prefix:</p><ul><li>Format: <code>run bench {prefix}-all</code></li><li>Example: <code>run bench core-all</code> ‚Üí runs all <code>core-*</code> benchmarks</li><li>Example: <code>run bench minimal-all</code> ‚Üí runs all <code>minimal-*</code> benchmarks</li><li>Example: <code>run bench gpu-all</code> ‚Üí runs all <code>gpu-*</code> benchmarks</li></ul></li></ol><p><strong>Naming convention for benchmark families:</strong></p><p>To use group labels effectively, follow this naming convention:</p><ul><li><code>{family}-{runner}</code> format (e.g., <code>core-ubuntu-latest</code>, <code>core-moonshot</code>)</li><li>All benchmarks in the same family share the same prefix</li><li>Group label <code>run bench {family}-all</code> will run all benchmarks in that family</li></ul><p><strong>Examples:</strong></p><ul><li><code>core-ubuntu-latest</code>, <code>core-moonshot</code>, <code>core-mothra</code> ‚Üí <code>run bench core-all</code></li><li><code>minimal-ubuntu-latest</code>, <code>minimal-macos</code> ‚Üí <code>run bench minimal-all</code></li><li><code>gpu-cuda12</code>, <code>gpu-cuda13</code> ‚Üí <code>run bench gpu-all</code></li></ul><h3 id="4.-(Optional)-Create-Individual-Workflow"><a class="docs-heading-anchor" href="#4.-(Optional)-Create-Individual-Workflow">4. (Optional) Create Individual Workflow</a><a id="4.-(Optional)-Create-Individual-Workflow-1"></a><a class="docs-heading-anchor-permalink" href="#4.-(Optional)-Create-Individual-Workflow" title="Permalink"></a></h3><div class="admonition is-info" id="Optional-Step-a032687e6f5fcaa"><header class="admonition-header">Optional Step<a class="admonition-anchor" href="#Optional-Step-a032687e6f5fcaa" title="Permalink"></a></header><div class="admonition-body"><p>Individual workflows are <strong>optional</strong>. The orchestrator will automatically run your benchmark based on the JSON configuration. Individual workflows are useful for:</p><ul><li>Manual testing via <code>workflow_dispatch</code></li><li>Running a specific benchmark without the orchestrator</li><li>Debugging</li></ul></div></div><p>Create <code>.github/workflows/benchmark-{id}.yml</code>:</p><pre><code class="language-yaml hljs">name: Benchmark {Name}

on:
  workflow_call:
  workflow_dispatch:

permissions:
  contents: write
  pull-requests: write

jobs:
  load-config:
    runs-on: ubuntu-latest
    outputs:
      config: ${{ steps.get-config.outputs.config }}
    steps:
      - uses: actions/checkout@v5
      - name: Get benchmark config
        id: get-config
        run: |
          CONFIG=$(jq -c &#39;.benchmarks[] | select(.id == &quot;{id}&quot;)&#39; benchmarks/benchmarks-config.json)
          echo &quot;config=$CONFIG&quot; &gt;&gt; $GITHUB_OUTPUT
  
  bench:
    needs: load-config
    uses: ./.github/workflows/benchmark-reusable.yml
    with:
      script_path: benchmarks/${{ fromJSON(needs.load-config.outputs.config).id }}.jl
      julia_version: ${{ fromJSON(needs.load-config.outputs.config).julia_version }}
      julia_arch: ${{ fromJSON(needs.load-config.outputs.config).julia_arch }}
      runs_on: ${{ fromJSON(needs.load-config.outputs.config).runs_on }}
      runner: ${{ fromJSON(needs.load-config.outputs.config).runner }}</code></pre><p><strong>Key features:</strong></p><ul><li><strong>Reads configuration from JSON</strong> - Single source of truth</li><li><strong>Uses ID to construct script path</strong> - <code>benchmarks/${{ fromJSON(...).id }}.jl</code> ensures consistency</li><li><strong>Can be triggered manually</strong> via <code>workflow_dispatch</code> for testing</li><li><strong>Can be called by orchestrator</strong> via <code>workflow_call</code></li><li><strong>No hardcoded values</strong> - Everything comes from JSON configuration</li></ul><h3 id="5.-Create-Documentation-Page-(Optional)"><a class="docs-heading-anchor" href="#5.-Create-Documentation-Page-(Optional)">5. Create Documentation Page (Optional)</a><a id="5.-Create-Documentation-Page-(Optional)-1"></a><a class="docs-heading-anchor-permalink" href="#5.-Create-Documentation-Page-(Optional)" title="Permalink"></a></h3><p>If you want to display results in the documentation, create <code>docs/src/benchmark-&lt;name&gt;.md.template</code>:</p><pre><code class="language-markdown hljs"># &lt;Name&gt; Benchmark

```@setup BENCH_&lt;NAME&gt;
include(joinpath(@__DIR__, &quot;assets&quot;, &quot;utils.jl&quot;))

const BENCH_DIR = &quot;benchmark-&lt;name&gt;&quot;
const BENCH_DATA = _read_benchmark_json(joinpath(@__DIR__, &quot;assets&quot;, BENCH_DIR, &quot;data.json&quot;))
```

## Description

Brief description of your benchmark configuration.

**Benchmark Configuration:**

- **Solvers:** List of solvers
- **Models:** List of models
- **Grid sizes:** Discretisation points
- **Tolerance:** 1e-6
- **Limits:** Max iterations and wall time

### üñ•Ô∏è Environment

&lt;!-- INCLUDE_ENVIRONMENT:
BENCH_DATA = BENCH_DATA
BENCH_DIR = BENCH_DIR
ENV_NAME = BENCH_&lt;NAME&gt;
--&gt;

### üìä Results

```@example BENCH_&lt;NAME&gt;
_print_results(BENCH_DATA) # hide
nothing # hide
```</code></pre><p>Then add it to <code>docs/make.jl</code>:</p><pre><code class="language-julia hljs">pages = [
    &quot;Introduction&quot; =&gt; &quot;index.md&quot;,
    &quot;Core benchmark&quot; =&gt; &quot;benchmark-core.md&quot;,
    &quot;&lt;Name&gt; Benchmark&quot; =&gt; &quot;benchmark-&lt;name&gt;.md&quot;,
    &quot;API&quot; =&gt; &quot;api.md&quot;,
    &quot;Development Guidelines&quot; =&gt; &quot;dev.md&quot;,
]</code></pre><h2 id="Testing-Your-Benchmark"><a class="docs-heading-anchor" href="#Testing-Your-Benchmark">Testing Your Benchmark</a><a id="Testing-Your-Benchmark-1"></a><a class="docs-heading-anchor-permalink" href="#Testing-Your-Benchmark" title="Permalink"></a></h2><ol><li><strong>Local testing:</strong> Run your script locally to verify it works</li><li><strong>Push changes:</strong> Commit and push all files</li><li><strong>Create PR:</strong> Open a pull request</li><li><strong>Add label:</strong> Add the <code>run bench &lt;name&gt;</code> label to trigger the workflow</li><li><strong>Monitor:</strong> Check the Actions tab to monitor execution</li></ol><h2 id="Troubleshooting"><a class="docs-heading-anchor" href="#Troubleshooting">Troubleshooting</a><a id="Troubleshooting-1"></a><a class="docs-heading-anchor-permalink" href="#Troubleshooting" title="Permalink"></a></h2><p><strong>Cache issues on self-hosted runners:</strong></p><ul><li>Ensure <code>runner: &#39;self-hosted&#39;</code> is set in your workflow</li><li>The reusable workflow uses <code>actions/cache</code> for artifacts only on self-hosted runners</li><li>If you see slow cache operations on self-hosted runners, verify the <code>runner</code> parameter is set correctly</li><li>Standard runners should NOT have the <code>runner</code> parameter (let it default to use <code>julia-actions/cache</code>)</li></ul><p><strong>Workflow not triggering:</strong></p><ul><li>Verify the label name matches exactly in the orchestrator</li><li>Check that the orchestrator&#39;s guard job includes your benchmark in outputs</li></ul><p><strong>Benchmark script fails:</strong></p><ul><li>Check Julia version compatibility</li><li>Verify all dependencies are available on the target runner</li><li>Review the benchmark function parameters</li></ul><h2 id="Examples"><a class="docs-heading-anchor" href="#Examples">Examples</a><a id="Examples-1"></a><a class="docs-heading-anchor-permalink" href="#Examples" title="Permalink"></a></h2><h3 id="Example-1:-Core-Moonshot-Benchmark-(CUDA-12)"><a class="docs-heading-anchor" href="#Example-1:-Core-Moonshot-Benchmark-(CUDA-12)">Example 1: Core Moonshot Benchmark (CUDA 12)</a><a id="Example-1:-Core-Moonshot-Benchmark-(CUDA-12)-1"></a><a class="docs-heading-anchor-permalink" href="#Example-1:-Core-Moonshot-Benchmark-(CUDA-12)" title="Permalink"></a></h3><p>A complete GPU benchmark using CUDA 12:</p><ul><li><strong>Script</strong>: <code>benchmarks/core-moonshot.jl</code></li><li><strong>Workflow</strong>: <code>.github/workflows/benchmark-core-moonshot.yml</code></li><li><strong>Label</strong>: <code>run bench core moonshot</code></li><li><strong>Runner</strong>: <code>[&quot;self-hosted&quot;, &quot;Linux&quot;, &quot;gpu&quot;, &quot;cuda&quot;, &quot;cuda12&quot;]</code></li><li><strong>Documentation</strong>: <code>docs/src/benchmark-core.md.template</code></li></ul><h3 id="Example-2:-Core-Mothra-Benchmark-(CUDA-13)"><a class="docs-heading-anchor" href="#Example-2:-Core-Mothra-Benchmark-(CUDA-13)">Example 2: Core Mothra Benchmark (CUDA 13)</a><a id="Example-2:-Core-Mothra-Benchmark-(CUDA-13)-1"></a><a class="docs-heading-anchor-permalink" href="#Example-2:-Core-Mothra-Benchmark-(CUDA-13)" title="Permalink"></a></h3><p>A GPU benchmark identical to Moonshot but using CUDA 13 to compare performance:</p><ul><li><p><strong>JSON entry</strong>: Added to <code>benchmarks/benchmarks-config.json</code></p><p><code>json   {   &quot;id&quot;: &quot;core-mothra&quot;,   &quot;julia_version&quot;: &quot;1.11&quot;,   &quot;julia_arch&quot;: &quot;x64&quot;,   &quot;runs_on&quot;: &quot;[\&quot;self-hosted\&quot;, \&quot;Linux\&quot;, \&quot;gpu\&quot;, \&quot;cuda\&quot;, \&quot;cuda13\&quot;]&quot;,   &quot;runner&quot;: &quot;self-hosted&quot;   }</code></p></li><li><p><strong>Script</strong>: <code>benchmarks/core-mothra.jl</code></p><ul><li>Only difference: <code>outpath</code> points to <code>docs/src/assets/benchmarks/core-mothra</code></li></ul></li><li><p><strong>Label</strong>: <code>run bench core-mothra</code></p></li><li><p><strong>Workflow</strong> (optional): <code>.github/workflows/benchmark-core-mothra.yml</code> reads from JSON</p></li></ul><p>This example demonstrates how to create a variant of an existing benchmark to test different hardware configurations.</p><h2 id="How-the-Orchestrator-Works"><a class="docs-heading-anchor" href="#How-the-Orchestrator-Works">How the Orchestrator Works</a><a id="How-the-Orchestrator-Works-1"></a><a class="docs-heading-anchor-permalink" href="#How-the-Orchestrator-Works" title="Permalink"></a></h2><h3 id="Matrix-Strategy"><a class="docs-heading-anchor" href="#Matrix-Strategy">Matrix Strategy</a><a id="Matrix-Strategy-1"></a><a class="docs-heading-anchor-permalink" href="#Matrix-Strategy" title="Permalink"></a></h3><p>The orchestrator uses a <strong>matrix strategy</strong> to dynamically call benchmarks:</p><ol><li><strong>Guard job</strong> reads <code>benchmarks/benchmarks-config.json</code></li><li>Based on PR labels, it builds a JSON array of selected benchmarks</li><li><strong>Benchmark job</strong> uses matrix to iterate over selected benchmarks</li><li>Each matrix iteration calls <code>benchmark-reusable.yml</code> with the appropriate parameters</li></ol><p><strong>Benefits:</strong></p><ul><li>No need to declare individual jobs for each benchmark</li><li>Adding a benchmark requires only JSON modification</li><li>All benchmarks run in parallel (matrix strategy)</li><li>Consistent behavior across all benchmarks</li></ul><h3 id="Label-System"><a class="docs-heading-anchor" href="#Label-System">Label System</a><a id="Label-System-1"></a><a class="docs-heading-anchor-permalink" href="#Label-System" title="Permalink"></a></h3><p>The orchestrator supports two types of labels with <strong>automatic prefix detection</strong>:</p><h4 id="Individual-Labels"><a class="docs-heading-anchor" href="#Individual-Labels">Individual Labels</a><a id="Individual-Labels-1"></a><a class="docs-heading-anchor-permalink" href="#Individual-Labels" title="Permalink"></a></h4><ul><li><strong>Format</strong>: <code>run bench {id}</code></li><li><strong>Behavior</strong>: Runs the specific benchmark with that exact ID</li><li><strong>Examples</strong>:<ul><li><code>run bench core-ubuntu-latest</code> ‚Üí runs only <code>core-ubuntu-latest</code></li><li><code>run bench minimal-macos</code> ‚Üí runs only <code>minimal-macos</code></li></ul></li></ul><h4 id="Group-Labels-(Generic)"><a class="docs-heading-anchor" href="#Group-Labels-(Generic)">Group Labels (Generic)</a><a id="Group-Labels-(Generic)-1"></a><a class="docs-heading-anchor-permalink" href="#Group-Labels-(Generic)" title="Permalink"></a></h4><ul><li><strong>Format</strong>: <code>run bench {prefix}-all</code></li><li><strong>Behavior</strong>: Automatically runs <strong>all</strong> benchmarks whose ID starts with <code>{prefix}-</code></li><li><strong>How it works</strong>:<ol><li>The orchestrator extracts the prefix from the label (e.g., <code>core</code> from <code>run bench core-all</code>)</li><li>It scans all benchmark IDs in the JSON</li><li>It selects all benchmarks matching the pattern <code>{prefix}-*</code></li></ol></li><li><strong>Examples</strong>:<ul><li><code>run bench core-all</code> ‚Üí runs <code>core-ubuntu-latest</code>, <code>core-moonshot</code>, <code>core-mothra</code></li><li><code>run bench minimal-all</code> ‚Üí runs <code>minimal-ubuntu-latest</code>, <code>minimal-macos</code></li><li><code>run bench gpu-all</code> ‚Üí runs <code>gpu-cuda12</code>, <code>gpu-cuda13</code></li></ul></li></ul><h4 id="Multiple-Labels"><a class="docs-heading-anchor" href="#Multiple-Labels">Multiple Labels</a><a id="Multiple-Labels-1"></a><a class="docs-heading-anchor-permalink" href="#Multiple-Labels" title="Permalink"></a></h4><p>You can combine multiple labels on a PR:</p><ul><li><code>run bench core-all</code> + <code>run bench minimal-ubuntu-latest</code> ‚Üí runs all <code>core-*</code> benchmarks + <code>minimal-ubuntu-latest</code></li><li><code>run bench core-moonshot</code> + <code>run bench gpu-all</code> ‚Üí runs <code>core-moonshot</code> + all <code>gpu-*</code> benchmarks</li></ul><h4 id="Automatic-Discovery"><a class="docs-heading-anchor" href="#Automatic-Discovery">Automatic Discovery</a><a id="Automatic-Discovery-1"></a><a class="docs-heading-anchor-permalink" href="#Automatic-Discovery" title="Permalink"></a></h4><p>The system is <strong>completely generic</strong> - no hardcoded family names:</p><ul><li>Add benchmarks with any prefix (e.g., <code>perf-*</code>, <code>stress-*</code>, <code>validation-*</code>)</li><li>Create corresponding group labels (e.g., <code>run bench perf-all</code>)</li><li>The orchestrator automatically detects and processes them</li></ul><h3 id="Configuration-File"><a class="docs-heading-anchor" href="#Configuration-File">Configuration File</a><a id="Configuration-File-1"></a><a class="docs-heading-anchor-permalink" href="#Configuration-File" title="Permalink"></a></h3><p>The <code>benchmarks/benchmarks-config.json</code> file is the <strong>single source of truth</strong>:</p><ul><li>Orchestrator reads it to discover available benchmarks</li><li>Individual workflows read it to get their configuration</li><li>Easy to maintain and validate</li><li>Can be extended with additional metadata</li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="api.html">¬´ API</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.15.0 on <span class="colophon-date" title="Saturday 8 November 2025 17:01">Saturday 8 November 2025</span>. Using Julia version 1.12.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
