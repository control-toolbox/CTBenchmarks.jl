var documenterSearchIndex = {"docs":
[{"location":"benchmark-core-cpu.html#Core-CPU-Benchmarks","page":"CPU","title":"Core CPU Benchmarks","text":"","category":"section"},{"location":"benchmark-core-cpu.html#Ubuntu-Latest","page":"CPU","title":"Ubuntu Latest","text":"This benchmark suite evaluates optimal control problems on a standard CPU platform using GitHub Actions runners.\n\nBenchmark Configuration:\n\n<div class=\"benchmark-config\" style=\"margin-bottom: 1.5em;\">\n\n_print_config(BENCH_CORE_UBUNTU) # hide\n\n</div>\n\nThis configuration focuses on CPU-based solvers and provides a comprehensive comparison across different modelling frameworks.\n\nnote: Note\nThe linear solver is MUMPS for all experiments.","category":"section"},{"location":"benchmark-core-cpu.html#Environment","page":"CPU","title":"üñ•Ô∏è Environment","text":"_basic_metadata(BENCH_CORE_UBUNTU) # hide\n\n_downloads_toml(BENCH_CORE_UBUNTU) # hide\n\n<details style=\"margin-bottom: 0.5em; margin-top: 0.5em;\"><summary>‚ÑπÔ∏è Version info</summary>\n\n_bench_data(BENCH_CORE_UBUNTU) # hide\n\n</details>\n\n<details style=\"margin-bottom: 0.5em;\"><summary>üì¶ Package status</summary>\n\n_package_status(BENCH_CORE_UBUNTU) # hide\n\n</details>\n\n<details style=\"margin-bottom: 0.5em;\"><summary>üìö Complete manifest</summary>\n\n_complete_manifest(BENCH_CORE_UBUNTU) # hide\n\n</details>","category":"section"},{"location":"benchmark-core-cpu.html#Results","page":"CPU","title":"üìä Results","text":"_plot_performance_profiles(BENCH_CORE_UBUNTU) # hide\n\n_print_benchmark_log(BENCH_CORE_UBUNTU) # hide","category":"section"},{"location":"benchmark-core-cpu.html#Moonshot","page":"CPU","title":"Moonshot","text":"This benchmark suite evaluates optimal control problems on self-hosted CPU hardware.\n\nBenchmark Configuration:\n\n<div class=\"benchmark-config\" style=\"margin-bottom: 1.5em;\">\n\n_print_config(BENCH_CORE_MOONSHOT_CPU) # hide\n\n</div>\n\nThis configuration tests larger grid sizes on dedicated hardware, comparing performance with the GitHub Actions runner.\n\nnote: Note\nThe linear solver is MUMPS for all experiments.","category":"section"},{"location":"benchmark-core-cpu.html#Environment-2","page":"CPU","title":"üöÄ Environment","text":"_basic_metadata(BENCH_CORE_MOONSHOT_CPU) # hide\n\n_downloads_toml(BENCH_CORE_MOONSHOT_CPU) # hide\n\n<details style=\"margin-bottom: 0.5em; margin-top: 0.5em;\"><summary>‚ÑπÔ∏è Version info</summary>\n\n_bench_data(BENCH_CORE_MOONSHOT_CPU) # hide\n\n</details>\n\n<details style=\"margin-bottom: 0.5em;\"><summary>üì¶ Package status</summary>\n\n_package_status(BENCH_CORE_MOONSHOT_CPU) # hide\n\n</details>\n\n<details style=\"margin-bottom: 0.5em;\"><summary>üìö Complete manifest</summary>\n\n_complete_manifest(BENCH_CORE_MOONSHOT_CPU) # hide\n\n</details>","category":"section"},{"location":"benchmark-core-cpu.html#Results-2","page":"CPU","title":"‚ö° Results","text":"_plot_performance_profiles(BENCH_CORE_MOONSHOT_CPU) # hide\n\n_print_benchmark_log(BENCH_CORE_MOONSHOT_CPU) # hide","category":"section"},{"location":"api/private.html#Private-API","page":"Private","title":"Private API","text":"This page lists the non-exported (internal) symbols of CTBenchmarks.\n\nAccess these symbols with:\n\nimport CTBenchmarks\nCTBenchmarks.<NAME>","category":"section"},{"location":"api/private.html#ITERATION","page":"Private","title":"ITERATION","text":"","category":"section"},{"location":"api/private.html#benchmark_data","page":"Private","title":"benchmark_data","text":"","category":"section"},{"location":"api/private.html#build_payload","page":"Private","title":"build_payload","text":"","category":"section"},{"location":"api/private.html#costate_multiplier","page":"Private","title":"costate_multiplier","text":"","category":"section"},{"location":"api/private.html#create_jump_layout","page":"Private","title":"create_jump_layout","text":"","category":"section"},{"location":"api/private.html#eval","page":"Private","title":"eval","text":"","category":"section"},{"location":"api/private.html#filter_models_for_backend","page":"Private","title":"filter_models_for_backend","text":"","category":"section"},{"location":"api/private.html#format_solution_label","page":"Private","title":"format_solution_label","text":"","category":"section"},{"location":"api/private.html#generate_metadata","page":"Private","title":"generate_metadata","text":"","category":"section"},{"location":"api/private.html#get_color","page":"Private","title":"get_color","text":"","category":"section"},{"location":"api/private.html#get_dimensions","page":"Private","title":"get_dimensions","text":"","category":"section"},{"location":"api/private.html#get_left_margin","page":"Private","title":"get_left_margin","text":"","category":"section"},{"location":"api/private.html#get_marker_indices","page":"Private","title":"get_marker_indices","text":"","category":"section"},{"location":"api/private.html#get_marker_style","page":"Private","title":"get_marker_style","text":"","category":"section"},{"location":"api/private.html#get_solution_dimensions","page":"Private","title":"get_solution_dimensions","text":"","category":"section"},{"location":"api/private.html#include","page":"Private","title":"include","text":"","category":"section"},{"location":"api/private.html#is_cuda_on","page":"Private","title":"is_cuda_on","text":"","category":"section"},{"location":"api/private.html#plot_jump_group","page":"Private","title":"plot_jump_group","text":"","category":"section"},{"location":"api/private.html#plot_jump_solution","page":"Private","title":"plot_jump_solution","text":"","category":"section"},{"location":"api/private.html#plot_jump_solution!","page":"Private","title":"plot_jump_solution!","text":"","category":"section"},{"location":"api/private.html#plot_ocp_group","page":"Private","title":"plot_ocp_group","text":"","category":"section"},{"location":"api/private.html#plot_ocp_solution","page":"Private","title":"plot_ocp_solution","text":"","category":"section"},{"location":"api/private.html#plot_ocp_solution!","page":"Private","title":"plot_ocp_solution!","text":"","category":"section"},{"location":"api/private.html#plot_solution_comparison","page":"Private","title":"plot_solution_comparison","text":"","category":"section"},{"location":"api/private.html#prettymemory","page":"Private","title":"prettymemory","text":"","category":"section"},{"location":"api/private.html#prettytime","page":"Private","title":"prettytime","text":"","category":"section"},{"location":"api/private.html#print_benchmark_line","page":"Private","title":"print_benchmark_line","text":"","category":"section"},{"location":"api/private.html#save_json","page":"Private","title":"save_json","text":"","category":"section"},{"location":"api/private.html#set_print_level","page":"Private","title":"set_print_level","text":"","category":"section"},{"location":"api/private.html#solve_and_extract_data","page":"Private","title":"solve_and_extract_data","text":"","category":"section"},{"location":"api/private.html#strip_benchmark_value","page":"Private","title":"strip_benchmark_value","text":"","category":"section"},{"location":"api/private.html#CTBenchmarks.benchmark_data","page":"Private","title":"CTBenchmarks.benchmark_data","text":"benchmark_data(;\n    problems,\n    solver_models,\n    grid_sizes,\n    disc_methods,\n    tol,\n    ipopt_mu_strategy,\n    print_trace\n    max_iter,\n    max_wall_time,\n    grid_size_max_cpu\n) -> DataFrame\n\nRun benchmarks on optimal control problems and return results as a DataFrame.\n\nFor each combination of problem, solver, model, and grid size, this function:\n\nSets up and solves the optimization problem\nCaptures timing and memory statistics using @btimed or CUDA.@timed\nExtracts solver statistics (objective value, iterations)\nStores all data in a DataFrame row\n\nArguments\n\nproblems: Vector of problem names (Symbols)\nsolver_models: Vector of Pairs mapping solver => models (e.g., [:ipopt => [:jump, :adnlp], :madnlp => [:exa, :exa_gpu]])\ngrid_sizes: Vector of grid sizes (Int)\ndisc_methods: Vector of discretization methods (Symbols)\ntol: Solver tolerance (Float64)\nipopt_mu_strategy: Mu strategy for Ipopt (String)\nprint_trace: Boolean - whether to print solver output (for debugging)\nmax_iter: Maximum number of iterations (Int)\nmax_wall_time: Maximum wall time in seconds (Float64)\n\nReturns\n\nA DataFrame with columns:\n\nproblem: Symbol - problem name\nsolver: Symbol - solver used (:ipopt or :madnlp)\nmodel: Symbol - model type (:jump, :adnlp, :exa, or :exa_gpu)\ndisc_method: Symbol - discretization method\ngrid_size: Int - number of grid points\ntol: Float64 - solver tolerance\nmu_strategy: Union{String, Missing} - mu strategy for Ipopt (missing for MadNLP)\nprint_level: Any - print level for solver (Int for Ipopt, MadNLP.LogLevels for MadNLP)\nmax_iter: Int - maximum number of iterations\nmax_wall_time: Float64 - maximum wall time in seconds\nbenchmark: NamedTuple - full benchmark object from @btimed or CUDA.@timed\nobjective: Union{Float64, Missing} - objective function value (missing if failed)\niterations: Union{Int, Missing} - number of solver iterations (missing if failed)\nstatus: Any - termination status (type depends on solver/model)\nsuccess: Bool - whether the solve succeeded\ncriterion: Union{String, Missing} - optimization sense (\"min\" or \"max\", missing if failed)\n\n\n\n\n\n","category":"function"},{"location":"api/private.html#CTBenchmarks.build_payload","page":"Private","title":"CTBenchmarks.build_payload","text":"build_payload(results::DataFrame, meta::Dict, config::Dict) -> Dict\n\nCombine benchmark results DataFrame, metadata, and configuration into a JSON-friendly dictionary. The DataFrame is converted to a vector of dictionaries (one per row) for easy JSON serialization and reconstruction.\n\nSolutions are extracted and kept in memory (not serialized to JSON) for later plot generation.\n\n\n\n\n\n","category":"function"},{"location":"api/private.html#CTBenchmarks.create_jump_layout","page":"Private","title":"CTBenchmarks.create_jump_layout","text":"create_jump_layout(n::Int, m::Int, problem::Symbol, grid_size::Int,\n                   state_labels::Vector{<:AbstractString},\n                   control_labels::Vector{<:AbstractString})\n\nCreate a nested plot layout for JuMP solutions with states and costates in two columns, and controls below spanning the full width.\n\nLayout structure:\n\nIndividual plots for each state (with labels), combined vertically into p_state\nIndividual plots for each costate (mirroring state labels), combined vertically into p_costate\npstate and pcostate combined horizontally into pstatecostate\nIndividual plots for each control, combined vertically into p_control\npstatecostate and pcontrol combined vertically into pfinal\n\n\n\n\n\n","category":"function"},{"location":"api/private.html#CTBenchmarks.eval","page":"Private","title":"CTBenchmarks.eval","text":"eval(expr)\n\nEvaluate an expression in the global scope of the containing module. Every Module (except those defined with baremodule) has a private 1-argument definition of eval, which evaluates expressions in that module, for use inside that module.\n\n\n\n\n\n","category":"function"},{"location":"api/private.html#CTBenchmarks.filter_models_for_backend","page":"Private","title":"CTBenchmarks.filter_models_for_backend","text":"filter_models_for_backend(models::Vector{Symbol}, disc_method::Symbol) -> Vector{Symbol}\n\nFilter solver models depending on backend availability and discretization support.\n\nGPU models (ending with _gpu) are kept only if CUDA is available.\nJuMP models are kept only when disc_method == :trapeze.\n\n\n\n\n\n","category":"function"},{"location":"api/private.html#CTBenchmarks.generate_metadata","page":"Private","title":"CTBenchmarks.generate_metadata","text":"generate_metadata() -> Dict{String, String}\n\nReturn metadata about the current environment:\n\ntimestamp (UTC, ISO8601)\njulia_version\nos\nmachine hostname\npkg_status - output of Pkg.status() with ANSI colors\nversioninfo - output of versioninfo() with ANSI colors\npkg_manifest - output of Pkg.status(mode=PKGMODE_MANIFEST) with ANSI colors\n\n\n\n\n\n","category":"function"},{"location":"api/private.html#CTBenchmarks.get_color","page":"Private","title":"CTBenchmarks.get_color","text":"get_color(model::Symbol, solver::Symbol, idx::Int; palette::Vector = [:blue, :red, :green, :orange, :purple, :brown, :pink, :gray])\n\nReturn a consistent color for a given (model, solver) pair.\n\nFixed mapping for known pairs:\n\n(adnlp, ipopt)  => :blue\n(exa,   ipopt)  => :red\n(adnlp, madnlp) => :green\n(exa,   madnlp) => :orange\n(jump,  ipopt)  => :purple\n(jump,  madnlp) => :brown\n\nIf the pair is not in the dictionary, fall back to the provided palette using idx.\n\n\n\n\n\n","category":"function"},{"location":"api/private.html#CTBenchmarks.get_dimensions","page":"Private","title":"CTBenchmarks.get_dimensions","text":"get_dimensions(group::SubDataFrame) -> (Int, Int)\n\nGet state and control dimensions from the first successful solution in group.\n\n\n\n\n\n","category":"function"},{"location":"api/private.html#CTBenchmarks.get_left_margin","page":"Private","title":"CTBenchmarks.get_left_margin","text":"get_left_margin(problem::Symbol)\n\nGet the left margin for plots based on the problem. Returns 5mm for :beam, 20mm for all other problems.\n\n\n\n\n\n","category":"function"},{"location":"api/private.html#CTBenchmarks.get_marker_indices","page":"Private","title":"CTBenchmarks.get_marker_indices","text":"get_marker_indices(idx::Int, card_g::Int, grid_size::Int, marker_interval::Int)\n\nCalculate marker indices with offset to avoid superposition between curves. For curve idx out of cardg curves, the first marker is offset by:     offset = (idx - 1) * markerinterval / card_g\n\nReturns the range of indices for markers.\n\n\n\n\n\n","category":"function"},{"location":"api/private.html#CTBenchmarks.get_marker_style","page":"Private","title":"CTBenchmarks.get_marker_style","text":"get_marker_style(model::Symbol, solver::Symbol, idx::Int, grid_size::Int)\n\nGet marker shape and spacing for a given (model, solver) pair and curve index.\n\nFixed mapping for known (model, solver) pairs:\n\n(adnlp, ipopt)  => :circle\n(exa,   ipopt)  => :square\n(adnlp, madnlp) => :diamond\n(exa,   madnlp) => :utriangle\n(jump,  ipopt)  => :dtriangle\n(jump,  madnlp) => :star5\n(exa_gpu, madnlp) => :hexagon\n\nIf the pair is not in the dictionary, fall back to cycling through a default marker list using idx.\n\nReturns (marker_shape, marker_interval) where marker_interval is calculated as grid_size/M with M = 6 to have approximately M markers per curve.\n\n\n\n\n\n","category":"function"},{"location":"api/private.html#CTBenchmarks.include","page":"Private","title":"CTBenchmarks.include","text":"include([mapexpr::Function,] path::AbstractString)\n\nEvaluate the contents of the input source file in the global scope of the containing module. Every Module (except those defined with baremodule) has a private 1-argument definition of include, which evaluates the file in that module, for use inside that module. Returns the result of the last evaluated expression of the input file. During including, a task-local include path is set to the directory containing the file. Nested calls to include will search relative to that path. This function is typically used to load source interactively, or to combine files in packages that are broken into multiple source files. The argument path is normalized using normpath which will resolve relative path tokens such as .. and convert / to the appropriate path separator.\n\nThe optional first argument mapexpr can be used to transform the included code before it is evaluated: for each parsed expression expr in path, the include function actually evaluates mapexpr(expr).  If it is omitted, mapexpr defaults to identity.\n\nUse Base.include to evaluate a file into another module.\n\nnote: Note\nJulia's syntax lowering recognizes an explicit call to a literal include at top-level and inserts an implicit @Core.latestworld to make any include'd definitions visible to subsequent code. Note however that this recognition is syntactic. I.e. assigning const myinclude = include may require and explicit @Core.latestworld call after myinclude.\n\ncompat: Julia 1.5\nJulia 1.5 is required for passing the mapexpr argument.\n\n\n\n\n\n","category":"function"},{"location":"api/private.html#CTBenchmarks.is_cuda_on","page":"Private","title":"CTBenchmarks.is_cuda_on","text":"is_cuda_on() -> Bool\n\nReturn true if CUDA is functional on this machine.\n\n\n\n\n\n","category":"function"},{"location":"api/private.html#CTBenchmarks.plot_jump_group","page":"Private","title":"CTBenchmarks.plot_jump_group","text":"plot_jump_group(jump_rows::SubDataFrame, plt, color_idx::Int,\n                problem::Symbol, grid_size::Int, n::Int, m::Int)\n\nPlot all JuMP solutions in a group. Creates the layout if plt is nothing.\n\nReturns: (plt, updatedcoloridx)\n\n\n\n\n\n","category":"function"},{"location":"api/private.html#CTBenchmarks.plot_jump_solution","page":"Private","title":"CTBenchmarks.plot_jump_solution","text":"plot_jump_solution(solution, model::Symbol, solver::Symbol, success::Bool, color,\n                   problem::Symbol, grid_size::Int, n::Int, m::Int, criterion)\n\nCreate a fresh layout and plot a single JuMP solution.\n\nReturns: plt\n\n\n\n\n\n","category":"function"},{"location":"api/private.html#CTBenchmarks.plot_jump_solution!","page":"Private","title":"CTBenchmarks.plot_jump_solution!","text":"plot_jump_solution!(plt, solution, model::Symbol, solver::Symbol, success::Bool, color,\n                   n::Int, m::Int, criterion)\n\nAdd a JuMP solution to an existing nested plot layout.\n\nEven with the nested layout, subplots are accessed linearly:\n\nplt[1:n] = states\nplt[n+1:2n] = costates\nplt[2n+1:2n+m] = controls\n\nReturns: plt\n\n\n\n\n\n","category":"function"},{"location":"api/private.html#CTBenchmarks.plot_ocp_group","page":"Private","title":"CTBenchmarks.plot_ocp_group","text":"plot_ocp_group(ocp_rows::SubDataFrame, plt, color_idx::Int, \n               problem::Symbol, grid_size::Int, n::Int, m::Int)\n\nPlot all OptimalControl solutions in a group. Creates the base plot if plt is nothing.\n\nReturns: (plt, updatedcoloridx)\n\n\n\n\n\n","category":"function"},{"location":"api/private.html#CTBenchmarks.plot_ocp_solution","page":"Private","title":"CTBenchmarks.plot_ocp_solution","text":"plot_ocp_solution(solution, model::Symbol, solver::Symbol, success::Bool, color, \n                  problem::Symbol, grid_size::Int, n::Int, m::Int, marker, marker_interval)\n\nCreate a new plot for a single OptimalControl solution with markers for better visibility.\n\nReturns: plt\n\n\n\n\n\n","category":"function"},{"location":"api/private.html#CTBenchmarks.plot_ocp_solution!","page":"Private","title":"CTBenchmarks.plot_ocp_solution!","text":"plot_ocp_solution!(plt, solution, model::Symbol, solver::Symbol, success::Bool, color, n::Int, m::Int, marker, marker_interval)\n\nAdd an OptimalControl solution to an existing plot with markers for better visibility.\n\nReturns: plt\n\n\n\n\n\n","category":"function"},{"location":"api/private.html#CTBenchmarks.plot_solution_comparison","page":"Private","title":"CTBenchmarks.plot_solution_comparison","text":"plot_solution_comparison(group::SubDataFrame, problem::Symbol, grid_size::Int)\n\nCreate a comparison plot for all solutions in a group (same problem, same grid_size).\n\nStrategy:\n\nPlot OptimalControl solutions first (easy with plot!)\nPlot JuMP solutions last (to get proper layout)\n\nLayout: 2 columns for states/costates, then controls below in full width\n\n\n\n\n\n","category":"function"},{"location":"api/private.html#CTBenchmarks.prettymemory","page":"Private","title":"CTBenchmarks.prettymemory","text":"prettymemory(bytes::Integer) -> String\n\nFormat a memory footprint bytes into a human-readable string using binary prefixes (bytes, KiB, MiB, GiB) with two decimal places.\n\n\n\n\n\n","category":"function"},{"location":"api/private.html#CTBenchmarks.prettytime","page":"Private","title":"CTBenchmarks.prettytime","text":"prettytime(t::Real) -> String\n\nFormat a duration t expressed in seconds into a human-readable string with three decimal places and adaptive units (ns, Œºs, ms, s).\n\n\n\n\n\n","category":"function"},{"location":"api/private.html#CTBenchmarks.print_benchmark_line","page":"Private","title":"CTBenchmarks.print_benchmark_line","text":"print_benchmark_line(model::Symbol, stats::NamedTuple)\n\nPrint a formatted line summarizing benchmark statistics for model with colors. Handles both CPU benchmarks (from @btimed) and GPU benchmarks (from CUDA.@timed).\n\nDisplays: time, allocations/memory, objective, iterations, and success status\n\n\n\n\n\n","category":"function"},{"location":"api/private.html#CTBenchmarks.save_json","page":"Private","title":"CTBenchmarks.save_json","text":"save_json(payload::Dict, filepath::AbstractString)\n\nSave a JSON payload to a file. Creates the parent directory if needed. Uses pretty printing for readability. Sanitizes NaN and Inf values to null for JSON compatibility.\n\nArguments\n\npayload::Dict: Benchmark results with metadata\nfilepath::AbstractString: Full path to the output JSON file (including filename)\n\nSolutions are excluded from JSON serialization (kept only in memory).\n\n\n\n\n\n","category":"function"},{"location":"api/private.html#CTBenchmarks.set_print_level","page":"Private","title":"CTBenchmarks.set_print_level","text":"set_print_level(solver::Symbol, print_trace::Bool) -> Int\n\nSet print level based on solver and print_trace flag.\n\n\n\n\n\n","category":"function"},{"location":"api/private.html#CTBenchmarks.solve_and_extract_data","page":"Private","title":"CTBenchmarks.solve_and_extract_data","text":"solve_and_extract_data(problem, solver, model, grid_size, disc_method, \n                      tol, mu_strategy, print_level, max_iter, max_wall_time) -> NamedTuple\n\nSolve an optimal control problem and extract performance and solver statistics.\n\nThis internal helper function handles the solve process and data extraction for different model types (JuMP, adnlp, exa, exa_gpu).\n\nArguments\n\nproblem::Symbol: problem name (e.g., :beam, :chain)\nsolver::Symbol: solver to use (:ipopt or :madnlp)\nmodel::Symbol: model type (:jump, :adnlp, :exa, or :exa_gpu)\ngrid_size::Int: number of grid points\ndisc_method::Symbol: discretization method\ntol::Float64: solver tolerance\nmu_strategy::Union{String, Missing}: mu strategy for Ipopt (missing for MadNLP)\nprint_level::Union{Int, MadNLP.LogLevels, Missing}: print level for solver (Int for Ipopt, MadNLP.LogLevels for MadNLP)\nmax_iter::Int: maximum number of iterations\nmax_wall_time::Float64: maximum wall time in seconds\n\nReturns\n\nA NamedTuple with fields:\n\nbenchmark: full benchmark object from @btimed (CPU) or CUDA.@timed (GPU)\nobjective::Union{Float64, Missing}: objective function value (missing if failed)\niterations::Union{Int, Missing}: number of solver iterations (missing if failed)\nstatus::Any: termination status (type depends on solver/model)\nsuccess::Bool: whether the solve succeeded\ncriterion::Union{String, Missing}: optimization sense (\"min\" or \"max\", missing if failed)\nsolution::Union{Any, Missing}: the solution object (JuMP model or OCP solution, missing if failed)\n\n\n\n\n\n","category":"function"},{"location":"api/private.html#CTBenchmarks.strip_benchmark_value","page":"Private","title":"CTBenchmarks.strip_benchmark_value","text":"strip_benchmark_value(bench)\n\nRemove the value field from benchmark outputs (NamedTuple or Dict) to ensure JSON-serializable data while preserving all other statistics.\n\n\n\n\n\n","category":"function"},{"location":"benchmark-core-gpu.html#Core-GPU-Benchmark","page":"GPU","title":"Core GPU Benchmark","text":"","category":"section"},{"location":"benchmark-core-gpu.html#Moonshot","page":"GPU","title":"Moonshot","text":"This benchmark suite evaluates optimal control problems on GPU-accelerated hardware, focusing on large-scale problems.\n\nBenchmark Configuration:\n\n<div class=\"benchmark-config\" style=\"margin-bottom: 1.5em;\">\n\n_print_config(BENCH_CORE_MOONSHOT_GPU) # hide\n\n</div>\n\nThis configuration demonstrates GPU acceleration capabilities with ExaModels on large-scale problems, comparing CPU vs GPU performance for the same modelling framework.\n\nnote: Note\nThe linear solver is MUMPS for all experiments.","category":"section"},{"location":"benchmark-core-gpu.html#Environment","page":"GPU","title":"üöÄ Environment","text":"_basic_metadata(BENCH_CORE_MOONSHOT_GPU) # hide\n\n_downloads_toml(BENCH_CORE_MOONSHOT_GPU) # hide\n\n<details style=\"margin-bottom: 0.5em; margin-top: 0.5em;\"><summary>‚ÑπÔ∏è Version info</summary>\n\n_bench_data(BENCH_CORE_MOONSHOT_GPU) # hide\n\n</details>\n\n<details style=\"margin-bottom: 0.5em;\"><summary>üì¶ Package status</summary>\n\n_package_status(BENCH_CORE_MOONSHOT_GPU) # hide\n\n</details>\n\n<details style=\"margin-bottom: 0.5em;\"><summary>üìö Complete manifest</summary>\n\n_complete_manifest(BENCH_CORE_MOONSHOT_GPU) # hide\n\n</details>","category":"section"},{"location":"benchmark-core-gpu.html#Results","page":"GPU","title":"‚ö° Results","text":"_plot_performance_profiles(BENCH_CORE_MOONSHOT_GPU) # hide\n\n_print_benchmark_log(BENCH_CORE_MOONSHOT_GPU) # hide","category":"section"},{"location":"dev.html#Development-Guidelines","page":"Development Guidelines","title":"Development Guidelines","text":"This guide explains how to add a new benchmark to the CTBenchmarks.jl pipeline.","category":"section"},{"location":"dev.html#Overview","page":"Development Guidelines","title":"Overview","text":"Adding a new benchmark involves creating several components:\n\nStep Description Status\nBenchmark script Julia script that runs the benchmark Required\nJSON configuration Add benchmark config to JSON file Required\nGitHub label Label to trigger the benchmark on pull requests Required\nIndividual workflow Workflow for manual testing (reads from JSON) Optional\nDocumentation page Display benchmark results in the documentation Optional","category":"section"},{"location":"dev.html#Step-by-Step-Guide","page":"Development Guidelines","title":"Step-by-Step Guide","text":"","category":"section"},{"location":"dev.html#1.-Create-the-Benchmark-Script-{#benchmark-script}","page":"Development Guidelines","title":"1. Create the Benchmark Script {#benchmark-script}","text":"Create a new Julia script in the benchmarks/ directory. Choose a descriptive filename that will serve as your benchmark identifier.\n\nNaming convention: Use kebab-case (e.g., core-ubuntu-latest.jl, core-moonshot-gpu.jl)\n\nExample: benchmarks/core-ubuntu-latest.jl\n\n# Benchmark script for <id>\n# Setup (Pkg.activate, instantiate, update, using CTBenchmarks) is handled by the workflow\n\nfunction run()\n    results = CTBenchmarks.benchmark(;\n        problems = [:problem1, :problem2, ...],\n        solver_models = [:solver => [:model1, :model2]],\n        grid_sizes = [100, 500, 1000],\n        disc_methods = [:trapeze],\n        tol = 1e-6,\n        ipopt_mu_strategy = \"adaptive\",\n        print_trace = false,\n        max_iter = 1000,\n        max_wall_time = 500.0\n    )\n    println(\"‚úÖ Benchmark completed successfully!\")\n    return results\nend\n\nKey points:\n\nSetup code is handled by the workflow - No need to include using Pkg, Pkg.activate(), Pkg.instantiate(), Pkg.update(), or using CTBenchmarks in your script. The GitHub Actions workflow handles all environment setup automatically.\nAll parameters are required - the benchmark function has no optional arguments\nDefine a run() function - it must take no arguments, return the Dict payload from CTBenchmarks.benchmark, and should not perform any file I/O\nThe workflow calls run(), saves the returned payload as {id}.json, and stores it under docs/src/assets/benchmarks/{id}/\nTOML files are copied by the workflow - Project.toml and Manifest.toml are automatically copied to the output directory by the GitHub Actions workflow to ensure reproducibility\nAvailable problems: The list of problems you can choose is available in the OptimalControlProblems.jl documentation\nFor local testing: See benchmarks/local.jl for an example that includes the setup code needed to run benchmarks locally","category":"section"},{"location":"dev.html#2.-Add-Configuration-to-JSON-{#json-config}","page":"Development Guidelines","title":"2. Add Configuration to JSON {#json-config}","text":"Edit benchmarks/benchmarks-config.json and add your benchmark configuration:\n\n{\n  \"benchmarks\": [\n    {\n      \"id\": \"your-benchmark-id\",\n      \"julia_version\": \"1.11\",\n      \"julia_arch\": \"x64\",\n      \"runs_on\": \"ubuntu-latest\",\n      \"runner\": \"github\"\n    }\n  ]\n}\n\nConfiguration fields:\n\nid (required): Unique identifier for the benchmark (kebab-case)\nMust exactly match your script filename (without the .jl extension)\nConvention: {family}-{runner} (e.g., core-ubuntu-latest, core-moonshot)\nExample: if your script is benchmarks/core-ubuntu-latest.jl, use \"id\": \"core-ubuntu-latest\"\nUsed in label: run bench {id}\njulia_version (required): Julia version to use (e.g., \"1.11\")\njulia_arch (required): Architecture (typically \"x64\")\nruns_on (required): GitHub runner specification\nFor standard runners: \"ubuntu-latest\"\nFor self-hosted runners with custom labels: \"[\\\"moonshot\\\"]\" or \"[\\\"mothra\\\"]\" (use the runner label configured in your self-hosted runner)`\nrunner (required): Runner type for caching strategy\n\"github\" for standard GitHub runners (uses julia-actions/cache)\n\"self-hosted\" for self-hosted runners (uses actions/cache for artifacts only)\n\nExamples:\n\n// Standard GitHub runner\n{\n  \"id\": \"core-ubuntu-latest\",\n  \"julia_version\": \"1.11\",\n  \"julia_arch\": \"x64\",\n  \"runs_on\": \"ubuntu-latest\",\n  \"runner\": \"github\"\n}\n\n// Self-hosted runner with custom label\n{\n  \"id\": \"core-moonshot\",\n  \"julia_version\": \"1.11\",\n  \"julia_arch\": \"x64\",\n  \"runs_on\": \"[\\\"moonshot\\\"]\",\n  \"runner\": \"self-hosted\"\n}","category":"section"},{"location":"dev.html#Automatic-Workflow-Execution","page":"Development Guidelines","title":"Automatic Workflow Execution","text":"Good news! You don't need to create a workflow file manually. The orchestrator automatically runs your benchmark based on the JSON configuration using a matrix strategy.\n\nWhen you add a label to a PR (e.g., run bench your-benchmark-id), the orchestrator:\n\nReads benchmarks/benchmarks-config.json\nFinds your benchmark configuration by matching the label with the id field\nCalls the reusable workflow with the parameters from the JSON (Julia version, architecture, runner, etc.)\nThe reusable workflow loads and executes your script at benchmarks/{id}.jl\nResults are saved to docs/src/assets/benchmarks/{id}/{id}.json\n\nEverything is automatic! ‚ú®","category":"section"},{"location":"dev.html#3.-Create-the-GitHub-Label-{#github-label}","page":"Development Guidelines","title":"3. Create the GitHub Label {#github-label}","text":"On GitHub, create a new label for your benchmark:\n\nGo to your repository ‚Üí Issues ‚Üí Labels\nClick New label\nName: run bench {id} where {id} matches your JSON configuration\nExample: run bench core-ubuntu-latest\nExample: run bench core-moonshot-gpu\nImportant: Use the exact benchmark ID from JSON\nChoose a color and description\nClick Create label\n\nLabel types:\n\nIndividual labels - Trigger a specific benchmark:\nFormat: run bench {id}\nExample: run bench core-moonshot-gpu\nExample: run bench minimal-ubuntu-latest\nGroup labels - Trigger all benchmarks with a common prefix:\nFormat: run bench {prefix}-all\nExample: run bench core-all ‚Üí runs all core-* benchmarks\nExample: run bench minimal-all ‚Üí runs all minimal-* benchmarks\nExample: run bench gpu-all ‚Üí runs all gpu-* benchmarks\n\nNaming convention for benchmark families:\n\nTo use group labels effectively, follow this naming convention:\n\n{family}-{runner} format (e.g., core-ubuntu-latest, core-moonshot)\nAll benchmarks in the same family share the same prefix\nGroup label run bench {family}-all will run all benchmarks in that family\n\nExamples:\n\ncore-ubuntu-latest, core-moonshot-gpu, core-mothra-gpu ‚Üí run bench core-all\nminimal-ubuntu-latest, minimal-moonshot-gpu, minimal-mothra-gpu ‚Üí run bench minimal-all\ngpu-cuda12, gpu-cuda13 ‚Üí run bench gpu-all","category":"section"},{"location":"dev.html#4.-(Optional)-Create-Individual-Workflow-{#individual-workflow}","page":"Development Guidelines","title":"4. (Optional) Create Individual Workflow {#individual-workflow}","text":"info: Optional Step\nIndividual workflows are optional. The orchestrator will automatically run your benchmark based on the JSON configuration. Individual workflows are useful for:Manual testing via workflow_dispatch\nRunning a specific benchmark without the orchestrator\nDebugging\n\nCreate .github/workflows/benchmark-{id}.yml:\n\nname: Benchmark {Name}\n\non:\n  workflow_call:\n  workflow_dispatch:\n\npermissions:\n  contents: write\n  pull-requests: write\n\njobs:\n  load-config:\n    runs-on: ubuntu-latest\n    outputs:\n      config: ${{ steps.get-config.outputs.config }}\n    steps:\n      - uses: actions/checkout@v5\n      - name: Get benchmark config\n        id: get-config\n        run: |\n          CONFIG=$(jq -c '.benchmarks[] | select(.id == \"{id}\")' benchmarks/benchmarks-config.json)\n          echo \"config=$CONFIG\" >> $GITHUB_OUTPUT\n  \n  bench:\n    needs: load-config\n    uses: ./.github/workflows/benchmark-reusable.yml\n    with:\n      script_path: benchmarks/${{ fromJSON(needs.load-config.outputs.config).id }}.jl\n      julia_version: ${{ fromJSON(needs.load-config.outputs.config).julia_version }}\n      julia_arch: ${{ fromJSON(needs.load-config.outputs.config).julia_arch }}\n      runs_on: ${{ fromJSON(needs.load-config.outputs.config).runs_on }}\n      runner: ${{ fromJSON(needs.load-config.outputs.config).runner }}\n\nKey features:\n\nReads configuration from JSON - Single source of truth\nUses ID to construct script path - benchmarks/${{ fromJSON(...).id }}.jl ensures consistency\nCan be triggered manually via workflow_dispatch for testing\nCan be called by orchestrator via workflow_call\nNo hardcoded values - Everything comes from JSON configuration","category":"section"},{"location":"dev.html#5.-(Optional)-Create-Documentation-Page-{#documentation-page}","page":"Development Guidelines","title":"5. (Optional) Create Documentation Page {#documentation-page}","text":"If you want to display results in the documentation, create docs/src/benchmark-<name>.md.template:\n\n# <Name> Benchmark\n\n```@setup BENCH\ninclude(joinpath(@__DIR__, \"assets\", \"jl\", \"utils.jl\"))\n\n# Define benchmark ID\nconst BENCH_ID = \"<id>\"\n```\n\n## Description\n\nBrief description of your benchmark configuration.\n\n**Benchmark Configuration:**\n\n- **Solvers:** List of solvers (e.g., Ipopt, MadNLP)\n- **Models:** List of models (e.g., JuMP, ADNLPModels, ExaModels)\n- **Grid sizes:** Discretisation points (e.g., 200, 500, 1000)\n- **Discretisation:** Method used (e.g., Trapeze)\n- **Tolerance:** 1e-6\n- **Limits:** Max iterations and wall time\n\n!!! note\n    Add any relevant notes about the benchmark setup (e.g., linear solver used)\n\n### üñ•Ô∏è Environment\n\n<!-- INCLUDE_ENVIRONMENT:\nBENCH_ID = BENCH_ID\nENV_NAME = BENCH\n-->\n\n### üìä Results\n\n```@example BENCH\n_plot_performance_profiles(BENCH_ID) # hide\n```\n\n```@example BENCH\n_print_benchmark_log(BENCH_ID) # hide\nnothing # hide\n```\n\nKey points:\n\nUse a single @setup BENCH block for all benchmarks in the same page\nDefine BENCH_ID as a constant with your benchmark identifier\nUse _plot_performance_profiles(BENCH_ID) to display performance plots (optional)\nUse _print_benchmark_log(BENCH_ID) to display detailed results table\nThe INCLUDE_ENVIRONMENT comment is processed by the documentation build system\n\nThen add it to docs/make.jl:\n\npages = [\n    \"Introduction\" => \"index.md\",\n    \"Core benchmark\" => \"benchmark-core.md\",\n    \"<Name> Benchmark\" => \"benchmark-<name>.md\",\n    \"API\" => \"api.md\",\n    \"Development Guidelines\" => \"dev.md\",\n]","category":"section"},{"location":"dev.html#Testing-Your-Benchmark","page":"Development Guidelines","title":"Testing Your Benchmark","text":"Local testing: Run your script locally to verify it works\nPush changes: Commit and push all files\nCreate PR: Open a pull request\nAdd label: Add the run bench <name> label to trigger the workflow\nMonitor: Check the Actions tab to monitor execution","category":"section"},{"location":"dev.html#Troubleshooting","page":"Development Guidelines","title":"Troubleshooting","text":"Cache issues on self-hosted runners:\n\nEnsure \"runner\": \"self-hosted\" is set in your JSON configuration\nThe reusable workflow uses actions/cache for artifacts only on self-hosted runners\nStandard GitHub runners should use \"runner\": \"github\" to enable full package caching\n\nWorkflow not triggering:\n\nVerify the label name matches exactly: run bench {id} where {id} is from your JSON\nCheck that your benchmark ID exists in benchmarks/benchmarks-config.json\nEnsure the benchmark script file exists at benchmarks/{id}.jl\n\nBenchmark script fails:\n\nCheck Julia version compatibility\nVerify all dependencies are available on the target runner\nReview the benchmark function parameters","category":"section"},{"location":"dev.html#Examples","page":"Development Guidelines","title":"Examples","text":"","category":"section"},{"location":"dev.html#Example-1:-Standard-GitHub-Runner","page":"Development Guidelines","title":"Example 1: Standard GitHub Runner","text":"A CPU benchmark running on GitHub Actions:\n\nJSON configuration:\n\n{\n  \"id\": \"core-ubuntu-latest\",\n  \"julia_version\": \"1.11\",\n  \"julia_arch\": \"x64\",\n  \"runs_on\": \"\\\"ubuntu-latest\\\"\",\n  \"runner\": \"github\"\n}\n\nFiles:\n\nScript: benchmarks/core-ubuntu-latest.jl\nLabel: run bench core-ubuntu-latest\nDocumentation: docs/src/benchmark-core.md.template","category":"section"},{"location":"dev.html#Example-2:-Self-Hosted-Runner-(Moonshot)","page":"Development Guidelines","title":"Example 2: Self-Hosted Runner (Moonshot)","text":"A GPU benchmark on a self-hosted runner with custom label:\n\nJSON configuration:\n\n{\n  \"id\": \"core-moonshot-gpu\",\n  \"julia_version\": \"1.11\",\n  \"julia_arch\": \"x64\",\n  \"runs_on\": \"[\\\"moonshot\\\"]\",\n  \"runner\": \"self-hosted\"\n}\n\nFiles:\n\nScript: benchmarks/core-moonshot-gpu.jl\nLabel: run bench core-moonshot-gpu\n\nKey points:\n\nUses simplified runner label [\"moonshot\"] instead of full system labels\nThe runner: \"self-hosted\" field tells the workflow to use artifact-only caching","category":"section"},{"location":"dev.html#Example-3:-Multiple-Runners,-Same-Hardware","page":"Development Guidelines","title":"Example 3: Multiple Runners, Same Hardware","text":"You can create CPU and GPU variants for the same hardware:\n\nCPU variant:\n\n{\n  \"id\": \"core-moonshot-cpu\",\n  \"julia_version\": \"1.11\",\n  \"julia_arch\": \"x64\",\n  \"runs_on\": \"[\\\"moonshot\\\"]\",\n  \"runner\": \"self-hosted\"\n}\n\nGPU variant:\n\n{\n  \"id\": \"core-moonshot-gpu\",\n  \"julia_version\": \"1.11\",\n  \"julia_arch\": \"x64\",\n  \"runs_on\": \"[\\\"moonshot\\\"]\",\n  \"runner\": \"self-hosted\"\n}\n\nBoth use the same runner label but different benchmark scripts with different solver configurations.","category":"section"},{"location":"dev.html#How-the-Orchestrator-Works","page":"Development Guidelines","title":"How the Orchestrator Works","text":"","category":"section"},{"location":"dev.html#Matrix-Strategy","page":"Development Guidelines","title":"Matrix Strategy","text":"The orchestrator uses a matrix strategy to dynamically call benchmarks:\n\nGuard job reads benchmarks/benchmarks-config.json\nBased on PR labels, it builds a JSON array of selected benchmarks\nBenchmark job uses matrix to iterate over selected benchmarks\nEach matrix iteration calls benchmark-reusable.yml with the appropriate parameters\n\nBenefits:\n\nNo need to declare individual jobs for each benchmark\nAdding a benchmark requires only JSON modification\nAll benchmarks run in parallel (matrix strategy)\nConsistent behavior across all benchmarks","category":"section"},{"location":"dev.html#Label-System","page":"Development Guidelines","title":"Label System","text":"The orchestrator supports two types of labels with automatic prefix detection:","category":"section"},{"location":"dev.html#Individual-Labels","page":"Development Guidelines","title":"Individual Labels","text":"Format: run bench {id}\nBehavior: Runs the specific benchmark with that exact ID\nExamples:\nrun bench core-ubuntu-latest ‚Üí runs only core-ubuntu-latest\nrun bench minimal-macos ‚Üí runs only minimal-macos","category":"section"},{"location":"dev.html#Group-Labels-(Generic)","page":"Development Guidelines","title":"Group Labels (Generic)","text":"Format: run bench {prefix}-all\nBehavior: Automatically runs all benchmarks whose ID starts with {prefix}-\nHow it works:\nThe orchestrator extracts the prefix from the label (e.g., core from run bench core-all)\nIt scans all benchmark IDs in the JSON\nIt selects all benchmarks matching the pattern {prefix}-*\nExamples:\nrun bench core-all ‚Üí runs core-ubuntu-latest, core-moonshot-cpu, core-moonshot-gpu, core-mothra-gpu\nrun bench minimal-all ‚Üí runs all benchmarks starting with minimal-","category":"section"},{"location":"dev.html#Multiple-Labels","page":"Development Guidelines","title":"Multiple Labels","text":"You can combine multiple labels on a PR:\n\nrun bench core-all + run bench minimal-ubuntu-latest ‚Üí runs all core-* benchmarks + minimal-ubuntu-latest\nrun bench core-moonshot + run bench gpu-all ‚Üí runs core-moonshot + all gpu-* benchmarks","category":"section"},{"location":"dev.html#Automatic-Discovery","page":"Development Guidelines","title":"Automatic Discovery","text":"The system is completely generic - no hardcoded family names:\n\nAdd benchmarks with any prefix (e.g., perf-*, stress-*, validation-*)\nCreate corresponding group labels (e.g., run bench perf-all)\nThe orchestrator automatically detects and processes them","category":"section"},{"location":"dev.html#Configuration-File","page":"Development Guidelines","title":"Configuration File","text":"The benchmarks/benchmarks-config.json file is the single source of truth:\n\nOrchestrator reads it to discover available benchmarks\nIndividual workflows read it to get their configuration\nEasy to maintain and validate\nCan be extended with additional metadata","category":"section"},{"location":"benchmark-core-beam.html#Core-Beam-Benchmark","page":"Beam","title":"Core Beam Benchmark","text":"This page presents benchmark results for the beam problem across different platforms and configurations.","category":"section"},{"location":"benchmark-core-beam.html#Ubuntu-Latest-CPU","page":"Beam","title":"Ubuntu Latest - CPU","text":"Results for the beam problem on GitHub Actions runners (Ubuntu latest).\n\nBenchmark Configuration:\n\n<div class=\"benchmark-config\" style=\"margin-bottom: 1.5em;\">\n\n_print_config(BENCH_CORE_UBUNTU) # hide\n\n</div>\n\nThis configuration focuses on CPU-based solvers and provides a comprehensive comparison across different modelling frameworks.\n\nnote: Note\nThe linear solver is MUMPS for all experiments.","category":"section"},{"location":"benchmark-core-beam.html#Ubuntu-Results","page":"Beam","title":"üìä Ubuntu Results","text":"_plot_time_vs_grid_size(PROBLEM, BENCH_CORE_UBUNTU) # hide\n\n_plot_time_vs_grid_size_bar(PROBLEM, BENCH_CORE_UBUNTU) # hide\n\n_print_benchmark_log(BENCH_CORE_UBUNTU; problems=[PROBLEM]) # hide","category":"section"},{"location":"benchmark-core-beam.html#Moonshot-CPU","page":"Beam","title":"Moonshot - CPU","text":"Results for the beam problem on self-hosted CPU hardware.\n\nBenchmark Configuration:\n\n<div class=\"benchmark-config\" style=\"margin-bottom: 1.5em;\">\n\n_print_config(BENCH_CORE_MOONSHOT_CPU) # hide\n\n</div>\n\nThis configuration tests larger grid sizes on dedicated hardware, comparing performance with the GitHub Actions runner.\n\nnote: Note\nThe linear solver is MUMPS for all experiments.","category":"section"},{"location":"benchmark-core-beam.html#Moonshot-CPU-Results","page":"Beam","title":"‚ö° Moonshot CPU Results","text":"_plot_time_vs_grid_size(PROBLEM, BENCH_CORE_MOONSHOT_CPU) # hide\n\n_plot_time_vs_grid_size_bar(PROBLEM, BENCH_CORE_MOONSHOT_CPU) # hide\n\n_print_benchmark_log(BENCH_CORE_MOONSHOT_CPU; problems=[PROBLEM]) # hide","category":"section"},{"location":"benchmark-core-beam.html#Moonshot-GPU","page":"Beam","title":"Moonshot - GPU","text":"Results for the beam problem on GPU-accelerated hardware.\n\nBenchmark Configuration:\n\n<div class=\"benchmark-config\" style=\"margin-bottom: 1.5em;\">\n\n_print_config(BENCH_CORE_MOONSHOT_GPU) # hide\n\n</div>\n\nThis configuration demonstrates GPU acceleration capabilities with ExaModels on large-scale problems, comparing CPU vs GPU performance for the same modelling framework.\n\nnote: Note\nThe linear solver is MUMPS for all experiments.","category":"section"},{"location":"benchmark-core-beam.html#Moonshot-GPU-Results","page":"Beam","title":"üöÄ Moonshot GPU Results","text":"_plot_time_vs_grid_size(PROBLEM, BENCH_CORE_MOONSHOT_GPU) # hide\n\n_plot_time_vs_grid_size_bar(PROBLEM, BENCH_CORE_MOONSHOT_GPU) # hide\n\n_print_benchmark_log(BENCH_CORE_MOONSHOT_GPU; problems=[PROBLEM]) # hide","category":"section"},{"location":"index.html#CTBenchmarks","page":"Introduction","title":"CTBenchmarks","text":"CTBenchmarks.jl is a comprehensive benchmarking suite for optimal control problems, designed to evaluate and compare the performance of different solvers and modelling approaches within the control-toolbox ecosystem.\n\nThis package provides:\n\nüöÄ Pre-configured benchmark suites for quick performance evaluation\nüìä Automated result collection and analysis\nüîß Flexible API for creating custom benchmarks\nüìà Detailed performance metrics including timing, memory usage, and solver statistics","category":"section"},{"location":"index.html#Installation","page":"Introduction","title":"Installation","text":"CTBenchmarks.jl is registered in the Julia General Registry. Install it using the package manager:\n\nusing Pkg\nPkg.add(\"CTBenchmarks\")\n\nOr in the Julia REPL package mode (press ]):\n\npkg> add CTBenchmarks\n\nOnce installed, load the package:\n\nusing CTBenchmarks","category":"section"},{"location":"index.html#Quick-Start","page":"Introduction","title":"Quick Start","text":"","category":"section"},{"location":"index.html#Running-Pre-configured-Benchmarks","page":"Introduction","title":"Running Pre-configured Benchmarks","text":"CTBenchmarks provides two pre-configured benchmark suites:","category":"section"},{"location":"index.html#Minimal-Benchmark-(Fast)","page":"Introduction","title":"Minimal Benchmark (Fast)","text":"Run a quick benchmark on a single problem to test your setup:\n\nCTBenchmarks.run(:minimal)\n\nThis runs the :beam problem with:\n\nGrid size: 100\nDiscretization: trapeze\nSolvers: Ipopt and MadNLP\nModels: JuMP, adnlp, exa, exa_gpu","category":"section"},{"location":"index.html#Complete-Benchmark-(Comprehensive)","page":"Introduction","title":"Complete Benchmark (Comprehensive)","text":"Run the full benchmark suite across all problems:\n\nCTBenchmarks.run(:complete)\n\nThis runs 14 optimal control problems with:\n\nGrid sizes: 100, 200, 500\nDiscretizations: trapeze, midpoint\nSolvers: Ipopt and MadNLP\nModels: JuMP, adnlp, exa, exa_gpu\n\ntip: Solver Output\nBy default, solver output is suppressed. To see detailed solver traces, use:CTBenchmarks.run(:minimal; print_trace=true)","category":"section"},{"location":"index.html#Saving-Results","page":"Introduction","title":"Saving Results","text":"To save benchmark results to a directory:\n\nresults = CTBenchmarks.run(:minimal; filepath=\"my_results/minimal.json\")\n\nThis returns the benchmark payload as a Dict and saves it to my_results/minimal.json (the directory is created automatically if needed). The filepath argument is optional but, when provided, it must end with .json.\n\nmy_results/minimal.json ‚Äì Benchmark results in JSON format","category":"section"},{"location":"index.html#Creating-Custom-Benchmarks","page":"Introduction","title":"Creating Custom Benchmarks","text":"For more control over your benchmarks, use the CTBenchmarks.benchmark function directly:\n\nresults = CTBenchmarks.benchmark(;\n    problems = [:beam, :chain, :robot],\n    solver_models = [\n        :ipopt => [:jump, :adnlp, :exa],\n        :madnlp => [:exa, :exa_gpu]\n    ],\n    grid_sizes = [200, 500, 1000],\n    disc_methods = [:trapeze],\n    tol = 1e-6,\n    ipopt_mu_strategy = \"adaptive\",\n    print_trace = false,\n    max_iter = 1000,\n    max_wall_time = 500.0\n)\n\nCTBenchmarks.save_json(results, \"path/to/custom_benchmark.json\")","category":"section"},{"location":"index.html#Available-Problems","page":"Introduction","title":"Available Problems","text":"CTBenchmarks includes 14 optimal control problems from OptimalControlProblems.jl:\n\n:beam - Beam control problem\n:chain - Chain of masses\n:double_oscillator - Double oscillator\n:ducted_fan - Ducted fan control\n:electric_vehicle - Electric vehicle optimization\n:glider - Glider trajectory\n:insurance - Insurance problem\n:jackson - Jackson problem\n:robbins - Robbins problem\n:robot - Robot arm control\n:rocket - Rocket trajectory\n:space_shuttle - Space shuttle re-entry\n:steering - Steering control\n:vanderpol - Van der Pol oscillator","category":"section"},{"location":"index.html#Solver-and-Model-Combinations","page":"Introduction","title":"Solver and Model Combinations","text":"Supported Solvers:\n\n:ipopt - Interior Point Optimizer\n:madnlp - Matrix-free Augmented Lagrangian NLP solver\n\nSupported Models:\n\n:jump - JuMP modelling framework\n:adnlp - Automatic differentiation NLP models\n:exa - ExaModels (CPU)\n:exa_gpu - ExaModels (GPU acceleration)","category":"section"},{"location":"index.html#Benchmark-Parameters","page":"Introduction","title":"Benchmark Parameters","text":"grid_sizes: Number of discretization points (e.g., [100, 200, 500])\ndisc_methods: Discretization schemes (:trapeze, :midpoint)\ntol: Solver tolerance (e.g., 1e-6)\nmax_iter: Maximum solver iterations (e.g., 1000)\nmax_wall_time: Maximum wall time in seconds (e.g., 500.0)","category":"section"},{"location":"index.html#Benchmark-Results-in-This-Documentation","page":"Introduction","title":"Benchmark Results in This Documentation","text":"This documentation includes pre-computed benchmark results from continuous integration runs on different platforms:\n\nUbuntu Latest - Standard CPU benchmarks on GitHub Actions runners\nMoonshot - GPU-accelerated benchmarks on dedicated hardware\n\nThese results provide reference performance data and demonstrate the capabilities of different solver and model combinations. You can explore them in the Core Benchmark section.\n\nEach benchmark result page includes:\n\nüìä Performance metrics (time, memory, iterations)\nüñ•Ô∏è Environment information (Julia version, OS, hardware)\nüìú Reproducible benchmark scripts\nüì¶ Complete dependency information","category":"section"},{"location":"index.html#Understanding-Benchmark-Output","page":"Introduction","title":"Understanding Benchmark Output","text":"When you run a benchmark, you'll see output similar to:\n\nBenchmarks results:\n\n‚îå‚îÄ problem: beam\n‚îÇ\n‚îú‚îÄ‚îÄ‚î¨ solver: ipopt, disc_method: trapeze\n‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  N : 100\n‚îÇ  ‚îÇ  ‚úì | JuMP    | time:    1.234 s | iters: 42    | obj: 1.234567e+00 (min) | CPU:    2.5 MiB\n‚îÇ  ‚îÇ  ‚úì | adnlp   | time:    0.987 s | iters: 42    | obj: 1.234567e+00 (min) | CPU:    2.1 MiB\n‚îÇ  ‚îÇ  ‚úì | exa     | time:    0.765 s | iters: 42    | obj: 1.234567e+00 (min) | CPU:    1.8 MiB\n‚îÇ  ‚îî‚îÄ\n‚îî‚îÄ\n\nLegend:\n\n‚úì / ‚úó - Success or failure indicator\nModel - Modelling framework (JuMP, ADNLPModels, ExaModels)\ntime - Total solve time\niters - Number of solver iterations\nobj - Objective function value and criterion (min or max problem)\nMemory - CPU memory usage (GPU memory shown separately for GPU models)","category":"section"},{"location":"index.html#Documentation-build-environment","page":"Introduction","title":"Documentation build environment","text":"_downloads_toml() # hide\n\n<details style=\"margin-bottom: 0.5em; margin-top: 1em;\"><summary>‚ÑπÔ∏è Version info</summary>\n\nversioninfo() # hide\n\n</details>\n\n<details style=\"margin-bottom: 0.5em;\"><summary>üì¶ Package status</summary>\n\nPkg.status() # hide\n\n</details>\n\n<details style=\"margin-bottom: 0.5em;\"><summary>üìö Complete manifest</summary>\n\nPkg.status(; mode = PKGMODE_MANIFEST) # hide\n\n</details>","category":"section"},{"location":"api/public.html#Public-API","page":"Public","title":"Public API","text":"This page lists the exported symbols of CTBenchmarks.\n\nLoad all public symbols into the current scope with:\n\nusing CTBenchmarks\n\nAlternatively, load only the module with:\n\nimport CTBenchmarks\n\nand then prefix all calls with CTBenchmarks. to create CTBenchmarks.<NAME>.","category":"section"},{"location":"api/public.html#benchmark","page":"Public","title":"benchmark","text":"","category":"section"},{"location":"api/public.html#plot_solutions","page":"Public","title":"plot_solutions","text":"","category":"section"},{"location":"api/public.html#run","page":"Public","title":"run","text":"","category":"section"},{"location":"api/public.html#CTBenchmarks.benchmark","page":"Public","title":"CTBenchmarks.benchmark","text":"benchmark(;\n    problems,\n    solver_models,\n    grid_sizes,\n    disc_methods,\n    tol,\n    ipopt_mu_strategy,\n    print_trace,\n    max_iter,\n    max_wall_time,\n    grid_size_max_cpu\n) -> Nothing\n\nRun benchmarks on optimal control problems and save results to a JSON file.\n\nThis function performs the following steps:\n\nDetects CUDA availability and filters out :exa_gpu if CUDA is not functional\nRuns benchmarks using benchmark_data() to generate a DataFrame of results\nCollects environment metadata (Julia version, OS, machine, timestamp)\nBuilds a JSON-friendly payload combining results and metadata\nReturns the payload as a Dict\n\nThe JSON file can be easily loaded and converted back to a DataFrame using:\n\nusing JSON, DataFrames\ndata = JSON.parsefile(\"path/to/data.json\")\ndf = DataFrame(data[\"results\"])\n\nnote: File Management in CI\nWhen run in the GitHub Actions workflow, Project.toml and Manifest.toml are  automatically copied to the output directory by the workflow itself. This ensures  reproducibility of benchmark results.\n\nnote: Return Value\nThis function returns Dict.\n\nArguments\n\nproblems: Vector of problem names (Symbols)\nsolver_models: Vector of Pairs mapping solver => models (e.g., [:ipopt => [:jump, :adnlp], :madnlp => [:exa, :exa_gpu]])\ngrid_sizes: Vector of grid sizes (Int)\ndisc_methods: Vector of discretization methods (Symbols)\ntol: Solver tolerance (Float64)\nipopt_mu_strategy: Mu strategy for Ipopt (String)\nprint_trace: Boolean - whether to print solver output (for debugging)\nmax_iter: Maximum number of iterations (Int)\nmax_wall_time: Maximum wall time in seconds (Float64)\ngrid_size_max_cpu: Maximum grid size for CPU models (Int)\n\nReturns\n\nDict\n\n\n\n\n\n","category":"function"},{"location":"api/public.html#CTBenchmarks.plot_solutions","page":"Public","title":"CTBenchmarks.plot_solutions","text":"plot_solutions(payload::Dict, output_dir::AbstractString)\n\nGenerate PDF plots comparing solutions for each (problem, grid_size) pair.\n\nArguments\n\npayload::Dict: Benchmark results with solutions\noutput_dir::AbstractString: Directory where to save PDF files\n\nDetails\n\nCreates one PDF per (problem, gridsize) combination directly inside `outputdir. Each plot overlays all solver-model combinations for comparison. Filename format:<problem>N<gridsize>.pdf`\n\nSolutions are plotted in order: OptimalControl solutions first (easy overlay), then JuMP solutions last (for proper layout).\n\n\n\n\n\n","category":"function"},{"location":"api/public.html#CTBenchmarks.run","page":"Public","title":"CTBenchmarks.run","text":"Run the benchmarks for a specific version.\n\nArguments\n\nversion::Symbol: version to run (:complete or :minimal)\nfilepath::Union{AbstractString, Nothing}: optional full path to the JSON output file  (including directory and filename). When provided, it must end with .json.\nprint_trace::Bool: whether to print the trace of the solver\n\nReturns\n\nDict containing benchmark results and metadata\n\n\n\n\n\n","category":"function"}]
}
