<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Development Guidelines ¬∑ CTBenchmarks</title><meta name="title" content="Development Guidelines ¬∑ CTBenchmarks"/><meta property="og:title" content="Development Guidelines ¬∑ CTBenchmarks"/><meta property="twitter:title" content="Development Guidelines ¬∑ CTBenchmarks"/><meta name="description" content="Documentation for CTBenchmarks."/><meta property="og:description" content="Documentation for CTBenchmarks."/><meta property="twitter:description" content="Documentation for CTBenchmarks."/><script data-outdated-warner src="assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="search_index.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script><link href="https://control-toolbox.org/assets/css/documentation.css" rel="stylesheet" type="text/css"/><script src="https://control-toolbox.org/assets/js/documentation.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="index.html">CTBenchmarks</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="index.html">Introduction</a></li><li><a class="tocitem" href="benchmark-core.html">Core benchmark</a></li><li><a class="tocitem" href="api.html">API</a></li><li class="is-active"><a class="tocitem" href="dev.html">Development Guidelines</a><ul class="internal"><li><a class="tocitem" href="#Overview"><span>Overview</span></a></li><li><a class="tocitem" href="#Step-by-Step-Guide"><span>Step-by-Step Guide</span></a></li><li><a class="tocitem" href="#Testing-Your-Benchmark"><span>Testing Your Benchmark</span></a></li><li><a class="tocitem" href="#Troubleshooting"><span>Troubleshooting</span></a></li><li><a class="tocitem" href="#Examples"><span>Examples</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href="dev.html">Development Guidelines</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="dev.html">Development Guidelines</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/control-toolbox/CTBenchmarks.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands">ÔÇõ</span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Development-Guidelines"><a class="docs-heading-anchor" href="#Development-Guidelines">Development Guidelines</a><a id="Development-Guidelines-1"></a><a class="docs-heading-anchor-permalink" href="#Development-Guidelines" title="Permalink"></a></h1><p>This guide explains how to add a new benchmark to the CTBenchmarks.jl pipeline.</p><h2 id="Overview"><a class="docs-heading-anchor" href="#Overview">Overview</a><a id="Overview-1"></a><a class="docs-heading-anchor-permalink" href="#Overview" title="Permalink"></a></h2><p>Adding a new benchmark involves creating several interconnected components:</p><ol><li><strong>Benchmark script</strong> ‚≠ê <em>Simple</em> - Julia script that runs the benchmark</li><li><strong>GitHub Actions workflow</strong> ‚≠ê <em>Simple</em> - Workflow that executes the script on a specific runner</li><li><strong>GitHub label</strong> ‚≠ê <em>Simple</em> - Label to trigger the benchmark on pull requests (manual step on GitHub)</li><li><strong>Orchestrator integration</strong> ‚ö†Ô∏è <em>Complex</em> - Update the orchestrator to manage the new workflow (<strong>14 locations to modify</strong>)</li><li><strong>Documentation page</strong> ‚≠ê <em>Simple</em> (optional) - Display benchmark results in the documentation</li></ol><div class="admonition is-success" id="Estimated-Time-d2c3735553821f5f"><header class="admonition-header">Estimated Time<a class="admonition-anchor" href="#Estimated-Time-d2c3735553821f5f" title="Permalink"></a></header><div class="admonition-body"><ul><li>Steps 1-3: ~10 minutes</li><li>Step 4 (Orchestrator): ~30-45 minutes (careful verification required)</li><li>Step 5: ~10 minutes</li></ul></div></div><h2 id="Step-by-Step-Guide"><a class="docs-heading-anchor" href="#Step-by-Step-Guide">Step-by-Step Guide</a><a id="Step-by-Step-Guide-1"></a><a class="docs-heading-anchor-permalink" href="#Step-by-Step-Guide" title="Permalink"></a></h2><h3 id="1.-Create-the-Benchmark-Script"><a class="docs-heading-anchor" href="#1.-Create-the-Benchmark-Script">1. Create the Benchmark Script</a><a id="1.-Create-the-Benchmark-Script-1"></a><a class="docs-heading-anchor-permalink" href="#1.-Create-the-Benchmark-Script" title="Permalink"></a></h3><p>Create a new Julia script in <code>scripts/benchmark-&lt;name&gt;.jl</code>:</p><pre><code class="language-julia hljs">using Pkg
const project_dir = normpath(@__DIR__, &quot;..&quot;)
ENV[&quot;PROJECT&quot;] = project_dir

Pkg.activate(project_dir)
Pkg.instantiate()

using CTBenchmarks

function main()
    outpath = joinpath(project_dir, &quot;docs&quot;, &quot;src&quot;, &quot;assets&quot;, &quot;benchmark-&lt;name&gt;&quot;)
    CTBenchmarks.benchmark(;
        outpath = outpath,
        problems = [:problem1, :problem2, ...],
        solver_models = [:solver =&gt; [:model1, :model2]],
        grid_sizes = [100, 500, 1000],
        disc_methods = [:trapeze],
        tol = 1e-6,
        ipopt_mu_strategy = &quot;adaptive&quot;,
        print_trace = false,
        max_iter = 1000,
        max_wall_time = 500.0
    )
    return outpath
end

main()</code></pre><p><strong>Key points:</strong></p><ul><li><strong>All parameters are required</strong> - the <code>benchmark</code> function has no optional arguments</li><li><strong>The <code>main()</code> function is crucial</strong> - it must:<ul><li>Take no arguments</li><li>Return the output path where files are saved</li></ul></li><li>The <code>benchmark</code> function generates JSON and TOML files in the specified <code>outpath</code></li><li>Print statements (like <code>println(&quot;üì¶ Activating...&quot;)</code>) are optional but helpful for debugging</li><li>The output directory follows the pattern <code>docs/src/assets/benchmark-&lt;name&gt;</code></li><li><strong>Available problems:</strong> The list of problems you can choose is available in the <a href="https://control-toolbox.org/OptimalControlProblems.jl/stable/problems_browser.html">OptimalControlProblems.jl documentation</a></li></ul><h3 id="2.-Create-the-GitHub-Actions-Workflow"><a class="docs-heading-anchor" href="#2.-Create-the-GitHub-Actions-Workflow">2. Create the GitHub Actions Workflow</a><a id="2.-Create-the-GitHub-Actions-Workflow-1"></a><a class="docs-heading-anchor-permalink" href="#2.-Create-the-GitHub-Actions-Workflow" title="Permalink"></a></h3><p>Create <code>.github/workflows/benchmark-&lt;name&gt;.yml</code>:</p><pre><code class="language-yaml hljs">name: Benchmark &lt;Name&gt;

on:
  workflow_call:

permissions:
  contents: write
  pull-requests: write

jobs:
  bench:
    uses: ./.github/workflows/benchmark-reusable.yml
    with:
      script_path: scripts/benchmark-&lt;name&gt;.jl
      julia_version: &#39;1.11&#39;
      julia_arch: x64
      runs_on: &#39;&lt;runner-specification&gt;&#39;
      runner: &#39;&lt;runner-type&gt;&#39;  # Only for self-hosted runners</code></pre><p><strong>Runner configuration:</strong></p><p>For <strong>standard GitHub runners</strong> (e.g., ubuntu-latest):</p><pre><code class="language-yaml hljs">runs_on: &#39;&quot;ubuntu-latest&quot;&#39;
# Do NOT include the &#39;runner&#39; parameter</code></pre><p>For <strong>self-hosted runners</strong> (e.g., GPU machines):</p><pre><code class="language-yaml hljs">runs_on: &#39;[&quot;self-hosted&quot;, &quot;Linux&quot;, &quot;gpu&quot;, &quot;cuda&quot;, &quot;cuda12&quot;]&#39;
runner: &#39;self-hosted&#39;</code></pre><p><strong>All inputs are required except <code>runner</code>:</strong></p><ul><li><code>script_path</code>: Path to your benchmark script</li><li><code>julia_version</code>: Julia version to use (e.g., &#39;1.11&#39;)</li><li><code>julia_arch</code>: Architecture (typically &#39;x64&#39;)</li><li><code>runs_on</code>: Runner specification (string or JSON array)</li><li><code>runner</code>: <strong>Optional</strong> - Only set to <code>&#39;self-hosted&#39;</code> for self-hosted runners</li></ul><h3 id="Understanding-Cache-Management"><a class="docs-heading-anchor" href="#Understanding-Cache-Management">Understanding Cache Management</a><a id="Understanding-Cache-Management-1"></a><a class="docs-heading-anchor-permalink" href="#Understanding-Cache-Management" title="Permalink"></a></h3><p>The <code>runner</code> parameter controls the caching strategy:</p><p><strong>Standard runners</strong> (omit <code>runner</code> parameter):</p><ul><li>Uses <code>julia-actions/cache@v2</code></li><li>Caches Julia artifacts, packages, AND registries</li><li>Cache stored on GitHub servers and restored on each run</li><li>Optimal for ephemeral runners that start fresh each time</li></ul><p><strong>Self-hosted runners</strong> (<code>runner: &#39;self-hosted&#39;</code>):</p><ul><li>Uses <code>actions/cache@v4</code> for artifacts only (<code>~/.julia/artifacts</code>)</li><li>Packages and registries persist naturally on the machine between runs</li><li>Avoids unnecessary upload/download to GitHub servers</li><li>More efficient since dependencies are already local</li></ul><p><strong>Why this matters:</strong> Self-hosted runners maintain their filesystem between runs. Using <code>julia-actions/cache</code> would wastefully upload/download gigabytes of data to/from GitHub when the files are already on the machine. We only cache artifacts to avoid re-downloading external dependencies.</p><h3 id="3.-Create-the-GitHub-Label"><a class="docs-heading-anchor" href="#3.-Create-the-GitHub-Label">3. Create the GitHub Label</a><a id="3.-Create-the-GitHub-Label-1"></a><a class="docs-heading-anchor-permalink" href="#3.-Create-the-GitHub-Label" title="Permalink"></a></h3><p>On GitHub, create a new label for your benchmark:</p><ol><li>Go to your repository ‚Üí <strong>Issues</strong> ‚Üí <strong>Labels</strong></li><li>Click <strong>New label</strong></li><li>Name: <code>run bench &lt;name&gt;</code> (e.g., <code>run bench core moonshot</code>)</li><li>Choose a color and description</li><li>Click <strong>Create label</strong></li></ol><h3 id="4.-Update-the-Orchestrator"><a class="docs-heading-anchor" href="#4.-Update-the-Orchestrator">4. Update the Orchestrator</a><a id="4.-Update-the-Orchestrator-1"></a><a class="docs-heading-anchor-permalink" href="#4.-Update-the-Orchestrator" title="Permalink"></a></h3><div class="admonition is-warning" id="Complex-Integration-5550b75909e11685"><header class="admonition-header">Complex Integration<a class="admonition-anchor" href="#Complex-Integration-5550b75909e11685" title="Permalink"></a></header><div class="admonition-body"><p>This is the <strong>most complex and error-prone step</strong>. The orchestrator requires modifications in <strong>14 different locations</strong> throughout the file. Missing even one location will cause workflow failures. Take your time and verify each step.</p></div></div><p>Edit <code>.github/workflows/benchmarks-orchestrator.yml</code> to integrate your new benchmark. Follow these steps carefully:</p><h4 id="Step-4.1:-Add-output-in-the-guard-job"><a class="docs-heading-anchor" href="#Step-4.1:-Add-output-in-the-guard-job">Step 4.1: Add output in the guard job</a><a id="Step-4.1:-Add-output-in-the-guard-job-1"></a><a class="docs-heading-anchor-permalink" href="#Step-4.1:-Add-output-in-the-guard-job" title="Permalink"></a></h4><p>In the <code>guard</code> job outputs section (~line 16-20):</p><pre><code class="language-yaml hljs">jobs:
  guard:
    outputs:
      run_ubuntu: ${{ steps.check.outputs.run_ubuntu }}
      run_moonshot: ${{ steps.check.outputs.run_moonshot }}
      run_&lt;name&gt;: ${{ steps.check.outputs.run_&lt;name&gt; }}  # Add this line
      benchmarks_summary: ${{ steps.check.outputs.benchmarks_summary }}</code></pre><h4 id="Step-4.2:-Initialize-the-variable"><a class="docs-heading-anchor" href="#Step-4.2:-Initialize-the-variable">Step 4.2: Initialize the variable</a><a id="Step-4.2:-Initialize-the-variable-1"></a><a class="docs-heading-anchor-permalink" href="#Step-4.2:-Initialize-the-variable" title="Permalink"></a></h4><p>In the guard job&#39;s check step, initialize the variable (~line 30-33):</p><pre><code class="language-bash hljs"># Initialize outputs
RUN_UBUNTU=&quot;false&quot;
RUN_MOONSHOT=&quot;false&quot;
RUN_&lt;NAME&gt;=&quot;false&quot;  # Add this line
BENCHMARKS_LIST=&quot;&quot;</code></pre><h4 id="Step-4.3:-Update-&quot;run-all&quot;-label-detection"><a class="docs-heading-anchor" href="#Step-4.3:-Update-&quot;run-all&quot;-label-detection">Step 4.3: Update &quot;run all&quot; label detection</a><a id="Step-4.3:-Update-&quot;run-all&quot;-label-detection-1"></a><a class="docs-heading-anchor-permalink" href="#Step-4.3:-Update-&quot;run-all&quot;-label-detection" title="Permalink"></a></h4><p>When the &quot;run bench core all&quot; label is detected (~line 57-64), add your benchmark:</p><pre><code class="language-bash hljs">if echo &quot;$LABELS&quot; | grep -q &quot;run bench core all&quot;; then
  echo &quot;‚úÖ Found &#39;run bench core all&#39; label&quot;
  RUN_UBUNTU=&quot;true&quot;
  RUN_MOONSHOT=&quot;true&quot;
  RUN_&lt;NAME&gt;=&quot;true&quot;  # Add this line
  BENCHMARKS_LIST=&quot;ubuntu-latest, moonshot, &lt;name&gt;&quot;  # Add &lt;name&gt; here</code></pre><h4 id="Step-4.4:-Add-specific-label-detection"><a class="docs-heading-anchor" href="#Step-4.4:-Add-specific-label-detection">Step 4.4: Add specific label detection</a><a id="Step-4.4:-Add-specific-label-detection-1"></a><a class="docs-heading-anchor-permalink" href="#Step-4.4:-Add-specific-label-detection" title="Permalink"></a></h4><p>After the moonshot label detection block (~line 68-77), add detection for your benchmark:</p><pre><code class="language-bash hljs">if echo &quot;$LABELS&quot; | grep -q &quot;run bench core &lt;name&gt;&quot;; then
  echo &quot;‚úÖ Found &#39;run bench core &lt;name&gt;&#39; label&quot;
  RUN_&lt;NAME&gt;=&quot;true&quot;
  if [ -n &quot;$BENCHMARKS_LIST&quot; ]; then
    BENCHMARKS_LIST=&quot;$BENCHMARKS_LIST, &lt;name&gt;&quot;
  else
    BENCHMARKS_LIST=&quot;&lt;name&gt;&quot;
  fi
fi</code></pre><h4 id="Step-4.5:-Update-&quot;no-labels&quot;-condition"><a class="docs-heading-anchor" href="#Step-4.5:-Update-&quot;no-labels&quot;-condition">Step 4.5: Update &quot;no labels&quot; condition</a><a id="Step-4.5:-Update-&quot;no-labels&quot;-condition-1"></a><a class="docs-heading-anchor-permalink" href="#Step-4.5:-Update-&quot;no-labels&quot;-condition" title="Permalink"></a></h4><p>Update the condition that checks if no benchmarks were selected (~line 79-83):</p><pre><code class="language-bash hljs">if [ &quot;$RUN_UBUNTU&quot; == &quot;false&quot; ] &amp;&amp; [ &quot;$RUN_MOONSHOT&quot; == &quot;false&quot; ] &amp;&amp; [ &quot;$RUN_&lt;NAME&gt;&quot; == &quot;false&quot; ]; then
  echo &quot;‚ùå No benchmark labels found&quot;
  echo &quot;‚ÑπÔ∏è  Expected labels: &#39;run bench core ubuntu&#39;, &#39;run bench core moonshot&#39;, &#39;run bench core &lt;name&gt;&#39;, or &#39;run bench core all&#39;&quot;
  BENCHMARKS_LIST=&quot;none&quot;
fi</code></pre><h4 id="Step-4.6:-Set-the-output"><a class="docs-heading-anchor" href="#Step-4.6:-Set-the-output">Step 4.6: Set the output</a><a id="Step-4.6:-Set-the-output-1"></a><a class="docs-heading-anchor-permalink" href="#Step-4.6:-Set-the-output" title="Permalink"></a></h4><p>Add the output for your benchmark (~line 88-91):</p><pre><code class="language-bash hljs"># Set outputs
echo &quot;run_ubuntu=$RUN_UBUNTU&quot; &gt;&gt; $GITHUB_OUTPUT
echo &quot;run_moonshot=$RUN_MOONSHOT&quot; &gt;&gt; $GITHUB_OUTPUT
echo &quot;run_&lt;name&gt;=$RUN_&lt;NAME&gt;&quot; &gt;&gt; $GITHUB_OUTPUT  # Add this line
echo &quot;benchmarks_summary=$BENCHMARKS_LIST&quot; &gt;&gt; $GITHUB_OUTPUT</code></pre><h4 id="Step-4.7:-Update-guard-summary-logs"><a class="docs-heading-anchor" href="#Step-4.7:-Update-guard-summary-logs">Step 4.7: Update guard summary logs</a><a id="Step-4.7:-Update-guard-summary-logs-1"></a><a class="docs-heading-anchor-permalink" href="#Step-4.7:-Update-guard-summary-logs" title="Permalink"></a></h4><p>In the guard decision summary step (~line 98-117), add logging for your benchmark:</p><pre><code class="language-bash hljs">RUN_UBUNTU=&quot;${{ steps.check.outputs.run_ubuntu }}&quot;
RUN_MOONSHOT=&quot;${{ steps.check.outputs.run_moonshot }}&quot;
RUN_&lt;NAME&gt;=&quot;${{ steps.check.outputs.run_&lt;name&gt; }}&quot;  # Add this line
SUMMARY=&quot;${{ steps.check.outputs.benchmarks_summary }}&quot;

if [ &quot;$RUN_UBUNTU&quot; == &quot;true&quot; ]; then
  echo &quot;  ‚úÖ benchmark-core-ubuntu-latest&quot;
fi
if [ &quot;$RUN_MOONSHOT&quot; == &quot;true&quot; ]; then
  echo &quot;  ‚úÖ benchmark-core-moonshot&quot;
fi
if [ &quot;$RUN_&lt;NAME&gt;&quot; == &quot;true&quot; ]; then  # Add this block
  echo &quot;  ‚úÖ benchmark-core-&lt;name&gt;&quot;
fi
if [ &quot;$RUN_UBUNTU&quot; != &quot;true&quot; ] &amp;&amp; [ &quot;$RUN_MOONSHOT&quot; != &quot;true&quot; ] &amp;&amp; [ &quot;$RUN_&lt;NAME&gt;&quot; != &quot;true&quot; ]; then
  echo &quot;  ‚è≠Ô∏è  None (conditions not met)&quot;
  echo &quot;&quot;
  echo &quot;üí° To run benchmarks on PRs, ensure:&quot;
  echo &quot;   ‚Ä¢ PR targets &#39;main&#39; branch&quot;
  echo &quot;   ‚Ä¢ PR has one of: &#39;run bench core ubuntu&#39;, &#39;run bench core moonshot&#39;, &#39;run bench core &lt;name&gt;&#39;, or &#39;run bench core all&#39;&quot;</code></pre><h4 id="Step-4.8:-Add-the-benchmark-job"><a class="docs-heading-anchor" href="#Step-4.8:-Add-the-benchmark-job">Step 4.8: Add the benchmark job</a><a id="Step-4.8:-Add-the-benchmark-job-1"></a><a class="docs-heading-anchor-permalink" href="#Step-4.8:-Add-the-benchmark-job" title="Permalink"></a></h4><p>After the existing benchmark jobs (~line 124-127), add your new job:</p><pre><code class="language-yaml hljs">benchmark-&lt;name&gt;:
  needs: guard
  if: needs.guard.outputs.run_&lt;name&gt; == &#39;true&#39;
  uses: ./.github/workflows/benchmark-&lt;name&gt;.yml</code></pre><h4 id="Step-4.9:-Update-docs-job-dependencies-and-conditions"><a class="docs-heading-anchor" href="#Step-4.9:-Update-docs-job-dependencies-and-conditions">Step 4.9: Update docs job dependencies and conditions</a><a id="Step-4.9:-Update-docs-job-dependencies-and-conditions-1"></a><a class="docs-heading-anchor-permalink" href="#Step-4.9:-Update-docs-job-dependencies-and-conditions" title="Permalink"></a></h4><p>Update the <code>docs</code> job (~line 129-138):</p><pre><code class="language-yaml hljs">docs:
  needs: [guard, benchmark-ubuntu, benchmark-moonshot, benchmark-&lt;name&gt;]  # Add benchmark-&lt;name&gt;
  if: |
    always() &amp;&amp;
    (needs.guard.result == &#39;success&#39;) &amp;&amp;
    (needs.benchmark-ubuntu.result != &#39;cancelled&#39;) &amp;&amp;
    (needs.benchmark-moonshot.result != &#39;cancelled&#39;) &amp;&amp;
    (needs.benchmark-&lt;name&gt;.result != &#39;cancelled&#39;) &amp;&amp;  # Add this line
    (needs.benchmark-ubuntu.result != &#39;failure&#39;) &amp;&amp;
    (needs.benchmark-moonshot.result != &#39;failure&#39;) &amp;&amp;
    (needs.benchmark-&lt;name&gt;.result != &#39;failure&#39;)  # Add this line</code></pre><h4 id="Step-4.10:-Update-notify-failure-job"><a class="docs-heading-anchor" href="#Step-4.10:-Update-notify-failure-job">Step 4.10: Update notify-failure job</a><a id="Step-4.10:-Update-notify-failure-job-1"></a><a class="docs-heading-anchor-permalink" href="#Step-4.10:-Update-notify-failure-job" title="Permalink"></a></h4><p>Update the <code>notify-failure</code> job dependencies (~line 167-168):</p><pre><code class="language-yaml hljs">notify-failure:
  needs: [guard, benchmark-ubuntu, benchmark-moonshot, benchmark-&lt;name&gt;, docs]  # Add benchmark-&lt;name&gt;</code></pre><p>And add failure detection in the script (~line 182-193):</p><pre><code class="language-javascript hljs">if (needs[&#39;benchmark-&lt;name&gt;&#39;] &amp;&amp; needs[&#39;benchmark-&lt;name&gt;&#39;].result === &#39;failure&#39;) {
  console.log(&#39;‚ùå Benchmark &lt;Name&gt; job failed&#39;);
  failedJobs.push(&#39;Benchmark &lt;Name&gt;&#39;);
}</code></pre><h4 id="Step-4.11:-Update-notify-success-job"><a class="docs-heading-anchor" href="#Step-4.11:-Update-notify-success-job">Step 4.11: Update notify-success job</a><a id="Step-4.11:-Update-notify-success-job-1"></a><a class="docs-heading-anchor-permalink" href="#Step-4.11:-Update-notify-success-job" title="Permalink"></a></h4><p>Update the <code>notify-success</code> job dependencies and conditions (~line 229-238):</p><pre><code class="language-yaml hljs">notify-success:
  needs: [guard, benchmark-ubuntu, benchmark-moonshot, benchmark-&lt;name&gt;, docs]  # Add benchmark-&lt;name&gt;
  if: |
    always() &amp;&amp;
    (needs.guard.result == &#39;success&#39;) &amp;&amp;
    (needs.docs.result == &#39;success&#39;) &amp;&amp;
    (needs.benchmark-ubuntu.result != &#39;cancelled&#39;) &amp;&amp;
    (needs.benchmark-moonshot.result != &#39;cancelled&#39;) &amp;&amp;
    (needs.benchmark-&lt;name&gt;.result != &#39;cancelled&#39;) &amp;&amp;  # Add this line
    (needs.benchmark-ubuntu.result != &#39;failure&#39;) &amp;&amp;
    (needs.benchmark-moonshot.result != &#39;failure&#39;) &amp;&amp;
    (needs.benchmark-&lt;name&gt;.result != &#39;failure&#39;)  # Add this line</code></pre><h4 id="Step-4.12:-Update-workflow-summary-job"><a class="docs-heading-anchor" href="#Step-4.12:-Update-workflow-summary-job">Step 4.12: Update workflow-summary job</a><a id="Step-4.12:-Update-workflow-summary-job-1"></a><a class="docs-heading-anchor-permalink" href="#Step-4.12:-Update-workflow-summary-job" title="Permalink"></a></h4><p>Update the <code>workflow-summary</code> job dependencies (~line 317-318):</p><pre><code class="language-yaml hljs">workflow-summary:
  needs: [guard, benchmark-ubuntu, benchmark-moonshot, benchmark-&lt;name&gt;, docs]  # Add benchmark-&lt;name&gt;</code></pre><p>And add summary logging (~line 340-346):</p><pre><code class="language-bash hljs">if [ &quot;${{ needs.benchmark-&lt;name&gt;.result }}&quot; == &quot;success&quot; ]; then
  echo &quot;üìä Benchmark &lt;Name&gt;: ‚úÖ SUCCESS&quot;
elif [ &quot;${{ needs.benchmark-&lt;name&gt;.result }}&quot; == &quot;failure&quot; ]; then
  echo &quot;üìä Benchmark &lt;Name&gt;: ‚ùå FAILED&quot;
elif [ &quot;${{ needs.benchmark-&lt;name&gt;.result }}&quot; == &quot;skipped&quot; ]; then
  echo &quot;üìä Benchmark &lt;Name&gt;: ‚è≠Ô∏è  SKIPPED&quot;
fi</code></pre><h4 id="Step-4.13:-Update-overall-status-check"><a class="docs-heading-anchor" href="#Step-4.13:-Update-overall-status-check">Step 4.13: Update overall status check</a><a id="Step-4.13:-Update-overall-status-check-1"></a><a class="docs-heading-anchor-permalink" href="#Step-4.13:-Update-overall-status-check" title="Permalink"></a></h4><p>Update the overall status condition (~line 362-365):</p><pre><code class="language-bash hljs">overall_status=&quot;‚úÖ SUCCESS&quot;
if [ &quot;${{ needs.benchmark-ubuntu.result }}&quot; == &quot;failure&quot; ] || 
   [ &quot;${{ needs.benchmark-moonshot.result }}&quot; == &quot;failure&quot; ] || 
   [ &quot;${{ needs.benchmark-&lt;name&gt;.result }}&quot; == &quot;failure&quot; ] ||  # Add this line
   [ &quot;${{ needs.docs.result }}&quot; == &quot;failure&quot; ]; then
  overall_status=&quot;‚ùå FAILED&quot;
fi</code></pre><h4 id="Verification-Checklist"><a class="docs-heading-anchor" href="#Verification-Checklist">Verification Checklist</a><a id="Verification-Checklist-1"></a><a class="docs-heading-anchor-permalink" href="#Verification-Checklist" title="Permalink"></a></h4><p>Before committing your changes, verify that you have updated <strong>all 14 locations</strong>:</p><ul><li>[ ] <strong>Step 4.1</strong>: Guard job outputs (add <code>run_&lt;name&gt;</code>)</li><li>[ ] <strong>Step 4.2</strong>: Variable initialization (add <code>RUN_&lt;NAME&gt;=&quot;false&quot;</code>)</li><li>[ ] <strong>Step 4.3</strong>: &quot;Run all&quot; label detection (add <code>RUN_&lt;NAME&gt;=&quot;true&quot;</code> and update <code>BENCHMARKS_LIST</code>)</li><li>[ ] <strong>Step 4.4</strong>: Specific label detection (add new <code>if</code> block for your label)</li><li>[ ] <strong>Step 4.5</strong>: No labels condition (add <code>RUN_&lt;NAME&gt;</code> check and update message)</li><li>[ ] <strong>Step 4.6</strong>: Set outputs (add <code>echo &quot;run_&lt;name&gt;=$RUN_&lt;NAME&gt;&quot;</code>)</li><li>[ ] <strong>Step 4.7</strong>: Guard summary logs (add <code>RUN_&lt;NAME&gt;</code> variable and logging block)</li><li>[ ] <strong>Step 4.8</strong>: New benchmark job (add complete job definition)</li><li>[ ] <strong>Step 4.9</strong>: Docs job (add to <code>needs</code> list and two condition lines)</li><li>[ ] <strong>Step 4.10</strong>: Notify-failure job (add to <code>needs</code> list and failure detection)</li><li>[ ] <strong>Step 4.11</strong>: Notify-success job (add to <code>needs</code> list and two condition lines)</li><li>[ ] <strong>Step 4.12</strong>: Workflow-summary job (add to <code>needs</code> list and logging block)</li><li>[ ] <strong>Step 4.13</strong>: Overall status check (add to failure condition)</li></ul><p><strong>Tip:</strong> Use <code>grep -n &quot;&lt;name&gt;&quot; .github/workflows/benchmarks-orchestrator.yml</code> to verify all occurrences of your benchmark name are present.</p><p><strong>Important:</strong> All these modifications must be done consistently. Missing even one location can cause the workflow to fail or behave unexpectedly.</p><h3 id="5.-Create-Documentation-Page-(Optional)"><a class="docs-heading-anchor" href="#5.-Create-Documentation-Page-(Optional)">5. Create Documentation Page (Optional)</a><a id="5.-Create-Documentation-Page-(Optional)-1"></a><a class="docs-heading-anchor-permalink" href="#5.-Create-Documentation-Page-(Optional)" title="Permalink"></a></h3><p>If you want to display results in the documentation, create <code>docs/src/benchmark-&lt;name&gt;.md.template</code>:</p><pre><code class="language-markdown hljs"># &lt;Name&gt; Benchmark

```@setup BENCH_&lt;NAME&gt;
include(joinpath(@__DIR__, &quot;assets&quot;, &quot;utils.jl&quot;))

const BENCH_DIR = &quot;benchmark-&lt;name&gt;&quot;
const BENCH_DATA = _read_benchmark_json(joinpath(@__DIR__, &quot;assets&quot;, BENCH_DIR, &quot;data.json&quot;))
```

## Description

Brief description of your benchmark configuration.

**Benchmark Configuration:**

- **Solvers:** List of solvers
- **Models:** List of models
- **Grid sizes:** Discretisation points
- **Tolerance:** 1e-6
- **Limits:** Max iterations and wall time

### üñ•Ô∏è Environment

&lt;!-- INCLUDE_ENVIRONMENT:
BENCH_DATA = BENCH_DATA
BENCH_DIR = BENCH_DIR
ENV_NAME = BENCH_&lt;NAME&gt;
--&gt;

### üìä Results

```@example BENCH_&lt;NAME&gt;
_print_results(BENCH_DATA) # hide
nothing # hide
```</code></pre><p>Then add it to <code>docs/make.jl</code>:</p><pre><code class="language-julia hljs">pages = [
    &quot;Introduction&quot; =&gt; &quot;index.md&quot;,
    &quot;Core benchmark&quot; =&gt; &quot;benchmark-core.md&quot;,
    &quot;&lt;Name&gt; Benchmark&quot; =&gt; &quot;benchmark-&lt;name&gt;.md&quot;,
    &quot;API&quot; =&gt; &quot;api.md&quot;,
    &quot;Development Guidelines&quot; =&gt; &quot;dev.md&quot;,
]</code></pre><h2 id="Testing-Your-Benchmark"><a class="docs-heading-anchor" href="#Testing-Your-Benchmark">Testing Your Benchmark</a><a id="Testing-Your-Benchmark-1"></a><a class="docs-heading-anchor-permalink" href="#Testing-Your-Benchmark" title="Permalink"></a></h2><ol><li><strong>Local testing:</strong> Run your script locally to verify it works</li><li><strong>Push changes:</strong> Commit and push all files</li><li><strong>Create PR:</strong> Open a pull request</li><li><strong>Add label:</strong> Add the <code>run bench &lt;name&gt;</code> label to trigger the workflow</li><li><strong>Monitor:</strong> Check the Actions tab to monitor execution</li></ol><h2 id="Troubleshooting"><a class="docs-heading-anchor" href="#Troubleshooting">Troubleshooting</a><a id="Troubleshooting-1"></a><a class="docs-heading-anchor-permalink" href="#Troubleshooting" title="Permalink"></a></h2><p><strong>Cache issues on self-hosted runners:</strong></p><ul><li>Ensure <code>runner: &#39;self-hosted&#39;</code> is set in your workflow</li><li>The reusable workflow uses <code>actions/cache</code> for artifacts only on self-hosted runners</li><li>If you see slow cache operations on self-hosted runners, verify the <code>runner</code> parameter is set correctly</li><li>Standard runners should NOT have the <code>runner</code> parameter (let it default to use <code>julia-actions/cache</code>)</li></ul><p><strong>Workflow not triggering:</strong></p><ul><li>Verify the label name matches exactly in the orchestrator</li><li>Check that the orchestrator&#39;s guard job includes your benchmark in outputs</li></ul><p><strong>Benchmark script fails:</strong></p><ul><li>Check Julia version compatibility</li><li>Verify all dependencies are available on the target runner</li><li>Review the benchmark function parameters</li></ul><h2 id="Examples"><a class="docs-heading-anchor" href="#Examples">Examples</a><a id="Examples-1"></a><a class="docs-heading-anchor-permalink" href="#Examples" title="Permalink"></a></h2><h3 id="Example-1:-Core-Moonshot-Benchmark-(CUDA-12)"><a class="docs-heading-anchor" href="#Example-1:-Core-Moonshot-Benchmark-(CUDA-12)">Example 1: Core Moonshot Benchmark (CUDA 12)</a><a id="Example-1:-Core-Moonshot-Benchmark-(CUDA-12)-1"></a><a class="docs-heading-anchor-permalink" href="#Example-1:-Core-Moonshot-Benchmark-(CUDA-12)" title="Permalink"></a></h3><p>A complete GPU benchmark using CUDA 12:</p><ul><li><strong>Script</strong>: <code>scripts/benchmark-core-moonshot.jl</code></li><li><strong>Workflow</strong>: <code>.github/workflows/benchmark-core-moonshot.yml</code></li><li><strong>Label</strong>: <code>run bench core moonshot</code></li><li><strong>Runner</strong>: <code>[&quot;self-hosted&quot;, &quot;Linux&quot;, &quot;gpu&quot;, &quot;cuda&quot;, &quot;cuda12&quot;]</code></li><li><strong>Documentation</strong>: <code>docs/src/benchmark-core.md.template</code></li></ul><h3 id="Example-2:-Core-Mothra-Benchmark-(CUDA-13)"><a class="docs-heading-anchor" href="#Example-2:-Core-Mothra-Benchmark-(CUDA-13)">Example 2: Core Mothra Benchmark (CUDA 13)</a><a id="Example-2:-Core-Mothra-Benchmark-(CUDA-13)-1"></a><a class="docs-heading-anchor-permalink" href="#Example-2:-Core-Mothra-Benchmark-(CUDA-13)" title="Permalink"></a></h3><p>A GPU benchmark identical to Moonshot but using CUDA 13 to compare performance:</p><ul><li><strong>Script</strong>: <code>scripts/benchmark-core-mothra.jl</code><ul><li>Only difference: <code>outpath</code> points to <code>benchmark-core-mothra</code></li></ul></li><li><strong>Workflow</strong>: <code>.github/workflows/benchmark-core-mothra.yml</code><ul><li>Only difference: <code>runs_on: &#39;[&quot;self-hosted&quot;, &quot;Linux&quot;, &quot;gpu&quot;, &quot;cuda&quot;, &quot;cuda13&quot;]&#39;</code></li></ul></li><li><strong>Label</strong>: <code>run bench core mothra</code></li><li><strong>Runner</strong>: <code>[&quot;self-hosted&quot;, &quot;Linux&quot;, &quot;gpu&quot;, &quot;cuda&quot;, &quot;cuda13&quot;]</code></li><li><strong>Orchestrator</strong>: Updated in 14 locations to integrate mothra</li></ul><p>This example demonstrates how to create a variant of an existing benchmark to test different hardware configurations.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="api.html">¬´ API</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.14.1 on <span class="colophon-date" title="Wednesday 15 October 2025 21:48">Wednesday 15 October 2025</span>. Using Julia version 1.12.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
