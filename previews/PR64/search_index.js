var documenterSearchIndex = {"docs":
[{"location":"api.html#API","page":"API","title":"API","text":"","category":"section"},{"location":"api.html","page":"API","title":"API","text":"Pages   = [\"api.md\"]\nModules = [CTBenchmarks]\nOrder = [:module, :constant, :type, :function, :macro]","category":"page"},{"location":"api.html#CTBenchmarks.benchmark-Tuple{}","page":"API","title":"CTBenchmarks.benchmark","text":"benchmark(;\n    outpath,\n    problems,\n    solver_models,\n    grid_sizes,\n    disc_methods,\n    tol,\n    ipopt_mu_strategy,\n    print_trace,\n    max_iter,\n    max_wall_time,\n    grid_size_max_cpu\n) -> String\n\nRun benchmarks on optimal control problems and save results to a JSON file.\n\nThis function performs the following steps:\n\nDetects CUDA availability and filters out :exa_gpu if CUDA is not functional\nRuns benchmarks using benchmark_data() to generate a DataFrame of results\nCollects environment metadata (Julia version, OS, machine, timestamp)\nBuilds a JSON-friendly payload combining results and metadata\nSaves the payload to outpath as pretty-printed JSON\n\nThe JSON file can be easily loaded and converted back to a DataFrame using:\n\nusing JSON, DataFrames\ndata = JSON.parsefile(\"path/to/data.json\")\ndf = DataFrame(data[\"results\"])\n\nArguments\n\noutpath: Path to save the JSON file\nproblems: Vector of problem names (Symbols)\nsolver_models: Vector of Pairs mapping solver => models (e.g., [:ipopt => [:JuMP, :adnlp], :madnlp => [:exa, :exa_gpu]])\ngrid_sizes: Vector of grid sizes (Int)\ndisc_methods: Vector of discretization methods (Symbols)\ntol: Solver tolerance (Float64)\nipopt_mu_strategy: Mu strategy for Ipopt (String)\nprint_trace: Boolean - whether to print solver output (for debugging)\nmax_iter: Maximum number of iterations (Int)\nmax_wall_time: Maximum wall time in seconds (Float64)\ngrid_size_max_cpu: Maximum grid size for CPU models (Int)\n\nReturns\n\nThe outpath of the saved JSON file.\n\n\n\n\n\n","category":"method"},{"location":"api.html#CTBenchmarks.benchmark_data-Tuple{}","page":"API","title":"CTBenchmarks.benchmark_data","text":"benchmark_data(;\n    problems,\n    solver_models,\n    grid_sizes,\n    disc_methods,\n    tol,\n    ipopt_mu_strategy,\n    print_trace\n    max_iter,\n    max_wall_time,\n    grid_size_max_cpu\n) -> DataFrame\n\nRun benchmarks on optimal control problems and return results as a DataFrame.\n\nFor each combination of problem, solver, model, and grid size, this function:\n\nSets up and solves the optimization problem\nCaptures timing and memory statistics using @btimed or CUDA.@timed\nExtracts solver statistics (objective value, iterations)\nStores all data in a DataFrame row\n\nArguments\n\nproblems: Vector of problem names (Symbols)\nsolver_models: Vector of Pairs mapping solver => models (e.g., [:ipopt => [:JuMP, :adnlp], :madnlp => [:exa, :exa_gpu]])\ngrid_sizes: Vector of grid sizes (Int)\ndisc_methods: Vector of discretization methods (Symbols)\ntol: Solver tolerance (Float64)\nipopt_mu_strategy: Mu strategy for Ipopt (String)\nprint_trace: Boolean - whether to print solver output (for debugging)\nmax_iter: Maximum number of iterations (Int)\nmax_wall_time: Maximum wall time in seconds (Float64)\n\nReturns\n\nA DataFrame with columns:\n\nproblem: Symbol - problem name\nsolver: Symbol - solver used (:ipopt or :madnlp)\nmodel: Symbol - model type (:JuMP, :adnlp, :exa, or :exa_gpu)\ndisc_method: Symbol - discretization method\ngrid_size: Int - number of grid points\ntol: Float64 - solver tolerance\nmu_strategy: Union{String, Missing} - mu strategy for Ipopt (missing for MadNLP)\nprint_level: Any - print level for solver (Int for Ipopt, MadNLP.LogLevels for MadNLP)\nmax_iter: Int - maximum number of iterations\nmax_wall_time: Float64 - maximum wall time in seconds\nbenchmark: NamedTuple - full benchmark object from @btimed or CUDA.@timed\nobjective: Union{Float64, Missing} - objective function value (missing if failed)\niterations: Union{Int, Missing} - number of solver iterations (missing if failed)\nstatus: Any - termination status (type depends on solver/model)\nsuccess: Bool - whether the solve succeeded\n\n\n\n\n\n","category":"method"},{"location":"api.html#CTBenchmarks.build_payload-Tuple{DataFrames.DataFrame, Dict}","page":"API","title":"CTBenchmarks.build_payload","text":"build_payload(results::DataFrame, meta::Dict) -> Dict\n\nCombine benchmark results DataFrame and metadata into a JSON-friendly dictionary. The DataFrame is converted to a vector of dictionaries (one per row) for easy JSON serialization and reconstruction.\n\n\n\n\n\n","category":"method"},{"location":"api.html#CTBenchmarks.filter_models_for_backend-Tuple{Vector{Symbol}, Symbol}","page":"API","title":"CTBenchmarks.filter_models_for_backend","text":"filter_models_for_backend(models::Vector{Symbol}, disc_method::Symbol) -> Vector{Symbol}\n\nFilter solver models depending on backend availability and discretization support.\n\nGPU models (ending with _gpu) are kept only if CUDA is available.\nJuMP models are kept only when disc_method == :trapeze.\n\n\n\n\n\n","category":"method"},{"location":"api.html#CTBenchmarks.generate_metadata-Tuple{}","page":"API","title":"CTBenchmarks.generate_metadata","text":"generate_metadata() -> Dict{String, String}\n\nReturn metadata about the current environment:\n\ntimestamp (UTC, ISO8601)\njulia_version\nos\nmachine hostname\npkg_status - output of Pkg.status() with ANSI colors\nversioninfo - output of versioninfo() with ANSI colors\npkg_manifest - output of Pkg.status(mode=PKGMODE_MANIFEST) with ANSI colors\n\n\n\n\n\n","category":"method"},{"location":"api.html#CTBenchmarks.is_cuda_on-Tuple{}","page":"API","title":"CTBenchmarks.is_cuda_on","text":"is_cuda_on() -> Bool\n\nReturn true if CUDA is functional on this machine.\n\n\n\n\n\n","category":"method"},{"location":"api.html#CTBenchmarks.prettymemory-Tuple{Any}","page":"API","title":"CTBenchmarks.prettymemory","text":"prettymemory(bytes::Integer) -> String\n\nFormat a memory footprint bytes into a human-readable string using binary prefixes (bytes, KiB, MiB, GiB) with two decimal places.\n\n\n\n\n\n","category":"method"},{"location":"api.html#CTBenchmarks.prettytime-Tuple{Any}","page":"API","title":"CTBenchmarks.prettytime","text":"prettytime(t::Real) -> String\n\nFormat a duration t expressed in seconds into a human-readable string with three decimal places and adaptive units (ns, Î¼s, ms, s).\n\n\n\n\n\n","category":"method"},{"location":"api.html#CTBenchmarks.print_benchmark_line-Tuple{Symbol, NamedTuple}","page":"API","title":"CTBenchmarks.print_benchmark_line","text":"print_benchmark_line(model::Symbol, stats::NamedTuple)\n\nPrint a formatted line summarizing benchmark statistics for model with colors. Handles both CPU benchmarks (from @btimed) and GPU benchmarks (from CUDA.@timed).\n\nDisplays: time, allocations/memory, objective, iterations, and success status\n\n\n\n\n\n","category":"method"},{"location":"api.html#CTBenchmarks.run","page":"API","title":"CTBenchmarks.run","text":"Run the benchmarks for a specific version.\n\nArguments\n\nversion::Symbol: version to run (:complete or :minimal)\noutpath::Union{AbstractString, Nothing}: directory path to save results (nothing for no saving)\nprint_trace::Bool: whether to print the trace of the solver\n\nReturns\n\nnothing\n\n\n\n\n\n","category":"function"},{"location":"api.html#CTBenchmarks.sanitize_for_json-Tuple{Any}","page":"API","title":"CTBenchmarks.sanitize_for_json","text":"sanitize_for_json(obj)\n\nRecursively replace NaN and Inf values with null for JSON compatibility.\n\n\n\n\n\n","category":"method"},{"location":"api.html#CTBenchmarks.save_json-Tuple{Dict, AbstractString}","page":"API","title":"CTBenchmarks.save_json","text":"save_json(payload::Dict, outpath::AbstractString)\n\nSave a JSON payload to a file. Creates the parent directory if needed. Uses pretty printing for readability. Sanitizes NaN and Inf values to null for JSON compatibility.\n\n\n\n\n\n","category":"method"},{"location":"api.html#CTBenchmarks.set_print_level-Tuple{Symbol, Bool}","page":"API","title":"CTBenchmarks.set_print_level","text":"set_print_level(solver::Symbol, print_trace::Bool) -> Int\n\nSet print level based on solver and print_trace flag.\n\n\n\n\n\n","category":"method"},{"location":"api.html#CTBenchmarks.solve_and_extract_data-Tuple{Symbol, Symbol, Symbol, Int64, Symbol, Float64, Union{Missing, String}, Bool, Int64, Float64}","page":"API","title":"CTBenchmarks.solve_and_extract_data","text":"solve_and_extract_data(problem, solver, model, grid_size, disc_method, \n                      tol, mu_strategy, print_level, max_iter, max_wall_time) -> NamedTuple\n\nSolve an optimal control problem and extract performance and solver statistics.\n\nThis internal helper function handles the solve process and data extraction for different model types (JuMP, adnlp, exa, exa_gpu).\n\nArguments\n\nproblem::Symbol: problem name (e.g., :beam, :chain)\nsolver::Symbol: solver to use (:ipopt or :madnlp)\nmodel::Symbol: model type (:JuMP, :adnlp, :exa, or :exa_gpu)\ngrid_size::Int: number of grid points\ndisc_method::Symbol: discretization method\ntol::Float64: solver tolerance\nmu_strategy::Union{String, Missing}: mu strategy for Ipopt (missing for MadNLP)\nprint_level::Union{Int, MadNLP.LogLevels, Missing}: print level for solver (Int for Ipopt, MadNLP.LogLevels for MadNLP)\nmax_iter::Int: maximum number of iterations\nmax_wall_time::Float64: maximum wall time in seconds\n\nReturns\n\nA NamedTuple with fields:\n\nbenchmark: full benchmark object from @btimed (CPU) or CUDA.@timed (GPU)\nobjective::Union{Float64, Missing}: objective function value (missing if failed)\niterations::Union{Int, Missing}: number of solver iterations (missing if failed)\nstatus::Any: termination status (type depends on solver/model)\nsuccess::Bool: whether the solve succeeded\n\n\n\n\n\n","category":"method"},{"location":"api.html#CTBenchmarks.strip_benchmark_value-Tuple{Any}","page":"API","title":"CTBenchmarks.strip_benchmark_value","text":"strip_benchmark_value(bench)\n\nRemove the value field from benchmark outputs (NamedTuple or Dict) to ensure JSON-serializable data while preserving all other statistics.\n\n\n\n\n\n","category":"method"},{"location":"dev.html#Development-Guidelines","page":"Development Guidelines","title":"Development Guidelines","text":"","category":"section"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"This guide explains how to add a new benchmark to the CTBenchmarks.jl pipeline.","category":"page"},{"location":"dev.html#Overview","page":"Development Guidelines","title":"Overview","text":"","category":"section"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Adding a new benchmark involves creating several interconnected components:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Benchmark script â­ Simple - Julia script that runs the benchmark\nGitHub Actions workflow â­ Simple - Workflow that executes the script on a specific runner\nGitHub label â­ Simple - Label to trigger the benchmark on pull requests (manual step on GitHub)\nOrchestrator integration â ï¸ Complex - Update the orchestrator to manage the new workflow (14 locations to modify)\nDocumentation page â­ Simple (optional) - Display benchmark results in the documentation","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"tip: Estimated Time\nSteps 1-3: ~10 minutes\nStep 4 (Orchestrator): ~30-45 minutes (careful verification required)\nStep 5: ~10 minutes","category":"page"},{"location":"dev.html#Step-by-Step-Guide","page":"Development Guidelines","title":"Step-by-Step Guide","text":"","category":"section"},{"location":"dev.html#1.-Create-the-Benchmark-Script","page":"Development Guidelines","title":"1. Create the Benchmark Script","text":"","category":"section"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Create a new Julia script in scripts/benchmark-<name>.jl:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"using Pkg\nconst project_dir = normpath(@__DIR__, \"..\")\nENV[\"PROJECT\"] = project_dir\n\nPkg.activate(project_dir)\nPkg.instantiate()\n\nusing CTBenchmarks\n\nfunction main()\n    outpath = joinpath(project_dir, \"docs\", \"src\", \"assets\", \"benchmark-<name>\")\n    CTBenchmarks.benchmark(;\n        outpath = outpath,\n        problems = [:problem1, :problem2, ...],\n        solver_models = [:solver => [:model1, :model2]],\n        grid_sizes = [100, 500, 1000],\n        disc_methods = [:trapeze],\n        tol = 1e-6,\n        ipopt_mu_strategy = \"adaptive\",\n        print_trace = false,\n        max_iter = 1000,\n        max_wall_time = 500.0\n    )\n    return outpath\nend\n\nmain()","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Key points:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"All parameters are required - the benchmark function has no optional arguments\nThe main() function is crucial - it must:\nTake no arguments\nReturn the output path where files are saved\nThe benchmark function generates JSON and TOML files in the specified outpath\nPrint statements (like println(\"ð¦ Activating...\")) are optional but helpful for debugging\nThe output directory follows the pattern docs/src/assets/benchmark-<name>\nAvailable problems: The list of problems you can choose is available in the OptimalControlProblems.jl documentation","category":"page"},{"location":"dev.html#2.-Create-the-GitHub-Actions-Workflow","page":"Development Guidelines","title":"2. Create the GitHub Actions Workflow","text":"","category":"section"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Create .github/workflows/benchmark-<name>.yml:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"name: Benchmark <Name>\n\non:\n  workflow_call:\n\npermissions:\n  contents: write\n  pull-requests: write\n\njobs:\n  bench:\n    uses: ./.github/workflows/benchmark-reusable.yml\n    with:\n      script_path: scripts/benchmark-<name>.jl\n      julia_version: '1.11'\n      julia_arch: x64\n      runs_on: '<runner-specification>'\n      runner: '<runner-type>'  # Only for self-hosted runners","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Runner configuration:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"For standard GitHub runners (e.g., ubuntu-latest):","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"runs_on: '\"ubuntu-latest\"'\n# Do NOT include the 'runner' parameter","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"For self-hosted runners (e.g., GPU machines):","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"runs_on: '[\"self-hosted\", \"Linux\", \"gpu\", \"cuda\", \"cuda12\"]'\nrunner: 'self-hosted'","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"All inputs are required except runner:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"script_path: Path to your benchmark script\njulia_version: Julia version to use (e.g., '1.11')\njulia_arch: Architecture (typically 'x64')\nruns_on: Runner specification (string or JSON array)\nrunner: Optional - Only set to 'self-hosted' for self-hosted runners","category":"page"},{"location":"dev.html#Understanding-Cache-Management","page":"Development Guidelines","title":"Understanding Cache Management","text":"","category":"section"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"The runner parameter controls the caching strategy:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Standard runners (omit runner parameter):","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Uses julia-actions/cache@v2\nCaches Julia artifacts, packages, AND registries\nCache stored on GitHub servers and restored on each run\nOptimal for ephemeral runners that start fresh each time","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Self-hosted runners (runner: 'self-hosted'):","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Uses actions/cache@v4 for artifacts only (~/.julia/artifacts)\nPackages and registries persist naturally on the machine between runs\nAvoids unnecessary upload/download to GitHub servers\nMore efficient since dependencies are already local","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Why this matters: Self-hosted runners maintain their filesystem between runs. Using julia-actions/cache would wastefully upload/download gigabytes of data to/from GitHub when the files are already on the machine. We only cache artifacts to avoid re-downloading external dependencies.","category":"page"},{"location":"dev.html#3.-Create-the-GitHub-Label","page":"Development Guidelines","title":"3. Create the GitHub Label","text":"","category":"section"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"On GitHub, create a new label for your benchmark:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Go to your repository â Issues â Labels\nClick New label\nName: run bench <name> (e.g., run bench core moonshot)\nChoose a color and description\nClick Create label","category":"page"},{"location":"dev.html#4.-Update-the-Orchestrator","page":"Development Guidelines","title":"4. Update the Orchestrator","text":"","category":"section"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"warning: Complex Integration\nThis is the most complex and error-prone step. The orchestrator requires modifications in 14 different locations throughout the file. Missing even one location will cause workflow failures. Take your time and verify each step.","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Edit .github/workflows/benchmarks-orchestrator.yml to integrate your new benchmark. Follow these steps carefully:","category":"page"},{"location":"dev.html#Step-4.1:-Add-output-in-the-guard-job","page":"Development Guidelines","title":"Step 4.1: Add output in the guard job","text":"","category":"section"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"In the guard job outputs section (~line 16-20):","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"jobs:\n  guard:\n    outputs:\n      run_ubuntu: ${{ steps.check.outputs.run_ubuntu }}\n      run_moonshot: ${{ steps.check.outputs.run_moonshot }}\n      run_<name>: ${{ steps.check.outputs.run_<name> }}  # Add this line\n      benchmarks_summary: ${{ steps.check.outputs.benchmarks_summary }}","category":"page"},{"location":"dev.html#Step-4.2:-Initialize-the-variable","page":"Development Guidelines","title":"Step 4.2: Initialize the variable","text":"","category":"section"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"In the guard job's check step, initialize the variable (~line 30-33):","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"# Initialize outputs\nRUN_UBUNTU=\"false\"\nRUN_MOONSHOT=\"false\"\nRUN_<NAME>=\"false\"  # Add this line\nBENCHMARKS_LIST=\"\"","category":"page"},{"location":"dev.html#Step-4.3:-Update-\"run-all\"-label-detection","page":"Development Guidelines","title":"Step 4.3: Update \"run all\" label detection","text":"","category":"section"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"When the \"run bench core all\" label is detected (~line 57-64), add your benchmark:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"if echo \"$LABELS\" | grep -q \"run bench core all\"; then\n  echo \"â Found 'run bench core all' label\"\n  RUN_UBUNTU=\"true\"\n  RUN_MOONSHOT=\"true\"\n  RUN_<NAME>=\"true\"  # Add this line\n  BENCHMARKS_LIST=\"ubuntu-latest, moonshot, <name>\"  # Add <name> here","category":"page"},{"location":"dev.html#Step-4.4:-Add-specific-label-detection","page":"Development Guidelines","title":"Step 4.4: Add specific label detection","text":"","category":"section"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"After the moonshot label detection block (~line 68-77), add detection for your benchmark:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"if echo \"$LABELS\" | grep -q \"run bench core <name>\"; then\n  echo \"â Found 'run bench core <name>' label\"\n  RUN_<NAME>=\"true\"\n  if [ -n \"$BENCHMARKS_LIST\" ]; then\n    BENCHMARKS_LIST=\"$BENCHMARKS_LIST, <name>\"\n  else\n    BENCHMARKS_LIST=\"<name>\"\n  fi\nfi","category":"page"},{"location":"dev.html#Step-4.5:-Update-\"no-labels\"-condition","page":"Development Guidelines","title":"Step 4.5: Update \"no labels\" condition","text":"","category":"section"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Update the condition that checks if no benchmarks were selected (~line 79-83):","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"if [ \"$RUN_UBUNTU\" == \"false\" ] && [ \"$RUN_MOONSHOT\" == \"false\" ] && [ \"$RUN_<NAME>\" == \"false\" ]; then\n  echo \"â No benchmark labels found\"\n  echo \"â¹ï¸  Expected labels: 'run bench core ubuntu', 'run bench core moonshot', 'run bench core <name>', or 'run bench core all'\"\n  BENCHMARKS_LIST=\"none\"\nfi","category":"page"},{"location":"dev.html#Step-4.6:-Set-the-output","page":"Development Guidelines","title":"Step 4.6: Set the output","text":"","category":"section"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Add the output for your benchmark (~line 88-91):","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"# Set outputs\necho \"run_ubuntu=$RUN_UBUNTU\" >> $GITHUB_OUTPUT\necho \"run_moonshot=$RUN_MOONSHOT\" >> $GITHUB_OUTPUT\necho \"run_<name>=$RUN_<NAME>\" >> $GITHUB_OUTPUT  # Add this line\necho \"benchmarks_summary=$BENCHMARKS_LIST\" >> $GITHUB_OUTPUT","category":"page"},{"location":"dev.html#Step-4.7:-Update-guard-summary-logs","page":"Development Guidelines","title":"Step 4.7: Update guard summary logs","text":"","category":"section"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"In the guard decision summary step (~line 98-117), add logging for your benchmark:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"RUN_UBUNTU=\"${{ steps.check.outputs.run_ubuntu }}\"\nRUN_MOONSHOT=\"${{ steps.check.outputs.run_moonshot }}\"\nRUN_<NAME>=\"${{ steps.check.outputs.run_<name> }}\"  # Add this line\nSUMMARY=\"${{ steps.check.outputs.benchmarks_summary }}\"\n\nif [ \"$RUN_UBUNTU\" == \"true\" ]; then\n  echo \"  â benchmark-core-ubuntu-latest\"\nfi\nif [ \"$RUN_MOONSHOT\" == \"true\" ]; then\n  echo \"  â benchmark-core-moonshot\"\nfi\nif [ \"$RUN_<NAME>\" == \"true\" ]; then  # Add this block\n  echo \"  â benchmark-core-<name>\"\nfi\nif [ \"$RUN_UBUNTU\" != \"true\" ] && [ \"$RUN_MOONSHOT\" != \"true\" ] && [ \"$RUN_<NAME>\" != \"true\" ]; then\n  echo \"  â­ï¸  None (conditions not met)\"\n  echo \"\"\n  echo \"ð¡ To run benchmarks on PRs, ensure:\"\n  echo \"   â¢ PR targets 'main' branch\"\n  echo \"   â¢ PR has one of: 'run bench core ubuntu', 'run bench core moonshot', 'run bench core <name>', or 'run bench core all'\"","category":"page"},{"location":"dev.html#Step-4.8:-Add-the-benchmark-job","page":"Development Guidelines","title":"Step 4.8: Add the benchmark job","text":"","category":"section"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"After the existing benchmark jobs (~line 124-127), add your new job:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"benchmark-<name>:\n  needs: guard\n  if: needs.guard.outputs.run_<name> == 'true'\n  uses: ./.github/workflows/benchmark-<name>.yml","category":"page"},{"location":"dev.html#Step-4.9:-Update-docs-job-dependencies-and-conditions","page":"Development Guidelines","title":"Step 4.9: Update docs job dependencies and conditions","text":"","category":"section"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Update the docs job (~line 129-138):","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"docs:\n  needs: [guard, benchmark-ubuntu, benchmark-moonshot, benchmark-<name>]  # Add benchmark-<name>\n  if: |\n    always() &&\n    (needs.guard.result == 'success') &&\n    (needs.benchmark-ubuntu.result != 'cancelled') &&\n    (needs.benchmark-moonshot.result != 'cancelled') &&\n    (needs.benchmark-<name>.result != 'cancelled') &&  # Add this line\n    (needs.benchmark-ubuntu.result != 'failure') &&\n    (needs.benchmark-moonshot.result != 'failure') &&\n    (needs.benchmark-<name>.result != 'failure')  # Add this line","category":"page"},{"location":"dev.html#Step-4.10:-Update-notify-failure-job","page":"Development Guidelines","title":"Step 4.10: Update notify-failure job","text":"","category":"section"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Update the notify-failure job dependencies (~line 167-168):","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"notify-failure:\n  needs: [guard, benchmark-ubuntu, benchmark-moonshot, benchmark-<name>, docs]  # Add benchmark-<name>","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"And add failure detection in the script (~line 182-193):","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"if (needs['benchmark-<name>'] && needs['benchmark-<name>'].result === 'failure') {\n  console.log('â Benchmark <Name> job failed');\n  failedJobs.push('Benchmark <Name>');\n}","category":"page"},{"location":"dev.html#Step-4.11:-Update-notify-success-job","page":"Development Guidelines","title":"Step 4.11: Update notify-success job","text":"","category":"section"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Update the notify-success job dependencies and conditions (~line 229-238):","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"notify-success:\n  needs: [guard, benchmark-ubuntu, benchmark-moonshot, benchmark-<name>, docs]  # Add benchmark-<name>\n  if: |\n    always() &&\n    (needs.guard.result == 'success') &&\n    (needs.docs.result == 'success') &&\n    (needs.benchmark-ubuntu.result != 'cancelled') &&\n    (needs.benchmark-moonshot.result != 'cancelled') &&\n    (needs.benchmark-<name>.result != 'cancelled') &&  # Add this line\n    (needs.benchmark-ubuntu.result != 'failure') &&\n    (needs.benchmark-moonshot.result != 'failure') &&\n    (needs.benchmark-<name>.result != 'failure')  # Add this line","category":"page"},{"location":"dev.html#Step-4.12:-Update-workflow-summary-job","page":"Development Guidelines","title":"Step 4.12: Update workflow-summary job","text":"","category":"section"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Update the workflow-summary job dependencies (~line 317-318):","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"workflow-summary:\n  needs: [guard, benchmark-ubuntu, benchmark-moonshot, benchmark-<name>, docs]  # Add benchmark-<name>","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"And add summary logging (~line 340-346):","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"if [ \"${{ needs.benchmark-<name>.result }}\" == \"success\" ]; then\n  echo \"ð Benchmark <Name>: â SUCCESS\"\nelif [ \"${{ needs.benchmark-<name>.result }}\" == \"failure\" ]; then\n  echo \"ð Benchmark <Name>: â FAILED\"\nelif [ \"${{ needs.benchmark-<name>.result }}\" == \"skipped\" ]; then\n  echo \"ð Benchmark <Name>: â­ï¸  SKIPPED\"\nfi","category":"page"},{"location":"dev.html#Step-4.13:-Update-overall-status-check","page":"Development Guidelines","title":"Step 4.13: Update overall status check","text":"","category":"section"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Update the overall status condition (~line 362-365):","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"overall_status=\"â SUCCESS\"\nif [ \"${{ needs.benchmark-ubuntu.result }}\" == \"failure\" ] || \n   [ \"${{ needs.benchmark-moonshot.result }}\" == \"failure\" ] || \n   [ \"${{ needs.benchmark-<name>.result }}\" == \"failure\" ] ||  # Add this line\n   [ \"${{ needs.docs.result }}\" == \"failure\" ]; then\n  overall_status=\"â FAILED\"\nfi","category":"page"},{"location":"dev.html#Verification-Checklist","page":"Development Guidelines","title":"Verification Checklist","text":"","category":"section"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Before committing your changes, verify that you have updated all 14 locations:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"[ ] Step 4.1: Guard job outputs (add run_<name>)\n[ ] Step 4.2: Variable initialization (add RUN_<NAME>=\"false\")\n[ ] Step 4.3: \"Run all\" label detection (add RUN_<NAME>=\"true\" and update BENCHMARKS_LIST)\n[ ] Step 4.4: Specific label detection (add new if block for your label)\n[ ] Step 4.5: No labels condition (add RUN_<NAME> check and update message)\n[ ] Step 4.6: Set outputs (add echo \"run_<name>=$RUN_<NAME>\")\n[ ] Step 4.7: Guard summary logs (add RUN_<NAME> variable and logging block)\n[ ] Step 4.8: New benchmark job (add complete job definition)\n[ ] Step 4.9: Docs job (add to needs list and two condition lines)\n[ ] Step 4.10: Notify-failure job (add to needs list and failure detection)\n[ ] Step 4.11: Notify-success job (add to needs list and two condition lines)\n[ ] Step 4.12: Workflow-summary job (add to needs list and logging block)\n[ ] Step 4.13: Overall status check (add to failure condition)","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Tip: Use grep -n \"<name>\" .github/workflows/benchmarks-orchestrator.yml to verify all occurrences of your benchmark name are present.","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Important: All these modifications must be done consistently. Missing even one location can cause the workflow to fail or behave unexpectedly.","category":"page"},{"location":"dev.html#5.-Create-Documentation-Page-(Optional)","page":"Development Guidelines","title":"5. Create Documentation Page (Optional)","text":"","category":"section"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"If you want to display results in the documentation, create docs/src/benchmark-<name>.md.template:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"# <Name> Benchmark\n\n```@setup BENCH_<NAME>\ninclude(joinpath(@__DIR__, \"assets\", \"utils.jl\"))\n\nconst BENCH_DIR = \"benchmark-<name>\"\nconst BENCH_DATA = _read_benchmark_json(joinpath(@__DIR__, \"assets\", BENCH_DIR, \"data.json\"))\n```\n\n## Description\n\nBrief description of your benchmark configuration.\n\n**Benchmark Configuration:**\n\n- **Solvers:** List of solvers\n- **Models:** List of models\n- **Grid sizes:** Discretisation points\n- **Tolerance:** 1e-6\n- **Limits:** Max iterations and wall time\n\n### ð¥ï¸ Environment\n\n<!-- INCLUDE_ENVIRONMENT:\nBENCH_DATA = BENCH_DATA\nBENCH_DIR = BENCH_DIR\nENV_NAME = BENCH_<NAME>\n-->\n\n### ð Results\n\n```@example BENCH_<NAME>\n_print_results(BENCH_DATA) # hide\nnothing # hide\n```","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Then add it to docs/make.jl:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"pages = [\n    \"Introduction\" => \"index.md\",\n    \"Core benchmark\" => \"benchmark-core.md\",\n    \"<Name> Benchmark\" => \"benchmark-<name>.md\",\n    \"API\" => \"api.md\",\n    \"Development Guidelines\" => \"dev.md\",\n]","category":"page"},{"location":"dev.html#Testing-Your-Benchmark","page":"Development Guidelines","title":"Testing Your Benchmark","text":"","category":"section"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Local testing: Run your script locally to verify it works\nPush changes: Commit and push all files\nCreate PR: Open a pull request\nAdd label: Add the run bench <name> label to trigger the workflow\nMonitor: Check the Actions tab to monitor execution","category":"page"},{"location":"dev.html#Troubleshooting","page":"Development Guidelines","title":"Troubleshooting","text":"","category":"section"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Cache issues on self-hosted runners:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Ensure runner: 'self-hosted' is set in your workflow\nThe reusable workflow uses actions/cache for artifacts only on self-hosted runners\nIf you see slow cache operations on self-hosted runners, verify the runner parameter is set correctly\nStandard runners should NOT have the runner parameter (let it default to use julia-actions/cache)","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Workflow not triggering:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Verify the label name matches exactly in the orchestrator\nCheck that the orchestrator's guard job includes your benchmark in outputs","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Benchmark script fails:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Check Julia version compatibility\nVerify all dependencies are available on the target runner\nReview the benchmark function parameters","category":"page"},{"location":"dev.html#Examples","page":"Development Guidelines","title":"Examples","text":"","category":"section"},{"location":"dev.html#Example-1:-Core-Moonshot-Benchmark-(CUDA-12)","page":"Development Guidelines","title":"Example 1: Core Moonshot Benchmark (CUDA 12)","text":"","category":"section"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"A complete GPU benchmark using CUDA 12:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Script: scripts/benchmark-core-moonshot.jl\nWorkflow: .github/workflows/benchmark-core-moonshot.yml\nLabel: run bench core moonshot\nRunner: [\"self-hosted\", \"Linux\", \"gpu\", \"cuda\", \"cuda12\"]\nDocumentation: docs/src/benchmark-core.md.template","category":"page"},{"location":"dev.html#Example-2:-Core-Mothra-Benchmark-(CUDA-13)","page":"Development Guidelines","title":"Example 2: Core Mothra Benchmark (CUDA 13)","text":"","category":"section"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"A GPU benchmark identical to Moonshot but using CUDA 13 to compare performance:","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"Script: scripts/benchmark-core-mothra.jl\nOnly difference: outpath points to benchmark-core-mothra\nWorkflow: .github/workflows/benchmark-core-mothra.yml\nOnly difference: runs_on: '[\"self-hosted\", \"Linux\", \"gpu\", \"cuda\", \"cuda13\"]'\nLabel: run bench core mothra\nRunner: [\"self-hosted\", \"Linux\", \"gpu\", \"cuda\", \"cuda13\"]\nOrchestrator: Updated in 14 locations to integrate mothra","category":"page"},{"location":"dev.html","page":"Development Guidelines","title":"Development Guidelines","text":"This example demonstrates how to create a variant of an existing benchmark to test different hardware configurations.","category":"page"},{"location":"benchmark-core.html#Core-benchmark","page":"Core benchmark","title":"Core benchmark","text":"","category":"section"},{"location":"benchmark-core.html#Ubuntu-Latest","page":"Core benchmark","title":"Ubuntu Latest","text":"","category":"section"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"This benchmark suite evaluates 14 optimal control problems on a standard CPU platform using GitHub Actions runners.","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"Benchmark Configuration:","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"Solvers: Ipopt and MadNLP\nModels: JuMP, ADNLPModels, ExaModels (CPU)\nGrid sizes: 200, 500, 1000 discretisation points\nDiscretisation: Trapeze method\nTolerance: 1e-6\nIpopt strategy: Adaptive barrier parameter\nLimits: 1000 iterations max, 500s wall time","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"This configuration focuses on CPU-based solvers and provides a comprehensive comparison across different modelling frameworks.","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"note: Note\nThe linear solver is MUMPS for all experiments.","category":"page"},{"location":"benchmark-core.html#Environment","page":"Core benchmark","title":"ð¥ï¸ Environment","text":"","category":"section"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"_basic_metadata(BENCH_DATA_UBUNTU) # hide","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"_downloads_toml(BENCH_DIR_UBUNTU) # hide","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"<details style=\"margin-bottom: 0.5em; margin-top: 0.5em;\"><summary>â¹ï¸ Version info</summary>","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"_bench_data(BENCH_DATA_UBUNTU) # hide","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"</details>","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"<details style=\"margin-bottom: 0.5em;\"><summary>ð¦ Package status</summary>","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"_package_status(BENCH_DATA_UBUNTU) # hide","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"</details>","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"<details style=\"margin-bottom: 0.5em;\"><summary>ð Complete manifest</summary>","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"_complete_manifest(BENCH_DATA_UBUNTU) # hide","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"</details>","category":"page"},{"location":"benchmark-core.html#Results","page":"Core benchmark","title":"ð Results","text":"","category":"section"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"_print_results(BENCH_DATA_UBUNTU) # hide\nnothing # hide","category":"page"},{"location":"benchmark-core.html#Moonshot","page":"Core benchmark","title":"Moonshot","text":"","category":"section"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"This benchmark suite evaluates 14 optimal control problems on GPU-accelerated hardware, focusing on large-scale problems.","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"Benchmark Configuration:","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"Solver: MadNLP (GPU-optimised)\nModels: ExaModels (CPU), ExaModels (GPU)\nGrid sizes: 1000, 5000, 10000 discretisation points\nDiscretisation: Trapeze method\nTolerance: 1e-6\nLimits: 1000 iterations max, 1000s wall time","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"This configuration demonstrates GPU acceleration capabilities with ExaModels on large-scale problems, comparing CPU vs GPU performance for the same modelling framework.","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"note: Note\nThe linear solver is MUMPS for all experiments.","category":"page"},{"location":"benchmark-core.html#Environment-2","page":"Core benchmark","title":"ð Environment","text":"","category":"section"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"_basic_metadata(BENCH_DATA_MOONSHOT) # hide","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"_downloads_toml(BENCH_DIR_MOONSHOT) # hide","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"<details style=\"margin-bottom: 0.5em; margin-top: 0.5em;\"><summary>â¹ï¸ Version info</summary>","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"_bench_data(BENCH_DATA_MOONSHOT) # hide","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"</details>","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"<details style=\"margin-bottom: 0.5em;\"><summary>ð¦ Package status</summary>","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"_package_status(BENCH_DATA_MOONSHOT) # hide","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"</details>","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"<details style=\"margin-bottom: 0.5em;\"><summary>ð Complete manifest</summary>","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"_complete_manifest(BENCH_DATA_MOONSHOT) # hide","category":"page"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"</details>","category":"page"},{"location":"benchmark-core.html#Results-2","page":"Core benchmark","title":"â¡ Results","text":"","category":"section"},{"location":"benchmark-core.html","page":"Core benchmark","title":"Core benchmark","text":"_print_results(BENCH_DATA_MOONSHOT) # hide\nnothing # hide","category":"page"},{"location":"index.html#CTBenchmarks","page":"Introduction","title":"CTBenchmarks","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"CTBenchmarks.jl is a comprehensive benchmarking suite for optimal control problems, designed to evaluate and compare the performance of different solvers and modelling approaches within the control-toolbox ecosystem.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"This package provides:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"ð Pre-configured benchmark suites for quick performance evaluation\nð Automated result collection and analysis\nð§ Flexible API for creating custom benchmarks\nð Detailed performance metrics including timing, memory usage, and solver statistics","category":"page"},{"location":"index.html#Installation","page":"Introduction","title":"Installation","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"warning: Development Package\nCTBenchmarks.jl is not yet registered in the Julia General Registry. You must clone the repository to use it.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"To install CTBenchmarks.jl, clone the repository and activate the project:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"using Pkg\n\n# Clone the repository\nPkg.develop(url=\"https://github.com/control-toolbox/CTBenchmarks.jl\")\n\n# Or clone manually and activate\n# git clone https://github.com/control-toolbox/CTBenchmarks.jl.git\n# cd CTBenchmarks.jl\n# julia --project=.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Once installed, load the package:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"using CTBenchmarks","category":"page"},{"location":"index.html#Quick-Start","page":"Introduction","title":"Quick Start","text":"","category":"section"},{"location":"index.html#Running-Pre-configured-Benchmarks","page":"Introduction","title":"Running Pre-configured Benchmarks","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"CTBenchmarks provides two pre-configured benchmark suites:","category":"page"},{"location":"index.html#Minimal-Benchmark-(Fast)","page":"Introduction","title":"Minimal Benchmark (Fast)","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Run a quick benchmark on a single problem to test your setup:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"CTBenchmarks.run(:minimal)","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"This runs the :beam problem with:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Grid size: 100\nDiscretisation: trapeze\nSolvers: Ipopt and MadNLP\nModels: JuMP, adnlp, exa, exa_gpu","category":"page"},{"location":"index.html#Complete-Benchmark-(Comprehensive)","page":"Introduction","title":"Complete Benchmark (Comprehensive)","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Run the full benchmark suite across all problems:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"CTBenchmarks.run(:complete)","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"This runs 14 optimal control problems with:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Grid sizes: 100, 200, 500\nDiscretisations: trapeze, midpoint\nSolvers: Ipopt and MadNLP\nModels: JuMP, adnlp, exa, exa_gpu","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"tip: Solver Output\nBy default, solver output is suppressed. To see detailed solver traces, use:CTBenchmarks.run(:minimal; print_trace=true)","category":"page"},{"location":"index.html#Saving-Results","page":"Introduction","title":"Saving Results","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"To save benchmark results to a directory:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"CTBenchmarks.run(:minimal; outpath=\"my_results\")","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"This creates a directory containing:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"data.json - Benchmark results in JSON format\nProject.toml - Package dependencies\nManifest.toml - Complete dependency tree","category":"page"},{"location":"index.html#Creating-Custom-Benchmarks","page":"Introduction","title":"Creating Custom Benchmarks","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"For more control over your benchmarks, use the CTBenchmarks.benchmark function directly:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"CTBenchmarks.benchmark(;\n    outpath = \"custom_benchmark\",\n    problems = [:beam, :chain, :robot],\n    solver_models = [\n        :ipopt => [:JuMP, :adnlp, :exa],\n        :madnlp => [:exa, :exa_gpu]\n    ],\n    grid_sizes = [200, 500, 1000],\n    disc_methods = [:trapeze],\n    tol = 1e-6,\n    ipopt_mu_strategy = \"adaptive\",\n    print_trace = false,\n    max_iter = 1000,\n    max_wall_time = 500.0\n)","category":"page"},{"location":"index.html#Available-Problems","page":"Introduction","title":"Available Problems","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"CTBenchmarks includes 14 optimal control problems from OptimalControlProblems.jl:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":":beam - Beam control problem\n:chain - Chain of masses\n:double_oscillator - Double oscillator\n:ducted_fan - Ducted fan control\n:electric_vehicle - Electric vehicle optimisation\n:glider - Glider trajectory\n:insurance - Insurance problem\n:jackson - Jackson problem\n:robbins - Robbins problem\n:robot - Robot arm control\n:rocket - Rocket trajectory\n:space_shuttle - Space shuttle re-entry\n:steering - Steering control\n:vanderpol - Van der Pol oscillator","category":"page"},{"location":"index.html#Solver-and-Model-Combinations","page":"Introduction","title":"Solver and Model Combinations","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Supported Solvers:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":":ipopt - Interior Point Optimizer\n:madnlp - Matrix-free Augmented Lagrangian NLP solver","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Supported Models:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":":JuMP - JuMP modelling framework\n:adnlp - Automatic differentiation NLP models\n:exa - ExaModels (CPU)\n:exa_gpu - ExaModels (GPU acceleration)","category":"page"},{"location":"index.html#Benchmark-Parameters","page":"Introduction","title":"Benchmark Parameters","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"grid_sizes: Number of discretisation points (e.g., [100, 200, 500])\ndisc_methods: Discretisation schemes (:trapeze, :midpoint)\ntol: Solver tolerance (default: 1e-6)\nmax_iter: Maximum solver iterations (default: 1000)\nmax_wall_time: Maximum wall time in seconds (default: 500.0)","category":"page"},{"location":"index.html#Benchmark-Results-in-This-Documentation","page":"Introduction","title":"Benchmark Results in This Documentation","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"This documentation includes pre-computed benchmark results from continuous integration runs on different platforms:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Ubuntu Latest - Standard CPU benchmarks on GitHub Actions runners\nMoonshot - GPU-accelerated benchmarks on dedicated hardware","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"These results provide reference performance data and demonstrate the capabilities of different solver and model combinations. You can explore them in the Core Benchmark section.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Each benchmark result page includes:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"ð Performance metrics (time, memory, iterations)\nð¥ï¸ Environment information (Julia version, OS, hardware)\nð Reproducible benchmark scripts\nð¦ Complete dependency information","category":"page"},{"location":"index.html#Understanding-Benchmark-Output","page":"Introduction","title":"Understanding Benchmark Output","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"When you run a benchmark, you'll see output similar to:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Benchmarks results:\n\nââ problem: beam\nâ\nââââ¬ solver: ipopt, disc_method: trapeze\nâ  â\nâ  â  N : 100\nâ  â  â | JuMP    :    1.234 s | obj: 1.234567e+00 | iters: 42    | CPU:    2.5 MiB\nâ  â  â | adnlp   :    0.987 s | obj: 1.234567e+00 | iters: 42    | CPU:    2.1 MiB\nâ  â  â | exa     :    0.765 s | obj: 1.234567e+00 | iters: 42    | CPU:    1.8 MiB\nâ  ââ\nââ","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Legend:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"â / â - Success or failure indicator\nTime - Total solve time (right-aligned for easy comparison)\nobj - Objective function value\niters - Number of solver iterations\nMemory - CPU memory usage (GPU memory shown separately for GPU models)","category":"page"},{"location":"index.html#Documentation-build-environment","page":"Introduction","title":"Documentation build environment","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"_downloads_toml(\".\") # hide","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"<details style=\"margin-bottom: 0.5em; margin-top: 1em;\"><summary>â¹ï¸ Version info</summary>","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"versioninfo() # hide","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"</details>","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"<details style=\"margin-bottom: 0.5em;\"><summary>ð¦ Package status</summary>","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Pkg.status() # hide","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"</details>","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"<details style=\"margin-bottom: 0.5em;\"><summary>ð Complete manifest</summary>","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Pkg.status(; mode = PKGMODE_MANIFEST) # hide","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"</details>","category":"page"}]
}
