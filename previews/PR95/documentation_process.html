<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Documentation Process · CTBenchmarks</title><meta name="title" content="Documentation Process · CTBenchmarks"/><meta property="og:title" content="Documentation Process · CTBenchmarks"/><meta property="twitter:title" content="Documentation Process · CTBenchmarks"/><meta name="description" content="Documentation for CTBenchmarks."/><meta property="og:description" content="Documentation for CTBenchmarks."/><meta property="twitter:description" content="Documentation for CTBenchmarks."/><script data-outdated-warner src="assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="search_index.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script><link href="https://control-toolbox.org/assets/css/documentation.css" rel="stylesheet" type="text/css"/><script src="https://control-toolbox.org/assets/js/documentation.js"></script><script src="assets/js/ctbenchmarks-details.js"></script><link href="assets/css/ctbenchmarks-details.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="index.html">CTBenchmarks</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="index.html">Introduction</a></li><li><a class="tocitem" href="performance_profile.html">Performance Profile</a></li><li><span class="tocitem">Core benchmarks</span><ul><li><a class="tocitem" href="core/cpu.html">CPU</a></li><li><a class="tocitem" href="core/gpu.html">GPU</a></li><li><input class="collapse-toggle" id="menuitem-3-3" type="checkbox"/><label class="tocitem" for="menuitem-3-3"><span class="docs-label">Problems</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="core/problems/beam.html">beam</a></li><li><a class="tocitem" href="core/problems/chain.html">chain</a></li><li><a class="tocitem" href="core/problems/double_oscillator.html">double_oscillator</a></li><li><a class="tocitem" href="core/problems/electric_vehicle.html">electric_vehicle</a></li><li><a class="tocitem" href="core/problems/glider.html">glider</a></li><li><a class="tocitem" href="core/problems/insurance.html">insurance</a></li><li><a class="tocitem" href="core/problems/jackson.html">jackson</a></li><li><a class="tocitem" href="core/problems/robbins.html">robbins</a></li><li><a class="tocitem" href="core/problems/robot.html">robot</a></li><li><a class="tocitem" href="core/problems/rocket.html">rocket</a></li><li><a class="tocitem" href="core/problems/space_shuttle.html">space_shuttle</a></li><li><a class="tocitem" href="core/problems/steering.html">steering</a></li><li><a class="tocitem" href="core/problems/vanderpol.html">vanderpol</a></li></ul></li></ul></li><li><span class="tocitem">API Reference</span><ul><li><a class="tocitem" href="api/public.html">Public</a></li><li><a class="tocitem" href="api/private.html">Private</a></li></ul></li><li><span class="tocitem">Developers Guidelines</span><ul><li><a class="tocitem" href="add_benchmark.html">Add a New Benchmark</a></li><li class="is-active"><a class="tocitem" href="documentation_process.html">Documentation Process</a><ul class="internal"><li><a class="tocitem" href="#High-Level-Overview"><span>High-Level Overview</span></a></li><li><a class="tocitem" href="#Documentation-Utilities-Directory-(docs/src/docutils)"><span>Documentation Utilities Directory (<code>docs/src/docutils</code>)</span></a></li><li><a class="tocitem" href="#docs/make.jl:-Orchestrating-the-Build"><span><code>docs/make.jl</code>: Orchestrating the Build</span></a></li><li><a class="tocitem" href="#Automatic-Problem-Pages"><span>Automatic Problem Pages</span></a></li><li><a class="tocitem" href="#Template-Processing-and-Special-Blocks"><span>Template Processing and Special Blocks</span></a></li><li><a class="tocitem" href="#Figure-Types,-Analysis,-and-Helper-Functions"><span>Figure Types, Analysis, and Helper Functions</span></a></li><li><a class="tocitem" href="#Adding-Documentation-for-a-Benchmark"><span>Adding Documentation for a Benchmark</span></a></li><li><a class="tocitem" href="#Summary"><span>Summary</span></a></li></ul></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Developers Guidelines</a></li><li class="is-active"><a href="documentation_process.html">Documentation Process</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="documentation_process.html">Documentation Process</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/control-toolbox/CTBenchmarks.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="documentation-process"><a class="docs-heading-anchor" href="#documentation-process">Documentation Generation Process</a><a id="documentation-process-1"></a><a class="docs-heading-anchor-permalink" href="#documentation-process" title="Permalink"></a></h1><p>This page explains how the CTBenchmarks.jl documentation is generated and how benchmark results are turned into rich documentation pages with figures, tables, and environment information.</p><p>It is mainly intended for developers who want to:</p><ul><li><strong>Understand</strong> the <code>docs/make.jl</code> pipeline.</li><li><strong>Add documentation</strong> for a new benchmark.</li><li><strong>Extend</strong> the existing template/figure system.</li></ul><p>If you only want to add a benchmark to the CI pipeline, see <code>Add a new benchmark</code> first. For documentation-specific details, come back to this page.</p><hr/><h2 id="High-Level-Overview"><a class="docs-heading-anchor" href="#High-Level-Overview">High-Level Overview</a><a id="High-Level-Overview-1"></a><a class="docs-heading-anchor-permalink" href="#High-Level-Overview" title="Permalink"></a></h2><p>The documentation build has three main stages:</p><ol><li><p><strong>Prepare the environment and utilities</strong></p><ul><li>Copy <code>Project.toml</code> and <code>Manifest.toml</code> under <code>docs/src/assets/toml/</code>.</li><li>Load documentation utilities from <code>docs/src/docutils/utils.jl</code>.</li></ul></li><li><p><strong>Generate and process templates</strong></p><ul><li>Automatically generate <code>.md.template</code> files for per-problem pages (core benchmark problems).</li><li>Process template files (including manual templates) to produce temporary <code>.md</code> files that Documenter can read.</li><li>While processing templates, replace special blocks such as <code>INCLUDE_ENVIRONMENT</code>, <code>INCLUDE_FIGURE</code>, and <code>INCLUDE_TEXT</code> (with legacy <code>INCLUDE_ANALYSIS</code> still supported as an alias).</li></ul></li><li><p><strong>Build and deploy documentation</strong></p><ul><li>Call <code>makedocs</code> with the processed <code>.md</code> files.</li><li>Clean up all generated templates and figures.</li><li>Deploy the documentation to GitHub Pages via <code>deploydocs</code>.</li></ul></li></ol><p>All of this is orchestrated by <code>docs/make.jl</code>.</p><pre><code class="language-text hljs">docs/make.jl
   ├─ copy Project/Manifest → docs/src/assets/toml
   ├─ include docs/src/docutils/utils.jl
   ├─ with_processed_template_problems(&quot;docs/src&quot;) do core_problems
   │    └─ with_processed_templates([core/cpu.md, core/gpu.md, core/problems], ... ) do
   │         └─ makedocs(...)
   └─ deploydocs(...)</code></pre><h2 id="Documentation-Utilities-Directory-(docs/src/docutils)"><a class="docs-heading-anchor" href="#Documentation-Utilities-Directory-(docs/src/docutils)">Documentation Utilities Directory (<code>docs/src/docutils</code>)</a><a id="Documentation-Utilities-Directory-(docs/src/docutils)-1"></a><a class="docs-heading-anchor-permalink" href="#Documentation-Utilities-Directory-(docs/src/docutils)" title="Permalink"></a></h2><p>The <code>docs/src/docutils/</code> directory contains the Julia code used <strong>only at documentation build time</strong>:</p><ul><li><code>CTBenchmarksDocUtils.jl</code> – main module that includes all submodules and exports the public API used in templates and <code>docs/make.jl</code>.</li><li><code>utils.jl</code> – entry point loaded by <code>include(joinpath(@__DIR__, &quot;src&quot;, &quot;docutils&quot;, &quot;utils.jl&quot;))</code> in <code>docs/make.jl</code> and by <code>@setup BENCH</code> blocks via <code>include(normpath(joinpath(@__DIR__, &quot;..&quot;, &quot;docutils&quot;, &quot;utils.jl&quot;)))</code>.</li><li><code>modules/</code> – helper modules implementing template generation/processing, figure generation, performance profiles, environment/log printers, and text generation.</li></ul><p>Unlike <code>docs/src/assets/</code>, which holds static files (TOML, benchmark JSON, generated figures) that are copied to the built site, the <code>docutils</code> directory is an <strong>internal implementation detail</strong> and is not deployed as part of the web assets.</p><hr/><h2 id="docs/make.jl:-Orchestrating-the-Build"><a class="docs-heading-anchor" href="#docs/make.jl:-Orchestrating-the-Build"><code>docs/make.jl</code>: Orchestrating the Build</a><a id="docs/make.jl:-Orchestrating-the-Build-1"></a><a class="docs-heading-anchor-permalink" href="#docs/make.jl:-Orchestrating-the-Build" title="Permalink"></a></h2><p>The main steps in <code>docs/make.jl</code> are:</p><ul><li><p><strong>Configuration</strong></p><ul><li><code>draft = false</code> controls execution of <code>@example</code> blocks.</li><li><code>exclude_problems_from_draft</code> can force specific problem pages to execute their examples even in draft mode.</li><li><code>debug = false</code> controls the verbosity of logs from documentation utilities: when set to <code>true</code>, additional per-block messages and full stacktraces are printed for easier debugging of template and figure generation.</li></ul></li><li><p><strong>Environment files</strong></p><ul><li><code>Project.toml</code> and <code>Manifest.toml</code> are copied into <code>docs/src/assets/toml/</code> so that the exact environment used for the documentation is preserved.</li></ul></li><li><p><strong>Documentation utilities</strong></p><ul><li><code>include(&quot;src/docutils/utils.jl&quot;)</code> loads all helper modules: template generation, template processing, figure generation, plotting, and log/environment printers.</li></ul></li><li><p><strong>Template generation for problems</strong></p><ul><li><code>with_processed_template_problems(joinpath(@__DIR__, &quot;src&quot;); ...) do core_problems</code>:<ul><li>Calls into <code>TemplateGenerator.write_core_benchmark_templates</code> to create <code>.md.template</code> files for all <strong>core benchmark problems</strong>.</li><li>Returns a list of generated template paths and the list of problem names <code>core_problems</code>.</li><li>Ensures that all generated <code>.md.template</code> files are deleted afterwards.</li></ul></li></ul><p>Flow (problems):</p><pre><code class="language-text hljs">with_processed_template_problems(src) do core_problems
    ├─ write_core_benchmark_templates(src, draft, exclude)
    │    ├─ read core benchmark JSONs
    │    ├─ collect all problems
    │    └─ write core/problems/&lt;problem&gt;.md.template
    ├─ core_problems = list of problem names
    └─ f(core_problems)  # calls into template processing + makedocs
    # finally: remove generated .md.template files
end</code></pre></li><li><p><strong>Template processing</strong></p><ul><li><p>Inside the <code>do core_problems</code> block, we call:</p><pre><code class="language-julia hljs">with_processed_templates(
    [
        joinpath(&quot;core&quot;, &quot;cpu.md&quot;),
        joinpath(&quot;core&quot;, &quot;gpu.md&quot;),
        joinpath(&quot;core&quot;, &quot;problems&quot;),
    ],
    joinpath(@__DIR__, &quot;src&quot;),
    joinpath(@__DIR__, &quot;src&quot;, &quot;assets&quot;, &quot;md&quot;),
) do
    makedocs(; ...)
end</code></pre><ul><li><code>with_processed_templates</code> (from <code>TemplateProcessor.jl</code>) takes a list of template files/directories and:<ol><li>Resolves them to concrete template paths (e.g., <code>core/cpu.md.template</code>, <code>core/problems/*.md.template</code>).</li><li>Processes each template, replacing <code>INCLUDE_ENVIRONMENT</code>, <code>INCLUDE_FIGURE</code>, and <code>INCLUDE_TEXT</code> blocks (and legacy <code>INCLUDE_ANALYSIS</code> blocks) and writing the resulting <code>.md</code> files.</li><li>Collects all figure paths generated during processing.</li><li>Runs <code>makedocs</code>.</li><li>Cleans up all generated <code>.md</code> files and figures in a <code>finally</code> block.</li></ol></li></ul></li></ul><p>Flow (templates):</p><pre><code class="language-text hljs">with_processed_templates(files, src, assets_md) do
    ├─ construct_template_files(files, src)
    │    └─ expand directories → list of *.md.template
    ├─ process_templates(...)
    │    ├─ for each template:
    │    │    ├─ replace_environment_blocks
    │    │    └─ replace_figure_blocks → generate figures (SVG + PDF)
    │    └─ write processed .md files
    ├─ makedocs(...)
    └─ finally
         ├─ remove generated .md files
         └─ remove generated figures (if any)
end</code></pre></li><li><p><strong>Building and deployment</strong></p><ul><li><code>makedocs</code> builds the HTML documentation.</li><li><code>deploydocs</code> publishes it to GitHub Pages.</li></ul></li></ul><p>The important takeaway: <strong>problem pages and some benchmark pages are not written by hand</strong>. They are generated and then processed via templates.</p><hr/><h2 id="Automatic-Problem-Pages"><a class="docs-heading-anchor" href="#Automatic-Problem-Pages">Automatic Problem Pages</a><a id="Automatic-Problem-Pages-1"></a><a class="docs-heading-anchor-permalink" href="#Automatic-Problem-Pages" title="Permalink"></a></h2><p>Problem pages under <code>docs/src/core/problems/</code> are generated automatically from benchmark data using <code>TemplateGenerator.jl</code>.</p><h3 id="Core-benchmark-templates"><a class="docs-heading-anchor" href="#Core-benchmark-templates">Core benchmark templates</a><a id="Core-benchmark-templates-1"></a><a class="docs-heading-anchor-permalink" href="#Core-benchmark-templates" title="Permalink"></a></h3><p>The function <code>write_core_benchmark_templates</code>:</p><ul><li>Reads the list of <strong>core benchmarks</strong> (e.g., <code>core-ubuntu-latest</code>, <code>core-moonshot-cpu</code>, <code>core-moonshot-gpu</code>).</li><li>For each benchmark, determines which <strong>problems</strong> appear in its JSON results (e.g., <code>beam</code>, <code>crane</code>, ...).</li><li>Builds a set of all problems across all core benchmarks.</li><li>For each problem, calls <code>generate_template_problem_from_list</code> to create a <code>.md.template</code> file under <code>core/problems/</code>.</li></ul><pre><code class="language-text hljs">core-*.json (benchmark results)
   └─ write_core_benchmark_templates
        ├─ get_problems_in_benchmarks → [problem_1, problem_2, ...]
        └─ for each problem
             └─ generate_template_problem_from_list
                  └─ core/problems/&lt;problem&gt;.md.template</code></pre><h3 id="Structure-of-a-generated-problem-page"><a class="docs-heading-anchor" href="#Structure-of-a-generated-problem-page">Structure of a generated problem page</a><a id="Structure-of-a-generated-problem-page-1"></a><a class="docs-heading-anchor-permalink" href="#Structure-of-a-generated-problem-page" title="Permalink"></a></h3><p>Inside <code>generate_template_problem</code> and <code>generate_template_problem_from_list</code>, a typical problem page contains:</p><ul><li>A title and description for the problem.</li><li>A single <code>@setup BENCH</code> block that loads <code>utils.jl</code>.</li><li>One <strong>section per benchmark configuration</strong> (e.g., one for <code>core-ubuntu-latest</code>, one for <code>core-moonshot-cpu</code>, etc.). For each section:<ul><li>An <code>INCLUDE_ENVIRONMENT</code> block that will display environment and configuration information.</li><li>One or several <code>INCLUDE_FIGURE</code> blocks for plots such as:<ul><li>Global performance profiles.</li><li>Time vs grid size (line and bar plots).</li></ul></li><li>A <code>@example BENCH</code> block that calls <code>_print_benchmark_log</code> with the corresponding <code>bench_id</code> to print detailed results.</li></ul></li></ul><p>You do <strong>not</strong> edit these pages by hand. They are regenerated from templates whenever documentation is built.</p><hr/><h2 id="Template-Processing-and-Special-Blocks"><a class="docs-heading-anchor" href="#Template-Processing-and-Special-Blocks">Template Processing and Special Blocks</a><a id="Template-Processing-and-Special-Blocks-1"></a><a class="docs-heading-anchor-permalink" href="#Template-Processing-and-Special-Blocks" title="Permalink"></a></h2><p>Template files (both auto-generated and manual) may contain special blocks of the form:</p><ul><li><code>&lt;!-- INCLUDE_ENVIRONMENT: ... --&gt;</code></li><li><code>&lt;!-- INCLUDE_FIGURE: ... --&gt;</code></li><li><code>&lt;!-- INCLUDE_TEXT: ... --&gt;</code></li></ul><p>These are handled by <code>TemplateProcessor.jl</code>.</p><h3 id="INCLUDE_ENVIRONMENT"><a class="docs-heading-anchor" href="#INCLUDE_ENVIRONMENT"><code>INCLUDE_ENVIRONMENT</code></a><a id="INCLUDE_ENVIRONMENT-1"></a><a class="docs-heading-anchor-permalink" href="#INCLUDE_ENVIRONMENT" title="Permalink"></a></h3><p><code>INCLUDE_ENVIRONMENT</code> blocks are used to inject environment and configuration information for a given benchmark. They look like:</p><pre><code class="language-markdown hljs">&lt;!-- INCLUDE_ENVIRONMENT:
BENCH_ID = &quot;core-ubuntu-latest&quot;
ENV_NAME = BENCH
--&gt;</code></pre><p>During template processing:</p><ul><li>The parameter block is parsed by <code>parse_include_params</code>.</li><li>The environment template <code>environment.md.template</code> is loaded.</li><li>Variables such as <code>BENCH_ID</code> and <code>ENV_NAME</code> are substituted.</li><li>The template is rendered using helper functions from <code>PrintEnvConfig.jl</code>, typically including:<ul><li>Download links for <code>Project.toml</code>, <code>Manifest.toml</code> and the benchmark script via <code>_downloads_toml</code>.</li><li>Basic metadata (timestamp, Julia version, OS, machine) via <code>_basic_metadata</code>.</li><li>Optional detailed metadata (<code>_version_info</code>, <code>_complete_manifest</code>, <code>_print_config</code>).</li></ul></li></ul><p>The resulting Markdown replaces the original comment block in the generated <code>.md</code> file.</p><pre><code class="language-text hljs">core/...md.template
   └─ &lt;!-- INCLUDE_ENVIRONMENT: BENCH_ID = &quot;core-ubuntu-latest&quot;, ... --&gt;
        └─ replace_environment_blocks
             └─ environment.md.template + PrintEnvConfig helpers
                  └─ Markdown block (links + metadata + config)</code></pre><h3 id="INCLUDE_FIGURE"><a class="docs-heading-anchor" href="#INCLUDE_FIGURE"><code>INCLUDE_FIGURE</code></a><a id="INCLUDE_FIGURE-1"></a><a class="docs-heading-anchor-permalink" href="#INCLUDE_FIGURE" title="Permalink"></a></h3><p><code>INCLUDE_FIGURE</code> blocks are used to generate and insert plots. For example:</p><pre><code class="language-markdown hljs">&lt;!-- INCLUDE_FIGURE:
FUNCTION = _plot_profile_default_cpu
ARGS = core-ubuntu-latest
--&gt;</code></pre><p>During processing:</p><ul><li>The function name and arguments are parsed from the block.</li><li><code>FigureGeneration.jl</code> looks up the function in the <code>FIGURE_FUNCTIONS</code> registry, which currently includes (among others):<ul><li><code>_plot_profile_default_cpu</code></li><li><code>_plot_profile_default_iter</code></li><li><code>_plot_time_vs_grid_size</code></li><li><code>_plot_time_vs_grid_size_bar</code></li></ul></li><li>The plotting function is called in the <code>BENCH</code> environment with string arguments.</li><li>Two files are generated in the figures directory (SVG + PDF), with a unique basename derived from the template name, function name, and arguments.</li><li>The template processor emits Markdown that:<ul><li>Embeds the SVG figure in the page.</li><li>Wraps the SVG in a link pointing to the PDF.</li></ul></li></ul><p>As a result, <strong>figures in the documentation are clickable</strong> and open a PDF version suitable for high-quality printing.</p><pre><code class="language-text hljs">core/...md.template
   └─ &lt;!-- INCLUDE_FIGURE: FUNCTION = _plot_profile_default_cpu, ARGS = core-ubuntu-latest --&gt;
        └─ replace_figure_blocks
             ├─ call_figure_function(FUNCTION, ARGS)
             ├─ generate_figure_files → SVG + PDF in assets/plots
             └─ emit @raw html block (img SVG, link PDF)</code></pre><h3 id="INCLUDE_TEXT"><a class="docs-heading-anchor" href="#INCLUDE_TEXT"><code>INCLUDE_TEXT</code></a><a id="INCLUDE_TEXT-1"></a><a class="docs-heading-anchor-permalink" href="#INCLUDE_TEXT" title="Permalink"></a></h3><p><code>INCLUDE_TEXT</code> blocks are used to generate textual analysis or other Markdown-compatible content from benchmark results, such as performance-profile summaries or tables.</p><p>For example:</p><pre><code class="language-markdown hljs">&lt;!-- INCLUDE_TEXT:
FUNCTION = _analyze_profile_default_cpu
ARGS = core-ubuntu-latest
--&gt;</code></pre><p>During processing:</p><ul><li>The function name and arguments are parsed from the block.</li><li><code>TextGeneration.jl</code> looks up the function in the <code>TEXT_FUNCTIONS</code> registry, which currently includes:<ul><li><code>_analyze_profile_default_cpu</code></li><li><code>_analyze_profile_default_iter</code></li><li><code>_print_benchmark_table_results</code></li></ul></li><li>The text function is called with string arguments and must return a Markdown-compatible string (for example, the output of <code>analyze_performance_profile(pp)</code>, a benchmark table, or a string containing <code>@raw html</code> blocks for more advanced layouts).</li><li>The returned content is inlined directly into the generated <code>.md</code> file.</li></ul><h4 id="Example:-Dynamic-multi-problem-benchmark-table"><a class="docs-heading-anchor" href="#Example:-Dynamic-multi-problem-benchmark-table">Example: Dynamic multi-problem benchmark table</a><a id="Example:-Dynamic-multi-problem-benchmark-table-1"></a><a class="docs-heading-anchor-permalink" href="#Example:-Dynamic-multi-problem-benchmark-table" title="Permalink"></a></h4><p>A common usage of <code>INCLUDE_TEXT</code> is to render benchmark-result tables via <code>_print_benchmark_table_results</code>:</p><pre><code class="language-markdown hljs">&lt;!-- INCLUDE_TEXT:
FUNCTION = _print_benchmark_table_results
ARGS = core-ubuntu-latest
--&gt;</code></pre><ul><li>If the benchmark contains <strong>a single problem</strong>, the function returns a standard Markdown table.</li><li>If the benchmark contains <strong>multiple problems</strong>, it returns a <code>@raw html</code> block containing:<ul><li>a <code>&lt;select&gt;</code> element listing all problems,</li><li>one HTML table per problem, each wrapped in a <code>&lt;div&gt;</code> and toggled via a small JavaScript snippet,</li><li>persistence of the last selected problem using <code>window.localStorage</code> with a key derived from the benchmark ID.</li></ul></li></ul><p>This allows long per-problem tables to remain compact and navigable in the rendered documentation while being generated from a single <code>INCLUDE_TEXT</code> block in the template.</p><hr/><h2 id="Figure-Types,-Analysis,-and-Helper-Functions"><a class="docs-heading-anchor" href="#Figure-Types,-Analysis,-and-Helper-Functions">Figure Types, Analysis, and Helper Functions</a><a id="Figure-Types,-Analysis,-and-Helper-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Figure-Types,-Analysis,-and-Helper-Functions" title="Permalink"></a></h2><p>Several helper modules provide the concrete plots and textual outputs:</p><ul><li><p><strong>Performance profiles</strong> — <code>PerformanceProfileCore.jl</code> + <code>PlotPerformanceProfile.jl</code> + <code>AnalyzePerformanceProfile.jl</code> + <code>TextGeneration.jl</code></p><ul><li><code>_plot_profile_default_cpu(bench_id)</code> / <code>_plot_profile_default_iter(bench_id)</code> (called via <code>INCLUDE_FIGURE</code>)</li><li><code>_analyze_profile_default_cpu(bench_id)</code> / <code>_analyze_profile_default_iter(bench_id)</code> (called via <code>INCLUDE_TEXT</code>)</li><li>Together, these functions compute, plot, and summarize Dolan–Moré-style performance profiles over <code>(problem, grid_size)</code> instances and <code>(model, solver)</code> combinations.</li></ul></li><li><p><strong>Time vs grid size</strong> — <code>PlotTimeVsGridSize.jl</code></p><ul><li><code>_plot_time_vs_grid_size(problem, bench_id, src_dir)</code></li><li><code>_plot_time_vs_grid_size_bar(problem, bench_id, src_dir)</code></li><li>Line and bar plots showing mean solve time as a function of grid size.</li></ul></li><li><p><strong>Benchmark logs</strong> — <code>PrintLogResults.jl</code></p><ul><li><code>_print_benchmark_log(bench_id, src_dir; problems=nothing)</code></li><li>Prints a tree-structured log by problem, solver, discretization, grid size, and model, with colored formatting.</li></ul></li><li><p><strong>Environment and configuration</strong> — <code>PrintEnvConfig.jl</code></p><ul><li><code>_downloads_toml(bench_id, src_dir)</code></li><li><code>_basic_metadata(bench_id, src_dir)</code></li><li><code>_version_info(bench_id, src_dir)</code></li><li><code>_complete_manifest(bench_id, src_dir)</code></li><li><code>_print_config(bench_id, src_dir)</code></li></ul></li></ul><p>These functions are all made available by <code>utils.jl</code> and are typically used indirectly via <code>INCLUDE_ENVIRONMENT</code>, <code>INCLUDE_FIGURE</code>, or <code>@example BENCH</code> blocks.</p><hr/><h2 id="Adding-Documentation-for-a-Benchmark"><a class="docs-heading-anchor" href="#Adding-Documentation-for-a-Benchmark">Adding Documentation for a Benchmark</a><a id="Adding-Documentation-for-a-Benchmark-1"></a><a class="docs-heading-anchor-permalink" href="#Adding-Documentation-for-a-Benchmark" title="Permalink"></a></h2><p>There are two complementary ways benchmark results appear in the documentation.</p><h3 id="1.-Automatic-per-problem-pages-(core-benchmarks)"><a class="docs-heading-anchor" href="#1.-Automatic-per-problem-pages-(core-benchmarks)">1. Automatic per-problem pages (core benchmarks)</a><a id="1.-Automatic-per-problem-pages-(core-benchmarks)-1"></a><a class="docs-heading-anchor-permalink" href="#1.-Automatic-per-problem-pages-(core-benchmarks)" title="Permalink"></a></h3><p>For <strong>core benchmarks</strong>, once the benchmark JSON files are present under <code>docs/src/assets/benchmarks/&lt;id&gt;/&lt;id&gt;.json</code>, the corresponding problem pages are generated automatically by <code>write_core_benchmark_templates</code>.</p><p>You do not need to create these pages manually. The system inspects the JSON results, discovers which problems were benchmarked, and creates one section per benchmark configuration in the appropriate problem page.</p><h3 id="2.-Manual-benchmark-pages"><a class="docs-heading-anchor" href="#2.-Manual-benchmark-pages">2. Manual benchmark pages</a><a id="2.-Manual-benchmark-pages-1"></a><a class="docs-heading-anchor-permalink" href="#2.-Manual-benchmark-pages" title="Permalink"></a></h3><p>You can also write dedicated pages for specific benchmarks, such as <code>docs/src/core/cpu.md.template</code> or <code>docs/src/benchmark-&lt;name&gt;.md.template</code>.</p><p>The general pattern for such a page is:</p><ol><li><p>Add a single <code>@setup BENCH</code> block at the top of the page:</p><pre><code class="language-julia hljs">```@setup BENCH
# Load utilities
include(normpath(joinpath(@__DIR__, &quot;..&quot;, &quot;docutils&quot;, &quot;utils.jl&quot;)))
```</code></pre></li><li><p>For each benchmark you want to show, add:</p><ul><li><p>An <code>INCLUDE_ENVIRONMENT</code> block with a literal <code>BENCH_ID</code>:</p><pre><code class="language-markdown hljs">&lt;!-- INCLUDE_ENVIRONMENT:
BENCH_ID = &quot;core-ubuntu-latest&quot;
ENV_NAME = BENCH
--&gt;</code></pre></li><li><p>One or more <code>INCLUDE_FIGURE</code> blocks, for example a CPU-time performance profile and an iterations profile:</p><pre><code class="language-markdown hljs">&lt;!-- INCLUDE_FIGURE:
FUNCTION = _plot_profile_default_cpu
ARGS = core-ubuntu-latest
--&gt;

&lt;!-- INCLUDE_FIGURE:
FUNCTION = _plot_profile_default_iter
ARGS = core-ubuntu-latest
--&gt;</code></pre></li><li><p>Optional <code>INCLUDE_TEXT</code> blocks to insert textual analysis:</p><pre><code class="language-markdown hljs">&lt;!-- INCLUDE_TEXT:
FUNCTION = _analyze_profile_default_cpu
ARGS = core-ubuntu-latest
--&gt;

&lt;!-- INCLUDE_TEXT:
FUNCTION = _print_benchmark_table_results
ARGS = core-ubuntu-latest
--&gt;</code></pre></li><li><p>A <code>@example BENCH</code> block to print the benchmark log:</p><pre><code class="language-julia hljs">```@example BENCH
_print_benchmark_log(&quot;core-ubuntu-latest&quot;) # hide
```</code></pre></li></ul></li><li><p>Add your page to <code>docs/make.jl</code> in the <code>pages</code> list so that Documenter knows about it.</p></li></ol><p>For a minimal template example, see the &quot;Documentation page&quot; step in <code>Add a new benchmark</code>. That section is intentionally concise and defers to this page for full details of the template processing pipeline.</p><hr/><h2 id="Summary"><a class="docs-heading-anchor" href="#Summary">Summary</a><a id="Summary-1"></a><a class="docs-heading-anchor-permalink" href="#Summary" title="Permalink"></a></h2><ul><li><code>docs/make.jl</code> drives the whole documentation build: copying environment files, generating templates, processing them, and calling <code>makedocs</code>.</li><li>Problem pages for core benchmarks are generated automatically from benchmark results.</li><li>Template processing replaces <code>INCLUDE_ENVIRONMENT</code>, <code>INCLUDE_FIGURE</code>, and <code>INCLUDE_TEXT</code> blocks (or legacy <code>INCLUDE_ANALYSIS</code> blocks) with rich content, figures, and textual analyses.</li><li>Helper modules provide plotting, logging, and environment/configuration utilities.</li><li>To document a new benchmark, you can rely on the automatic problem pages and optionally add a manual page following the template pattern above.</li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="add_benchmark.html">« Add a New Benchmark</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.0 on <span class="colophon-date" title="Tuesday 18 November 2025 17:13">Tuesday 18 November 2025</span>. Using Julia version 1.12.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
